#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import socket
import asyncio
import ipaddress
import subprocess
import urllib.parse
import random
import traceback
from collections.abc import Iterable
from urllib.parse import urlparse, quote

import requests
from bs4 import BeautifulSoup

# ==================== å…¨å±€é…ç½® ====================
CONFIG = {
    # æ–‡ä»¶é…ç½®
    'final_file': 'result.txt',
    
    # æµ‹é€Ÿæƒé‡é…ç½®
    'response_time_weight': 0.6,
    'resolution_weight': 0.4,
    
    # é¢‘é“é…ç½®
    'max_urls_per_channel': 8,
    
    # æ€§èƒ½é…ç½®
    'max_concurrent_tasks': 10,
    
    # æµ‹é€Ÿé…ç½®
    'open_sort': True,
    'ffmpeg_time': 10,
    
    # ç½‘ç»œé…ç½®
    'ipv_type': "ipv4",
    
    # è¿‡æ»¤é…ç½®
    'domain_blacklist': [],
    'url_keywords_blacklist': [],
    'search_ignore_key': ["é«˜æ¸…", "4K", "HD", "HDR", "æœæ¯”", "Dolby"],
    
    # æœç´¢é…ç½®
    'search_regions': ["å…¨å›½"],
    'search_page_num': 8,
    
    # çˆ¬å–æ¨¡å¼é…ç½® (1-tonkiangç»„æ’­æº, 2-crawl_urls, 3-å…¨éƒ¨)
    'crawl_type': "3",
    
    # è®¢é˜…æºé…ç½® (ç”¨äºæå–RTPè·¯å¾„)
    'search_dict': {
        "ä¸Šæµ·": "https://raw.githubusercontent.com/xisohi/IPTV-Multicast-source/main/shanghai/telecom.txt",
        "åŒ—äº¬": "https://raw.githubusercontent.com/xisohi/IPTV-Multicast-source/main/beijing/unicom.txt",
        "å¹¿ä¸œ": "https://raw.githubusercontent.com/xisohi/IPTV-Multicast-source/main/guangdong/telecom.txt"
    },
    
    # å…¶ä»–ç›´æ’­æºURL
    'crawl_urls': [
        "https://raw.githubusercontent.com/PizazzGY/TVBox/main/live.txt",
        "https://raw.githubusercontent.com/YanG-1989/m3u/main/Gather.m3u",
        "https://raw.githubusercontent.com/ssili126/tv/main/itvlist.txt"
    ],
    
    # æ¨¡æ¿é¢‘é“åˆ—è¡¨é…ç½®
    'channel_list': {
        "å¤®è§†é¢‘é“": [
            "CCTV-1", "CCTV-2", "CCTV-3", "CCTV-4", "CCTV-5", "CCTV-5+", "CCTV-6", "CCTV-7", 
            "CCTV-8", "CCTV-9", "CCTV-10", "CCTV-11", "CCTV-12", "CCTV-13", "CCTV-14", "CCTV-15",
            "CCTV-16", "CCTV-17", "CCTV-æ–°é—»", "CCTV-å°‘å„¿", "CCTV-éŸ³ä¹", "CCTV-æˆæ›²", "CCTV-ç¤¾ä¼šä¸æ³•"
        ],
        "å«è§†é¢‘é“": [
            "åŒ—äº¬å«è§†", "å¤©æ´¥å«è§†", "æ²³åŒ—å«è§†", "å±±è¥¿å«è§†", "å†…è’™å¤å«è§†", "è¾½å®å«è§†", "å‰æ—å«è§†", 
            "é»‘é¾™æ±Ÿå«è§†", "ä¸œæ–¹å«è§†", "æ±Ÿè‹å«è§†", "æµ™æ±Ÿå«è§†", "å®‰å¾½å«è§†", "ç¦å»ºå«è§†", "æ±Ÿè¥¿å«è§†", 
            "å±±ä¸œå«è§†", "æ²³å—å«è§†", "æ¹–åŒ—å«è§†", "æ¹–å—å«è§†", "å¹¿ä¸œå«è§†", "å¹¿è¥¿å«è§†", "æµ·å—å«è§†", 
            "é‡åº†å«è§†", "å››å·å«è§†", "è´µå·å«è§†", "äº‘å—å«è§†", "é™•è¥¿å«è§†", "ç”˜è‚ƒå«è§†", "å®å¤å«è§†"
        ],
        "é«˜æ¸…é¢‘é“": [
            "CCTV-1é«˜æ¸…", "CCTV-2é«˜æ¸…", "CCTV-3é«˜æ¸…", "CCTV-4é«˜æ¸…", "CCTV-5é«˜æ¸…", "CCTV-6é«˜æ¸…",
            "CCTV-7é«˜æ¸…", "CCTV-8é«˜æ¸…", "CCTV-9é«˜æ¸…", "CCTV-10é«˜æ¸…", "CCTV-11é«˜æ¸…", "CCTV-12é«˜æ¸…",
            "CCTV-13é«˜æ¸…", "CCTV-14é«˜æ¸…", "CCTV-15é«˜æ¸…", "åŒ—äº¬å«è§†é«˜æ¸…", "æ¹–å—å«è§†é«˜æ¸…", 
            "æµ™æ±Ÿå«è§†é«˜æ¸…", "æ±Ÿè‹å«è§†é«˜æ¸…", "ä¸œæ–¹å«è§†é«˜æ¸…", "å¹¿ä¸œå«è§†é«˜æ¸…", "æ·±åœ³å«è§†é«˜æ¸…"
        ],
        "4Ké¢‘é“": [
            "CCTV-4K", "åŒ—äº¬å«è§†4K", "ä¸Šæµ·çºªå®4K", "æ¹–å—å«è§†4K", "æµ™æ±Ÿå«è§†4K", "æ±Ÿè‹å«è§†4K",
            "ä¸œæ–¹å«è§†4K", "å¹¿ä¸œå«è§†4K", "æ·±åœ³å«è§†4K", "CCTV-16-4K"
        ],
        "åœ°æ–¹é¢‘é“": [
            "åŒ—äº¬æ–‡è‰º", "åŒ—äº¬ç§‘æ•™", "åŒ—äº¬å½±è§†", "åŒ—äº¬è´¢ç»", "åŒ—äº¬ç”Ÿæ´»", "åŒ—äº¬é’å¹´", "åŒ—äº¬æ–°é—»",
            "ä¸Šæµ·æ–°é—»ç»¼åˆ", "ä¸Šæµ·ä¸œæ–¹å½±è§†", "ä¸Šæµ·å¨±ä¹", "ä¸Šæµ·ä½“è‚²", "ä¸Šæµ·çºªå®", "ä¸Šæµ·ç¬¬ä¸€è´¢ç»",
            "å¹¿ä¸œç æ±Ÿ", "å¹¿ä¸œä½“è‚²", "å¹¿ä¸œå…¬å…±", "å¹¿ä¸œæ–°é—»", "æ·±åœ³éƒ½å¸‚", "æ·±åœ³å…¬å…±", "æ·±åœ³ç”µè§†å‰§",
            "é‡åº†æ–°é—»", "é‡åº†å½±è§†", "é‡åº†æ–‡è‰º", "é‡åº†ç¤¾ä¼šä¸æ³•", "æµ™æ±Ÿé’±æ±Ÿ", "æµ™æ±Ÿæ•™è‚²ç§‘æŠ€",
            "æ±Ÿè‹åŸå¸‚", "æ±Ÿè‹å½±è§†", "æ±Ÿè‹å…¬å…±æ–°é—»", "æ¹–å—ç»è§†", "æ¹–å—éƒ½å¸‚", "æ¹–å—å¨±ä¹"
        ],
        "ä½“è‚²é¢‘é“": [
            "CCTV-5", "CCTV-5+", "å¹¿ä¸œä½“è‚²", "åŒ—äº¬ä½“è‚²", "ä¸Šæµ·ä½“è‚²", "æ±Ÿè‹ä½“è‚²", "æµ™æ±Ÿä½“è‚²",
            "å±±ä¸œä½“è‚²", "è¾½å®ä½“è‚²", "æ¹–åŒ—ä½“è‚²", "æ¹–å—ä½“è‚²", "å››å·ä½“è‚²", "å¤©æ´¥ä½“è‚²", "é‡åº†ä½“è‚²",
            "åŠ²çˆ†ä½“è‚²", "è¶³çƒé¢‘é“", "é«˜å°”å¤«ç½‘çƒ"
        ],
        "å½±è§†å¨±ä¹": [
            "CCTV-6", "CCTV-8", "ä¸œæ–¹å½±è§†", "æ¹–å—ç”µå½±", "å¹¿ä¸œç”µå½±", "æ±Ÿè‹å½±è§†", "æµ™æ±Ÿå½±è§†",
            "å±±ä¸œå½±è§†", "å››å·å½±è§†", "é‡åº†å½±è§†", "æ¹–åŒ—å½±è§†", "å¤©æ´¥å½±è§†", "åŒ—äº¬å½±è§†", "ä¸Šæµ·å½±è§†"
        ],
        "å°‘å„¿å¡é€š": [
            "CCTV-14", "å¡é…·å°‘å„¿", "ç‚«åŠ¨å¡é€š", "é‡‘é¹°å¡é€š", "ä¼˜æ¼«å¡é€š", "å˜‰ä½³å¡é€š", "åŒ—äº¬å°‘å„¿",
            "ä¸Šæµ·å“ˆå“ˆç‚«åŠ¨", "å¹¿ä¸œå°‘å„¿", "æ±Ÿè‹ä¼˜æ¼«", "æµ™æ±Ÿå°‘å„¿", "å±±ä¸œå°‘å„¿", "æ¹–å—é‡‘é¹°"
        ],
        "æ–°é—»è´¢ç»": [
            "CCTV-13", "CCTV-2", "å‡¤å‡°èµ„è®¯", "æ·±åœ³è´¢ç»", "ç¬¬ä¸€è´¢ç»", "å¹¿ä¸œæ–°é—»", "åŒ—äº¬æ–°é—»",
            "ä¸Šæµ·æ–°é—»", "æ±Ÿè‹æ–°é—»", "æµ™æ±Ÿæ–°é—»", "å±±ä¸œæ–°é—»", "å››å·æ–°é—»", "æ¹–åŒ—ç»¼åˆ", "æ¹–å—ç»è§†"
        ]
    }
}

# ==================== é…ç½®ç±» ====================
class DynamicConfig:
    """åŠ¨æ€é…ç½®ç±»"""
    def __init__(self):
        # ç›´æ¥ä»å…¨å±€é…ç½®åŠ è½½
        for key, value in CONFIG.items():
            setattr(self, key, value)
        
        # éªŒè¯é…ç½®æœ‰æ•ˆæ€§
        self._validate_config()
    
    def _validate_config(self):
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        # éªŒè¯æƒé‡é…ç½®
        if not (0 <= self.response_time_weight <= 1 and 0 <= self.resolution_weight <= 1):
            print("âš  æƒé‡é…ç½®æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.response_time_weight = 0.5
            self.resolution_weight = 0.5
        
        # éªŒè¯å¹¶å‘æ•°
        if self.max_concurrent_tasks <= 0:
            print("âš  å¹¶å‘ä»»åŠ¡æ•°æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.max_concurrent_tasks = 10
        
        # éªŒè¯URLæ•°é‡é™åˆ¶
        if self.max_urls_per_channel <= 0:
            print("âš  URLæ•°é‡é™åˆ¶æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.max_urls_per_channel = 8
        
        # éªŒè¯é¢‘é“åˆ—è¡¨
        if not self.channel_list:
            print("âš  é¢‘é“åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é¢‘é“")
            self.channel_list = {
                "é»˜è®¤é¢‘é“": ["CCTV-1", "CCTV-2", "æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†"]
            }

# ==================== æ ¸å¿ƒåŠŸèƒ½ç±» ====================
class IPTVProcessor:
    """IPTVç›´æ’­æºå¤„ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.previous_result_dict = {}
    
    def getChannelItems(self):
        """ä»é…ç½®è·å–é¢‘é“é¡¹ - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿"""
        channels = {}
        
        # ä¸¥æ ¼æŒ‰ç…§é…ç½®çš„channel_listè·å–é¢‘é“
        if hasattr(self.config, 'channel_list') and self.config.channel_list:
            for category, channel_names in self.config.channel_list.items():
                channels[category] = {}
                for channel_name in channel_names:
                    channels[category][channel_name] = []  # ç©ºçš„URLåˆ—è¡¨
        
        return channels
    
    def updateChannelUrlsTxt(self, cate, channelUrls):
        """æ›´æ–°åˆ†ç±»å’Œé¢‘é“URLåˆ°æœ€ç»ˆæ–‡ä»¶ - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¡ºåº"""
        try:
            with open("result_new.txt", "a", encoding="utf-8") as f:
                f.write(f"{cate},#genre#\n")
                for name, urls in channelUrls.items():
                    for url in urls:
                        if url and url.strip():
                            f.write(f"{name},{url}\n")
                f.write("\n")
        except Exception as e:
            print(f"âŒ æ›´æ–°é¢‘é“URLæ–‡ä»¶é”™è¯¯: {e}")
    
    def updateFile(self, final_file, old_file):
        """æ›´æ–°æ–‡ä»¶"""
        try:
            if os.path.exists(old_file):
                if os.path.exists(final_file):
                    os.remove(final_file)
                    time.sleep(1)
                os.replace(old_file, final_file)
                print(f"âœ“ æ–‡ä»¶æ›´æ–°å®Œæˆ: {final_file}")
            else:
                print(f"âš  ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨: {old_file}")
        except Exception as e:
            print(f"âŒ æ–‡ä»¶æ›´æ–°é”™è¯¯: {e}")
    
    async def check_stream_speed(self, url_info):
        """æ£€æŸ¥æµåª’ä½“é€Ÿåº¦"""
        try:
            url = url_info[0]
            if not url or not url.strip():
                return float("-inf")
            
            video_info = await self.ffmpeg_url(url, self.config.ffmpeg_time)
            if video_info is None:
                return float("-inf")
            
            frame, _ = self.analyse_video_info(video_info)
            if frame is None:
                return float("-inf")
            
            return frame
        except Exception as e:
            print(f"âŒ æµåª’ä½“é€Ÿåº¦æ£€æŸ¥é”™è¯¯ {url_info[0]}: {e}")
            return float("-inf")
    
    async def getSpeed(self, url_info):
        """è·å–é€Ÿåº¦"""
        try:
            url, _, _ = url_info
            if not url or not url.strip():
                return float("-inf")
                
            if "$" in url:
                url = url.split('$')[0]
            url = quote(url, safe=':/?&=$[]')
            url_info[0] = url
            
            speed = await self.check_stream_speed(url_info)
            return speed
        except Exception as e:
            print(f"âŒ è·å–é€Ÿåº¦é”™è¯¯ {url_info[0] if url_info else 'Unknown'}: {e}")
            return float("-inf")
    
    async def limited_getSpeed(self, url_info, semaphore):
        """é™é€Ÿè·å–é€Ÿåº¦"""
        async with semaphore:
            return await self.getSpeed(url_info)
    
    async def compareSpeedAndResolution(self, infoList):
        """æ¯”è¾ƒé€Ÿåº¦å’Œåˆ†è¾¨ç‡"""
        if not infoList:
            return None
        
        semaphore = asyncio.Semaphore(self.config.max_concurrent_tasks)
        
        try:
            response_times = await asyncio.gather(
                *[self.limited_getSpeed(url_info, semaphore) for url_info in infoList],
                return_exceptions=True
            )
        except Exception as e:
            print(f"âŒ æµ‹é€Ÿä»»åŠ¡æ‰§è¡Œé”™è¯¯: {e}")
            return None
        
        # å¤„ç†å¼‚å¸¸æƒ…å†µ
        valid_responses = []
        for info, rt in zip(infoList, response_times):
            if isinstance(rt, Exception):
                print(f"âš  æµ‹é€Ÿå¼‚å¸¸: {rt}")
                continue
            if rt != float("-inf"):
                valid_responses.append((info, rt))

        def extract_resolution(resolution_str):
            """æå–åˆ†è¾¨ç‡æ•°å€¼"""
            if not resolution_str:
                return 0
            try:
                numbers = re.findall(r"\d+x\d+", resolution_str)
                if numbers:
                    width, height = map(int, numbers[0].split("x"))
                    return width * height
            except (ValueError, IndexError):
                pass
            return 0

        # éªŒè¯æƒé‡é…ç½®
        response_time_weight = max(0, min(1, getattr(self.config, "response_time_weight", 0.5)))
        resolution_weight = max(0, min(1, getattr(self.config, "resolution_weight", 0.5)))
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = response_time_weight + resolution_weight
        if total_weight == 0:
            response_time_weight = 0.5
            resolution_weight = 0.5
        else:
            response_time_weight /= total_weight
            resolution_weight /= total_weight

        def combined_key(item):
            """ç»„åˆæ’åºé”®"""
            try:
                (_, _, resolution), response_time = item
                resolution_value = extract_resolution(resolution) if resolution else 0
                return (
                    response_time_weight * response_time +
                    resolution_weight * resolution_value
                )
            except Exception:
                return float("-inf")

        try:
            sorted_res = sorted(valid_responses, key=combined_key, reverse=True)
            return sorted_res
        except Exception as e:
            print(f"âŒ æ’åºé”™è¯¯: {e}")
            return valid_responses
    
    def getTotalUrls(self, data):
        """è·å–æ€»URL - é™åˆ¶ä¸º8ä¸ª"""
        if not data:
            return []
        try:
            max_urls = min(self.config.max_urls_per_channel, 8)  # ç¡®ä¿æœ€å¤š8ä¸ª
            if len(data) > max_urls:
                total_urls = [url for (url, _, _), _ in data[:max_urls]]
            else:
                total_urls = [url for (url, _, _), _ in data]
            return list(dict.fromkeys(total_urls))
        except Exception as e:
            print(f"âŒ è·å–URLåˆ—è¡¨é”™è¯¯: {e}")
            return []
    
    def getTotalUrlsFromInfoList(self, infoList):
        """ä»ä¿¡æ¯åˆ—è¡¨è·å–æ€»URL - é™åˆ¶ä¸º8ä¸ª"""
        if not infoList:
            return []
        try:
            max_urls = min(self.config.max_urls_per_channel, 8)  # ç¡®ä¿æœ€å¤š8ä¸ª
            total_urls = [
                url for url, _, _ in infoList[:max_urls]
            ]
            return list(dict.fromkeys(total_urls))
        except Exception as e:
            print(f"âŒ ä»ä¿¡æ¯åˆ—è¡¨è·å–URLé”™è¯¯: {e}")
            return []
    
    def is_ipv6(self, url):
        """æ£€æŸ¥æ˜¯å¦ä¸ºIPv6"""
        try:
            host = urllib.parse.urlparse(url).hostname
            if host:
                ipaddress.IPv6Address(host)
                return True
            return False
        except (ValueError, ipaddress.AddressValueError):
            return False
    
    def checkUrlIPVType(self, url):
        """æ£€æŸ¥URL IPç±»å‹"""
        ipv_type = getattr(self.config, "ipv_type", "ipv4")
        if ipv_type == "ipv4":
            return not self.is_ipv6(url)
        elif ipv_type == "ipv6":
            return self.is_ipv6(url)
        else:
            return True
    
    def checkByDomainBlacklist(self, url):
        """æ£€æŸ¥åŸŸåé»‘åå•"""
        try:
            domain_blacklist = [
                urlparse(domain).netloc if urlparse(domain).scheme else domain
                for domain in getattr(self.config, "domain_blacklist", [])
            ]
            return urlparse(url).netloc not in domain_blacklist
        except Exception:
            return True
    
    def checkByURLKeywordsBlacklist(self, url):
        """æ£€æŸ¥URLå…³é”®è¯é»‘åå•"""
        try:
            url_keywords_blacklist = getattr(self.config, "url_keywords_blacklist", [])
            return not any(keyword in url for keyword in url_keywords_blacklist)
        except Exception:
            return True
    
    def filterUrlsByPatterns(self, urls):
        """æ ¹æ®æ¨¡å¼è¿‡æ»¤URL"""
        if not urls:
            return []
        filtered_urls = []
        for url in urls:
            if not url or not url.strip():
                continue
            if not self.checkUrlIPVType(url):
                continue
            if not self.checkByDomainBlacklist(url):
                continue
            if not self.checkByURLKeywordsBlacklist(url):
                continue
            filtered_urls.append(url)
        return filtered_urls
    
    def filter_CCTV_key(self, key: str):
        """è¿‡æ»¤CCTVå…³é”®è¯"""
        if not key:
            return key
        try:
            key = re.sub(r'\[.*?\]', '', key)
            if "cctv" not in key.lower():
                return key.strip()
            chinese_pattern = re.compile("[\u4e00-\u9fa5]+")
            filtered_text = chinese_pattern.sub('', key)
            result = re.sub(r'\[\d+\*\d+\]', '', filtered_text)
            if "-" not in result:
                result = result.replace("CCTV", "CCTV-")
            if result.upper().endswith("HD"):
                result = result[:-2]
            return result.strip()
        except Exception:
            return key
    
    async def ffmpeg_url(self, url, timeout, cmd='ffmpeg'):
        """FFmpeg URLæµ‹è¯•"""
        if not url or not url.strip():
            return None
            
        args = [cmd, '-t', str(timeout), '-stats', '-i', url, '-f', 'null', '-']
        proc = None
        res = None
        try:
            proc = await asyncio.create_subprocess_exec(
                *args,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            out, err = await asyncio.wait_for(proc.communicate(), timeout=timeout + 5)
            if out:
                res = out.decode('utf-8', errors='ignore')
            if err:
                res = err.decode('utf-8', errors='ignore')
            return res
        except asyncio.TimeoutError:
            if proc:
                try:
                    proc.kill()
                except:
                    pass
            return None
        except Exception:
            if proc:
                try:
                    proc.kill()
                except:
                    pass
            return None
        finally:
            if proc:
                try:
                    await proc.wait()
                except:
                    pass
    
    def analyse_video_info(self, video_info):
        """åˆ†æè§†é¢‘ä¿¡æ¯"""
        frame_size = float("-inf")
        if video_info is not None:
            try:
                info_data = video_info.replace(" ", "")
                matches = re.findall(r"frame=(\d+).*?fps=([\d\.]+).*?speed=([\d\.]+)x", info_data)
                if matches:
                    total_frame = 0
                    total_fps = 0.0
                    total_speed = 0.0
                    count = 0
                    for m in matches:
                        try:
                            frame = int(m[0])
                            fps = float(m[1])
                            speed = float(m[2])
                            total_frame += frame
                            total_fps += fps
                            total_speed += speed
                            count += 1
                        except (ValueError, IndexError):
                            continue
                    if count > 0:
                        avg_frame = total_frame / count
                        avg_fps = total_fps / count
                        avg_speed = total_speed / count
                        frame_size = avg_frame + avg_fps + avg_speed
            except Exception:
                pass
        return frame_size, None
    
    def find_matching_values(self, dictionary, partial_key):
        """æŸ¥æ‰¾åŒ¹é…å€¼"""
        if not dictionary or not partial_key:
            return None
        result = []
        matching_keys = []
        try:
            for key in dictionary:
                if partial_key not in key:
                    continue
                if not key.replace(partial_key, ""):
                    matching_keys.append(key)
                elif key.replace(partial_key, "") in self.config.search_ignore_key:
                    matching_keys.append(key)
            if not matching_keys:
                return None
            for m_key in matching_keys:
                if m_key in dictionary:
                    result.extend(dictionary[m_key])
        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾åŒ¹é…å€¼é”™è¯¯: {e}")
        return result if result else None
    
    def get_previous_results(self, file_path):
        """è·å–å…ˆå‰ç»“æœ"""
        channel_dict = {}
        if not os.path.exists(file_path):
            return channel_dict
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                lines = file.readlines()
                for line in lines:
                    if "#genre#" in line:
                        continue
                    parts = line.strip().split(',')
                    if len(parts) == 2:
                        channel_name, url = parts
                        if channel_name and url:
                            if channel_name in channel_dict:
                                channel_dict[channel_name].append(url)
                            else:
                                channel_dict[channel_name] = [url]
        except Exception as e:
            print(f"âŒ è¯»å–å…ˆå‰ç»“æœé”™è¯¯: {e}")
        return channel_dict

# ==================== å¢å¼ºçš„çˆ¬å–å’Œæœç´¢åŠŸèƒ½ ====================
class IPTVCrawler:
    """IPTVçˆ¬å–å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, config, processor):
        self.config = config
        self.processor = processor
        self.rtp_paths = []
    
    def extract_rtp_paths(self):
        """ä»search_dictä¸­æå–RTPè·¯å¾„"""
        rtp_paths = []
        for region, url in self.config.search_dict.items():
            try:
                print(f"ğŸ“¡ æå–RTPè·¯å¾„ä»: {region}")
                response = requests.get(url, timeout=15)
                if response.status_code == 200:
                    content = response.text
                    lines = content.split('\n')
                    for line in lines:
                        line = line.strip()
                        if line.startswith('rtp://') or 'rtp://' in line:
                            rtp_match = re.search(r'rtp://[^/]+(/.*)', line)
                            if rtp_match:
                                path = rtp_match.group(1)
                                if path not in rtp_paths:
                                    rtp_paths.append(path)
                                    print(f"  âœ… æ‰¾åˆ°RTPè·¯å¾„: {path}")
                else:
                    print(f"  âŒ HTTPé”™è¯¯: {response.status_code}")
            except Exception as e:
                print(f"âŒ æå–RTPè·¯å¾„å¤±è´¥ {region}: {e}")
        
        print(f"ğŸ“Š æ€»å…±æå–åˆ° {len(rtp_paths)} ä¸ªRTPè·¯å¾„")
        return rtp_paths
    
    def crawl_tonkiang_all_multicast(self, page_num=5):
        """ä»tonkiang.usçˆ¬å–æ‰€æœ‰ç»„æ’­æºï¼ˆä¸æŒ‡å®šå…³é”®è¯ï¼‰"""
        print("ğŸŒ çˆ¬å–tonkiang.usæ‰€æœ‰ç»„æ’­æº...")
        ip_headers = []
        
        for page in range(1, page_num + 1):
            try:
                # è®¿é—®ç»„æ’­é¡µé¢
                url = f"http://tonkiang.us/hotellist.html?page={page}"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=15)
                if response.status_code != 200:
                    print(f"  âŒ ç¬¬{page}é¡µHTTPé”™è¯¯: {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                channel_divs = soup.find_all('div', class_='channel')
                
                for div in channel_divs:
                    try:
                        result_div = div.find('div', class_='result')
                        if not result_div:
                            continue
                        
                        # æŸ¥æ‰¾IPå¤´ä¿¡æ¯
                        ip_pattern = r'\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+)\b'
                        matches = re.findall(ip_pattern, result_div.get_text())
                        
                        for match in matches:
                            if match not in ip_headers:
                                ip_headers.append(match)
                    
                    except Exception as e:
                        continue
                
                print(f"  ğŸ“„ ç¬¬{page}é¡µæ‰¾åˆ° {len(channel_divs)} ä¸ªé¢‘é“ï¼ŒIPå¤´æ€»æ•°: {len(ip_headers)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                if not channel_divs:
                    break
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–tonkiang.usç¬¬{page}é¡µé”™è¯¯: {e}")
                continue
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(ip_headers)} ä¸ªIPå¤´")
        return ip_headers
    
    def crawl_tonkiang_by_region(self, region, page_num=5):
        """ä»tonkiang.usæŒ‰åœ°åŒºçˆ¬å–ç»„æ’­æº"""
        print(f"ğŸŒ çˆ¬å–tonkiang.usåœ°åŒºç»„æ’­æº: {region}")
        ip_headers = []
        
        for page in range(1, page_num + 1):
            try:
                # ä½¿ç”¨åœ°åŒºä½œä¸ºæœç´¢å…³é”®è¯
                url = f"http://tonkiang.us/hotellist.html?s={quote(region)}&page={page}"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=15)
                if response.status_code != 200:
                    print(f"  âŒ åœ°åŒº{region}ç¬¬{page}é¡µHTTPé”™è¯¯: {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                channel_divs = soup.find_all('div', class_='channel')
                
                for div in channel_divs:
                    try:
                        result_div = div.find('div', class_='result')
                        if not result_div:
                            continue
                        
                        # æŸ¥æ‰¾IPå¤´ä¿¡æ¯
                        ip_pattern = r'\b(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+)\b'
                        matches = re.findall(ip_pattern, result_div.get_text())
                        
                        for match in matches:
                            if match not in ip_headers:
                                ip_headers.append(match)
                    
                    except Exception as e:
                        continue
                
                print(f"  ğŸ“„ åœ°åŒº{region}ç¬¬{page}é¡µæ‰¾åˆ° {len(channel_divs)} ä¸ªé¢‘é“ï¼ŒIPå¤´æ€»æ•°: {len(ip_headers)}")
                
                if not channel_divs:
                    break
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–åœ°åŒº{region}ç¬¬{page}é¡µé”™è¯¯: {e}")
                continue
        
        print(f"ğŸ“Š åœ°åŒº{region}æ€»å…±æ‰¾åˆ° {len(ip_headers)} ä¸ªIPå¤´")
        return ip_headers
    
    def combine_rtp_urls(self, ip_headers, rtp_paths):
        """ç»„åˆRTP URLï¼šå°†IPå¤´ä¸RTPè·¯å¾„æ‹¼æ¥æˆå®Œæ•´URL"""
        combined_urls = []
        
        for ip_header in ip_headers:
            for path in rtp_paths:
                full_url = f"http://{ip_header}/rtp{path}"
                combined_urls.append(full_url)
        
        return combined_urls
    
    def get_crawl_result(self):
        """è·å–çˆ¬å–ç»“æœ - å¢å¼ºç‰ˆ"""
        print("ğŸš€ å¼€å§‹çˆ¬å–ç›´æ’­æº...")
        crawl_result_dict = {}
        
        # æå–RTPè·¯å¾„
        rtp_paths = self.extract_rtp_paths()
        self.rtp_paths = rtp_paths
        
        if self.config.crawl_type in ["1", "3"] and rtp_paths:
            print("ğŸ” å¢å¼ºtonkiang.usç»„æ’­æºçˆ¬å–...")
            
            # è·å–æœç´¢åœ°åŒºé…ç½®
            search_regions = getattr(self.config, 'search_regions', ["å…¨å›½"])
            all_ip_headers = []
            
            # æŒ‰é…ç½®çš„åœ°åŒºè¿›è¡Œæœç´¢
            for region in search_regions:
                if region == "å…¨å›½":
                    # æœç´¢æ‰€æœ‰ç»„æ’­æº
                    region_ips = self.crawl_tonkiang_all_multicast(self.config.search_page_num)
                else:
                    # æœç´¢æŒ‡å®šåœ°åŒº
                    region_ips = self.crawl_tonkiang_by_region(region, self.config.search_page_num)
                
                all_ip_headers.extend(region_ips)
                print(f"ğŸ“ åœ°åŒº {region} æ‰¾åˆ° {len(region_ips)} ä¸ªIPå¤´")
            
            # å»é‡
            all_ip_headers = list(set(all_ip_headers))
            print(f"ğŸ“Š æ‰€æœ‰åœ°åŒºæ€»å…±æ‰¾åˆ° {len(all_ip_headers)} ä¸ªå”¯ä¸€IPå¤´")
            
            if all_ip_headers:
                # ç»„åˆRTP URL
                combined_rtp_urls = self.combine_rtp_urls(all_ip_headers, rtp_paths)
                print(f"ğŸ”— ç”Ÿæˆ {len(combined_rtp_urls)} ä¸ªç»„åˆRTP URL")
                
                # å°†ç»„åˆçš„URLåˆ†é…åˆ°å¯¹åº”é¢‘é“
                channels = self.processor.getChannelItems()
                for category, channel_dict in channels.items():
                    for channel_name in channel_dict.keys():
                        filtered_name = self.processor.filter_CCTV_key(channel_name)
                        if filtered_name:
                            # ä¸ºæ¯ä¸ªé¢‘é“åˆ†é…ä¸€éƒ¨åˆ†ç»„åˆURL
                            if channel_name not in crawl_result_dict:
                                crawl_result_dict[channel_name] = []
                            
                            # éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†URLåˆ†é…ç»™è¯¥é¢‘é“ï¼ˆé¿å…æ¯ä¸ªé¢‘é“éƒ½æœ‰å…¨éƒ¨URLï¼‰
                            sample_size = min(8, len(combined_rtp_urls))  # æ¯ä¸ªé¢‘é“æœ€å¤š8ä¸ªURL
                            if sample_size > 0 and combined_rtp_urls:
                                sampled_urls = random.sample(combined_rtp_urls, sample_size)
                                crawl_result_dict[channel_name].extend(sampled_urls)
                
                print(f"ğŸ“º ä¸º {len(crawl_result_dict)} ä¸ªé¢‘é“åˆ†é…äº†ç»„åˆRTP URL")
        
        if self.config.crawl_type in ["2", "3"]:
            print("ğŸŒ çˆ¬å–é…ç½®çš„URLæº...")
            for url in self.config.crawl_urls:
                try:
                    print(f"  ğŸ“¡ çˆ¬å–: {url}")
                    response = requests.get(url, timeout=15)
                    if response.status_code == 200:
                        content = response.text
                        lines = content.split('\n')
                        url_count = 0
                        for line in lines:
                            line = line.strip()
                            if ',' in line and '#genre#' not in line:
                                parts = line.split(',', 1)
                                if len(parts) == 2:
                                    channel, url_val = parts[0].strip(), parts[1].strip()
                                    if channel and url_val:
                                        if channel not in crawl_result_dict:
                                            crawl_result_dict[channel] = []
                                        if url_val not in crawl_result_dict[channel]:
                                            crawl_result_dict[channel].append(url_val)
                                            url_count += 1
                        print(f"  âœ… æˆåŠŸçˆ¬å– {url_count} ä¸ªURL")
                    else:
                        print(f"  âŒ HTTPé”™è¯¯: {response.status_code}")
                except Exception as e:
                    print(f"âŒ çˆ¬å–å¤±è´¥ {url}: {e}")
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(crawl_result_dict)} ä¸ªé¢‘é“")
        return crawl_result_dict
    
    def search_hotel_ip(self):
        """æœç´¢é…’åº—IP"""
        print("ğŸ¨ æœç´¢é…’åº—IP...")
        subscribe_dict = {}
        
        # ä»è®¢é˜…æºè·å–ç›´æ’­æº
        for region, url in self.config.search_dict.items():
            try:
                print(f"  ğŸ“¡ åŠ è½½è®¢é˜…æº: {region}")
                response = requests.get(url, timeout=15)
                if response.status_code == 200:
                    content = response.text
                    lines = content.split('\n')
                    url_count = 0
                    for line in lines:
                        line = line.strip()
                        if ',' in line and '#genre#' not in line:
                            parts = line.split(',', 1)
                            if len(parts) == 2:
                                channel, url_val = parts[0].strip(), parts[1].strip()
                                if channel and url_val:
                                    if channel not in subscribe_dict:
                                        subscribe_dict[channel] = []
                                    if url_val not in subscribe_dict[channel]:
                                        subscribe_dict[channel].append(url_val)
                                        url_count += 1
                    print(f"  âœ… æˆåŠŸåŠ è½½ {region} è®¢é˜…æºï¼Œ{url_count} ä¸ªURL")
                else:
                    print(f"  âŒ HTTPé”™è¯¯: {response.status_code}")
            except Exception as e:
                print(f"âŒ åŠ è½½è®¢é˜…æºå¤±è´¥ {region}: {e}")
        
        # ç”Ÿæˆæœç´¢å…³é”®è¯ - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¢‘é“åˆ—è¡¨
        search_keyword_list = []
        channels = self.processor.getChannelItems()
        for category, channel_dict in channels.items():
            for channel_name in channel_dict.keys():
                filtered_name = self.processor.filter_CCTV_key(channel_name)
                if filtered_name:
                    search_keyword_list.append(filtered_name)
        
        print(f"ğŸ” æœç´¢å®Œæˆï¼Œå…± {len(search_keyword_list)} ä¸ªæœç´¢å…³é”®è¯")
        return subscribe_dict, {}, search_keyword_list

# ==================== ä¸»æ›´æ–°ç±» ====================
class UpdateSource:
    """æ›´æ–°æºä¸»ç±»"""
    
    def __init__(self, crawl_result_dict, subscribe_dict, kw_zbip_dict, search_keyword_list):
        self.config = DynamicConfig()
        self.processor = IPTVProcessor(self.config)
        self.crawl_result_dict = crawl_result_dict
        self.subscribe_dict = subscribe_dict
        self.kw_zbip_dict = kw_zbip_dict
        self.search_keyword_list = search_keyword_list
    
    async def process_channel_urls(self, channel_name, filtered_name):
        """å¤„ç†å•ä¸ªé¢‘é“çš„URL - æ‰€æœ‰æ‰¾åˆ°çš„URLéƒ½è¿›è¡Œæµ‹é€Ÿ"""
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„URLæº
        all_urls = []
        
        # 1. ä»çˆ¬å–ç»“æœè·å–URLï¼ˆç»„åˆçš„RTP URLï¼‰
        if filtered_name and filtered_name in self.crawl_result_dict:
            all_urls.extend(self.crawl_result_dict[filtered_name])
        
        # 2. ä»è®¢é˜…æºè·å–URL
        if filtered_name:
            matching_urls = self.processor.find_matching_values(self.subscribe_dict, filtered_name)
            if matching_urls:
                all_urls.extend(matching_urls)
        
        # è¿‡æ»¤URL
        filtered_urls = self.processor.filterUrlsByPatterns(all_urls)
        
        best_urls = []
        if filtered_urls:
            # å‡†å¤‡æµ‹é€Ÿ - æ‰€æœ‰URLéƒ½è¿›è¡Œæµ‹é€Ÿ
            info_list = [[url, None, None] for url in filtered_urls]
            
            print(f"  âš¡ å¯¹ {len(info_list)} ä¸ªURLè¿›è¡Œæµ‹é€Ÿæ’åº...")
            try:
                # å¼‚æ­¥æµ‹é€Ÿæ’åº
                sorted_data = await self.processor.compareSpeedAndResolution(info_list)
                if sorted_data:
                    best_urls = self.processor.getTotalUrls(sorted_data)
                else:
                    best_urls = self.processor.getTotalUrlsFromInfoList(info_list)
            except Exception as e:
                print(f"âŒ æµ‹é€Ÿæ’åºé”™è¯¯: {e}")
                best_urls = self.processor.getTotalUrlsFromInfoList(info_list)
            
            # ç¡®ä¿æœ€å¤š8ä¸ªURL
            best_urls = best_urls[:8]
            print(f"  âœ… æµ‹é€Ÿå®Œæˆï¼Œé€‰æ‹© {len(best_urls)} ä¸ªæœ€ä½³æº")
        
        return best_urls
    
    async def main(self):
        """ä¸»æ‰§è¡Œå‡½æ•° - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¢‘é“åˆ—è¡¨"""
        print("ğŸš€ å¼€å§‹æ›´æ–°ç›´æ’­æº...")
        start_time = time.time()
        
        try:
            # æ¸…ç†æ—§æ–‡ä»¶
            if os.path.exists("result_new.txt"):
                os.remove("result_new.txt")
            
            # è·å–é¢‘é“æ•°æ® - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿
            channels = self.processor.getChannelItems()
            if not channels:
                print("âŒ é”™è¯¯: æ— æ³•è¯»å–é¢‘é“æ•°æ®")
                return
            
            total_channels = sum(len(channel_dict) for channel_dict in channels.values())
            processed_channels = 0
            
            print(f"ğŸ“º å¼€å§‹å¤„ç† {total_channels} ä¸ªé¢‘é“...")
            print("ğŸ“‹ ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¢‘é“åˆ—è¡¨è¿›è¡Œæœç´¢å’Œæµ‹é€Ÿ...")
            
            # æŒ‰ç…§æ¨¡æ¿åˆ†ç±»é¡ºåºå¤„ç†
            for category, channel_dict in channels.items():
                print(f"\nğŸ·ï¸ å¤„ç†åˆ†ç±»: {category}")
                category_channels = {}
                
                # æŒ‰ç…§æ¨¡æ¿é¢‘é“é¡ºåºå¤„ç†
                for channel_name in channel_dict.keys():
                    processed_channels += 1
                    print(f"  ğŸ“» å¤„ç†é¢‘é“ [{processed_channels}/{total_channels}]: {channel_name}")
                    
                    filtered_name = self.processor.filter_CCTV_key(channel_name)
                    best_urls = await self.process_channel_urls(channel_name, filtered_name)
                    
                    if best_urls:
                        category_channels[channel_name] = best_urls
                        print(f"    âœ… æ‰¾åˆ° {len(best_urls)} ä¸ªå¯ç”¨æº")
                    else:
                        print(f"    âš  æœªæ‰¾åˆ°å¯ç”¨æº")
                        category_channels[channel_name] = []  # å³ä½¿æ²¡æœ‰æºä¹Ÿä¿ç•™é¢‘é“
                
                # æ›´æ–°åˆ†ç±»ç»“æœ - ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¡ºåº
                if category_channels:
                    self.processor.updateChannelUrlsTxt(category, category_channels)
                    print(f"âœ… åˆ†ç±» {category} å¤„ç†å®Œæˆï¼Œå…± {len(category_channels)} ä¸ªé¢‘é“")
            
            # å®Œæˆæ–‡ä»¶æ›´æ–°
            self.processor.updateFile(self.config.final_file, "result_new.txt")
            
            end_time = time.time()
            processing_time = end_time - start_time
            print(f"\nğŸ‰ æ›´æ–°å®Œæˆï¼è€—æ—¶: {processing_time:.2f} ç§’")
            print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {self.config.final_file}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.show_statistics()
            
        except Exception as e:
            print(f"âŒ æ›´æ–°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            traceback.print_exc()
    
    def show_statistics(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        if os.path.exists(self.config.final_file):
            try:
                with open(self.config.final_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                lines = content.split('\n')
                channel_count = 0
                url_count = 0
                categories = []
                
                for line in lines:
                    if line.strip() and '#genre#' in line:
                        categories.append(line.replace(',#genre#', ''))
                    elif line.strip() and '#genre#' not in line and ',' in line:
                        channel_count += 1
                        url_count += 1
                
                print(f"\nğŸ“Š æœ€ç»ˆç»“æœç»Ÿè®¡:")
                print(f"  ğŸ“ åˆ†ç±»æ•°é‡: {len(categories)}")
                print(f"  ğŸ“º é¢‘é“æ•°é‡: {channel_count}")
                print(f"  ğŸ”— URLæ•°é‡: {url_count}")
                print(f"  ğŸ·ï¸ åˆ†ç±»åˆ—è¡¨: {', '.join(categories)}")
            except Exception as e:
                print(f"âŒ ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºé”™è¯¯: {e}")
        else:
            print("âš  ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")

# ==================== ä¸»å‡½æ•° ====================
async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸ¬ IPTVç›´æ’­æºç®¡ç†å·¥å…· - å¢å¼ºRTPç»„åˆç‰ˆ")
    print("âœ¨ ç‰¹ç‚¹:")
    print("  â€¢ ğŸ“‹ ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿é¢‘é“åˆ—è¡¨æœç´¢")
    print("  â€¢ âš¡ æ‰€æœ‰æ‰¾åˆ°çš„URLéƒ½è¿›è¡Œæµ‹é€Ÿ") 
    print("  â€¢ ğŸ“Š æŒ‰ç…§æ¨¡æ¿é¡ºåºç”Ÿæˆç»“æœæ–‡ä»¶")
    print("  â€¢ ğŸ¯ æ¯ä¸ªé¢‘é“æœ€å¤š8ä¸ªä¼˜è´¨æº")
    print("  â€¢ ğŸ”§ é›†æˆé…ç½®ï¼Œæ— éœ€å¤–éƒ¨æ–‡ä»¶")
    print("=" * 70)
    
    # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
    try:
        result = subprocess.run(['ffmpeg', '-version'], capture_output=True, timeout=5, text=True)
        if result.returncode == 0:
            print("âœ… FFmpegå¯ç”¨")
        else:
            print("âš  FFmpegå¯èƒ½ä¸å¯ç”¨ï¼Œå°†å½±å“æµåª’ä½“æµ‹é€Ÿ")
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        print("âš  è­¦å‘Š: FFmpegä¸å¯ç”¨ï¼Œå°†å½±å“æµåª’ä½“æµ‹é€Ÿ")
    
    # æ‰§è¡Œæ›´æ–°
    try:
        config = DynamicConfig()
        processor = IPTVProcessor(config)
        crawler = IPTVCrawler(config, processor)
        
        processor.previous_result_dict = processor.get_previous_results(config.final_file)
        crawl_result_dict = crawler.get_crawl_result()
        subscribe_dict, kw_zbip_dict, search_keyword_list = crawler.search_hotel_ip()
        
        update_source = UpdateSource(crawl_result_dict, subscribe_dict, kw_zbip_dict, search_keyword_list)
        await update_source.main()
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        traceback.print_exc()
