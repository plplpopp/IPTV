import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
from urllib.parse import urlparse
from tqdm import tqdm
import sys
import json

# ============================ é…ç½®æ–‡ä»¶ ============================
# æºURLåˆ—è¡¨
URL_SOURCES = [
    "https://raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/gd/output/ipv4/result.m3u",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt",
    "http://47.120.41.246:8899/zb.txt",
]

# æœ¬åœ°æºæ–‡ä»¶
LOCAL_SOURCE_FILE = "local.txt"

# æ¨¡æ¿é¢‘é“æ–‡ä»¶
TEMPLATE_FILE = "demo.txt"

# è¾“å‡ºæ–‡ä»¶
OUTPUT_TXT = "iptv.txt"
OUTPUT_M3U = "iptv.m3u"
OUTPUT_SPEED_TEST = "speed_test_results.txt"
OUTPUT_CHANNEL_STATS = "channel_stats.json"

# æ¯ä¸ªé¢‘é“ä¿ç•™çš„æ¥å£æ•°é‡
MAX_STREAMS_PER_CHANNEL = 5

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 8

# æµ‹é€Ÿè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SPEED_TEST_TIMEOUT = 12

# æœ€å¤§çº¿ç¨‹æ•°
MAX_WORKERS = 20

# æµ‹é€Ÿæ¨¡å¼
SPEED_TEST_MODE = "all"  # "all": æµ‹è¯•æ‰€æœ‰æº, "sample": æŠ½æ ·æµ‹è¯•

# ============================ æ­£åˆ™è¡¨è¾¾å¼ ============================
# IPv4åœ°å€åŒ¹é…
ipv4_pattern = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')

# é¢‘é“åç§°å’ŒURLåŒ¹é…
channel_pattern = re.compile(r"^([^,]+?),\s*(https?://.+)", re.IGNORECASE)

# M3Uæ ¼å¼è§£æ
extinf_pattern = re.compile(r'tvg-name="([^"]*)"', re.IGNORECASE)
extinf_name_pattern = re.compile(r'#EXTINF:.*?,(.+)', re.IGNORECASE)

def create_default_template():
    """åˆ›å»ºé»˜è®¤æ¨¡æ¿æ–‡ä»¶"""
    print("ğŸ“ åˆ›å»ºé»˜è®¤æ¨¡æ¿æ–‡ä»¶...")
    
    template_content = """å¤®è§†é¢‘é“,#genre#
CCTV-1 ç»¼åˆ,http://example.com/cctv1
CCTV-2 è´¢ç»,http://example.com/cctv2
CCTV-3 ç»¼è‰º,http://example.com/cctv3
CCTV-4 ä¸­æ–‡å›½é™…,http://example.com/cctv4
CCTV-5 ä½“è‚²,http://example.com/cctv5
CCTV-5+ ä½“è‚²èµ›äº‹,http://example.com/cctv5plus
CCTV-6 ç”µå½±,http://example.com/cctv6
CCTV-7 å›½é˜²å†›äº‹,http://example.com/cctv7
CCTV-8 ç”µè§†å‰§,http://example.com/cctv8
CCTV-9 çºªå½•,http://example.com/cctv9
CCTV-10 ç§‘æ•™,http://example.com/cctv10
CCTV-11 æˆæ›²,http://example.com/cctv11
CCTV-12 ç¤¾ä¼šä¸æ³•,http://example.com/cctv12
CCTV-13 æ–°é—»,http://example.com/cctv13
CCTV-14 å°‘å„¿,http://example.com/cctv14
CCTV-15 éŸ³ä¹,http://example.com/cctv15
CCTV-16 å¥¥æ—åŒ¹å…‹,http://example.com/cctv16
CCTV-17 å†œä¸šå†œæ‘,http://example.com/cctv17

å«è§†é¢‘é“,#genre#
æ¹–å—å«è§†,http://example.com/hunan
æµ™æ±Ÿå«è§†,http://example.com/zhejiang
æ±Ÿè‹å«è§†,http://example.com/jiangsu
ä¸œæ–¹å«è§†,http://example.com/dongfang
åŒ—äº¬å«è§†,http://example.com/beijing
å¤©æ´¥å«è§†,http://example.com/tianjin
å±±ä¸œå«è§†,http://example.com/shandong
å®‰å¾½å«è§†,http://example.com/anhui
å¹¿ä¸œå«è§†,http://example.com/guangdong
æ·±åœ³å«è§†,http://example.com/shenzhen
è¾½å®å«è§†,http://example.com/liaoning
é»‘é¾™æ±Ÿå«è§†,http://example.com/heilongjiang
æ¹–åŒ—å«è§†,http://example.com/hubei
æ²³å—å«è§†,http://example.com/henan
å››å·å«è§†,http://example.com/sichuan
é‡åº†å«è§†,http://example.com/chongqing
ä¸œå—å«è§†,http://example.com/dongnan
æ±Ÿè¥¿å«è§†,http://example.com/jiangxi

å…¶ä»–é¢‘é“,#genre#
åŒ—äº¬çºªå®,http://example.com/beijingjishi
ä¸Šæµ·çºªå®,http://example.com/shanghaijishi
é‡‘é¹°å¡é€š,http://example.com/jinyingkatong
å¡é…·å°‘å„¿,http://example.com/kakushaonian
ç‚«åŠ¨å¡é€š,http://example.com/xuandongkatong"""
    
    try:
        with open(TEMPLATE_FILE, 'w', encoding='utf-8') as f:
            f.write(template_content)
        print(f"âœ… åˆ›å»ºé»˜è®¤æ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
        return True
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return False

def load_template_channels():
    """åŠ è½½æ¨¡æ¿é¢‘é“åˆ—è¡¨"""
    if not os.path.exists(TEMPLATE_FILE):
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶ {TEMPLATE_FILE} ä¸å­˜åœ¨")
        if not create_default_template():
            return []
    
    template_channels = []
    try:
        print(f"ğŸ“ æ­£åœ¨åŠ è½½æ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    template_channels.append(line)
        
        # ç»Ÿè®¡ä¿¡æ¯
        genre_lines = [line for line in template_channels if '#genre#' in line]
        channel_lines = [line for line in template_channels if '#genre#' not in line and ',' in line]
        
        print(f"âœ… æ¨¡æ¿æ–‡ä»¶åŠ è½½å®Œæˆ:")
        print(f"  - æ€»è¡Œæ•°: {len(template_channels)}")
        print(f"  - åˆ†ç»„æ•°: {len(genre_lines)}")
        print(f"  - é¢‘é“æ•°: {len(channel_lines)}")
        
        if not channel_lines:
            print("âš ï¸  è­¦å‘Š: æ¨¡æ¿æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“è¡Œ")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return []
    
    return template_channels

def load_local_sources():
    """ä¼˜å…ˆåŠ è½½æœ¬åœ°æºæ–‡ä»¶"""
    local_streams = []
    if not os.path.exists(LOCAL_SOURCE_FILE):
        print(f"âš ï¸  æœ¬åœ°æºæ–‡ä»¶ {LOCAL_SOURCE_FILE} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return local_streams
    
    try:
        print(f"ğŸ“ æ­£åœ¨ä¼˜å…ˆåŠ è½½æœ¬åœ°æºæ–‡ä»¶: {LOCAL_SOURCE_FILE}")
        with open(LOCAL_SOURCE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    local_streams.append(('local', line))  # æ ‡è®°æ¥æºä¸ºæœ¬åœ°
        print(f"âœ… æœ¬åœ°æºæ–‡ä»¶åŠ è½½å®Œæˆï¼Œå…± {len(local_streams)} ä¸ªæµ")
    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")
    
    return local_streams

def fetch_online_sources():
    """æŠ“å–åœ¨çº¿æºæ•°æ®"""
    online_streams = []
    
    def fetch_single_url(url):
        """è·å–å•ä¸ªURLçš„æºæ•°æ®"""
        try:
            print(f"ğŸŒ æ­£åœ¨æŠ“å–: {url}")
            response = requests.get(url, timeout=25, verify=False)
            response.encoding = 'utf-8'
            if response.status_code == 200:
                lines = [line.strip() for line in response.text.splitlines() if line.strip()]
                print(f"âœ… æˆåŠŸæŠ“å– {url}: {len(lines)} è¡Œ")
                return (url, lines)
            else:
                print(f"âŒ æŠ“å– {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return (url, [])
        except requests.exceptions.Timeout:
            print(f"â° æŠ“å– {url} è¶…æ—¶")
            return (url, [])
        except Exception as e:
            print(f"âŒ æŠ“å– {url} å¤±è´¥: {str(e)[:100]}...")
            return (url, [])
    
    if not URL_SOURCES:
        print("âš ï¸  æ²¡æœ‰é…ç½®åœ¨çº¿æºURL")
        return online_streams
    
    print("ğŸŒ æ­£åœ¨æŠ“å–åœ¨çº¿æº...")
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(URL_SOURCES), 6)) as executor:
            future_to_url = {executor.submit(fetch_single_url, url): url for url in URL_SOURCES}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    source_url, result = future.result()
                    # æ ‡è®°æ¥æºä¸ºåœ¨çº¿
                    online_streams.extend([(source_url, line) for line in result])
                except Exception as e:
                    print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… åœ¨çº¿æºæŠ“å–å®Œæˆï¼Œå…±è·å– {len(online_streams)} è¡Œæ•°æ®")
    except Exception as e:
        print(f"âŒ æŠ“å–åœ¨çº¿æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    return online_streams

def parse_stream_line(source, line):
    """è§£ææµæ•°æ®è¡Œï¼Œæå–é¢‘é“åç§°å’ŒURL"""
    # è·³è¿‡M3Uæ ¼å¼çš„EXTINFè¡Œ
    if line.startswith('#EXTINF'):
        return None
    
    # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
    if not line or line.startswith('#'):
        return None
    
    # å°è¯•åŒ¹é…æ ‡å‡†æ ¼å¼: é¢‘é“åç§°,URL
    match = channel_pattern.match(line)
    if match:
        channel_name = match.group(1).strip()
        url = match.group(2).strip()
        return (channel_name, url, source)
    
    # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
    if ',' in line:
        parts = line.split(',', 1)
        if len(parts) == 2 and parts[1].startswith(('http://', 'https://')):
            return (parts[0].strip(), parts[1].strip(), source)
    
    return None

def build_complete_channel_database(local_streams, online_streams):
    """
    æ„å»ºå®Œæ•´çš„é¢‘é“æ•°æ®åº“ï¼ˆåˆå¹¶æ‰€æœ‰æºï¼‰
    è¿”å›: {channel_name: [(url, source, speed_info)]}
    """
    print("ğŸ“Š æ­£åœ¨æ„å»ºå®Œæ•´é¢‘é“æ•°æ®åº“...")
    channel_db = {}
    processed_count = 0
    
    # å¤„ç†æ‰€æœ‰æµæ•°æ®ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰
    all_streams = local_streams + online_streams
    
    for source, line in all_streams:
        result = parse_stream_line(source, line)
        if result:
            channel_name, url, source_info = result
            
            # æ ‡å‡†åŒ–é¢‘é“åç§°
            channel_name = re.sub(r'\s+', ' ', channel_name).strip()
            
            if channel_name not in channel_db:
                channel_db[channel_name] = []
            
            # é¿å…é‡å¤URL
            if not any(existing_url == url for existing_url, _, _ in channel_db[channel_name]):
                channel_db[channel_name].append((url, source_info, {}))  # é¢„ç•™æµ‹é€Ÿä¿¡æ¯ä½ç½®
            
            processed_count += 1
    
    print(f"âœ… å®Œæ•´é¢‘é“æ•°æ®åº“æ„å»ºå®Œæˆ:")
    print(f"  - å¤„ç†æ•°æ®è¡Œ: {processed_count}")
    print(f"  - å”¯ä¸€é¢‘é“æ•°: {len(channel_db)}")
    print(f"  - æ€»æµæ•°é‡: {sum(len(urls) for urls in channel_db.values())}")
    
    # æ˜¾ç¤ºé¢‘é“ç»Ÿè®¡
    print("\nğŸ“ˆ é¢‘é“æ•°é‡ç»Ÿè®¡:")
    channel_counts = {}
    for channel_name, urls in channel_db.items():
        count = len(urls)
        if count not in channel_counts:
            channel_counts[count] = []
        channel_counts[count].append(channel_name)
    
    # æŒ‰æµæ•°é‡æ’åºæ˜¾ç¤º
    for count in sorted(channel_counts.keys(), reverse=True)[:10]:
        channels = channel_counts[count]
        print(f"  - {count}ä¸ªæµ: {len(channels)}ä¸ªé¢‘é“")
        if count >= 5:  # æ˜¾ç¤ºæµæ•°é‡å¤šçš„é¢‘é“ç¤ºä¾‹
            print(f"    ç¤ºä¾‹: {', '.join(channels[:3])}")
    
    return channel_db

def comprehensive_speed_test(url):
    """
    å…¨é¢æµ‹é€ŸåŠŸèƒ½ - æµ‹è¯•å“åº”æ—¶é—´å’Œè¿æ¥è´¨é‡
    è¿”å›: (is_alive, response_time_ms, download_speed_kbps, error_message)
    """
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šæµ‹è¯•å“åº”æ—¶é—´
        start_time = time.time()
        response = requests.head(url, timeout=REQUEST_TIMEOUT, verify=False, 
                               headers={'User-Agent': 'Mozilla/5.0'})
        head_time = time.time()
        response_time_ms = round((head_time - start_time) * 1000)
        
        if response.status_code != 200:
            return (False, None, None, f"HTTP {response.status_code}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šç®€å•ä¸‹è½½æµ‹è¯•
        download_speed = None
        try:
            chunk_size = 1024 * 50  # 50KB
            start_download = time.time()
            download_response = requests.get(url, timeout=SPEED_TEST_TIMEOUT, 
                                           verify=False, stream=True,
                                           headers={'User-Agent': 'Mozilla/5.0'})
            
            downloaded = 0
            for chunk in download_response.iter_content(chunk_size=chunk_size):
                if chunk:
                    downloaded += len(chunk)
                    if downloaded >= chunk_size:  # ä¸‹è½½50KBååœæ­¢
                        break
            
            end_download = time.time()
            download_time = end_download - start_download
            
            if download_time > 0.1:  # è‡³å°‘0.1ç§’æ‰è®¡ç®—é€Ÿåº¦
                download_speed = round((downloaded / 1024) / download_time)  # KB/s
            
            download_response.close()
            
        except Exception as e:
            download_speed = None
        
        return (True, response_time_ms, download_speed, None)
        
    except requests.exceptions.Timeout:
        return (False, None, None, "Timeout")
    except requests.exceptions.ConnectionError:
        return (False, None, None, "Connection Error")
    except Exception as e:
        return (False, None, None, str(e)[:50])

def speed_test_all_channels(channel_db):
    """
    å¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ
    è¿”å›: æ›´æ–°åçš„é¢‘é“æ•°æ®åº“å’Œæµ‹é€Ÿç»Ÿè®¡
    """
    print("\nğŸš€ å¼€å§‹å…¨é¢æµ‹é€Ÿ...")
    
    total_urls = sum(len(urls) for urls in channel_db.values())
    print(f"ğŸ“Š éœ€è¦æµ‹é€Ÿçš„URLæ€»æ•°: {total_urls}")
    
    # å‡†å¤‡æ‰€æœ‰éœ€è¦æµ‹é€Ÿçš„URL
    all_urls_to_test = []
    url_to_channel_map = {}
    
    for channel_name, urls in channel_db.items():
        for url, source, _ in urls:
            all_urls_to_test.append(url)
            url_to_channel_map[url] = channel_name
    
    # æµ‹é€Ÿç»Ÿè®¡
    speed_stats = {
        'total_tested': 0,
        'success_count': 0,
        'timeout_count': 0,
        'error_count': 0,
        'response_times': []
    }
    
    # ä½¿ç”¨è¿›åº¦æ¡è¿›è¡Œæµ‹é€Ÿ
    print("â±ï¸  æ­£åœ¨è¿›è¡Œå…¨é¢æµ‹é€Ÿ...")
    with tqdm(total=len(all_urls_to_test), desc="å…¨é¢æµ‹é€Ÿ", unit="URL", 
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}') as pbar:
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘æµ‹é€Ÿ
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(comprehensive_speed_test, url): url for url in all_urls_to_test}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                channel_name = url_to_channel_map[url]
                
                try:
                    is_alive, response_time, download_speed, error_msg = future.result()
                    speed_stats['total_tested'] += 1
                    
                    # æ›´æ–°é¢‘é“æ•°æ®åº“ä¸­çš„æµ‹é€Ÿä¿¡æ¯
                    for i, (stream_url, source, speed_info) in enumerate(channel_db[channel_name]):
                        if stream_url == url:
                            channel_db[channel_name][i] = (
                                stream_url, 
                                source, 
                                {
                                    'alive': is_alive,
                                    'response_time': response_time,
                                    'download_speed': download_speed,
                                    'error': error_msg,
                                    'score': calculate_stream_score(is_alive, response_time, download_speed)
                                }
                            )
                            break
                    
                    if is_alive:
                        speed_stats['success_count'] += 1
                        if response_time:
                            speed_stats['response_times'].append(response_time)
                    else:
                        if error_msg == "Timeout":
                            speed_stats['timeout_count'] += 1
                        else:
                            speed_stats['error_count'] += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix(
                        success=f"{speed_stats['success_count']}/{speed_stats['total_tested']}",
                        avg_time=f"{sum(speed_stats['response_times'])/len(speed_stats['response_times']) if speed_stats['response_times'] else 0:.0f}ms"
                    )
                    pbar.update(1)
                    
                except Exception as e:
                    pbar.update(1)
    
    # è®¡ç®—æµ‹é€Ÿç»Ÿè®¡
    if speed_stats['response_times']:
        avg_response_time = sum(speed_stats['response_times']) / len(speed_stats['response_times'])
        min_response_time = min(speed_stats['response_times'])
        max_response_time = max(speed_stats['response_times'])
    else:
        avg_response_time = min_response_time = max_response_time = 0
    
    print(f"\nâœ… å…¨é¢æµ‹é€Ÿå®Œæˆ:")
    print(f"  - æµ‹è¯•æ€»æ•°: {speed_stats['total_tested']}")
    print(f"  - æˆåŠŸ: {speed_stats['success_count']} ({speed_stats['success_count']/speed_stats['total_tested']*100:.1f}%)")
    print(f"  - è¶…æ—¶: {speed_stats['timeout_count']}")
    print(f"  - é”™è¯¯: {speed_stats['error_count']}")
    print(f"  - å¹³å‡å“åº”: {avg_response_time:.0f}ms")
    print(f"  - æœ€å¿«å“åº”: {min_response_time}ms")
    print(f"  - æœ€æ…¢å“åº”: {max_response_time}ms")
    
    return channel_db, speed_stats

def calculate_stream_score(is_alive, response_time, download_speed):
    """è®¡ç®—æµè´¨é‡ç»¼åˆè¯„åˆ†"""
    if not is_alive:
        return 0
    
    score = 0
    
    # å“åº”æ—¶é—´è¯„åˆ† (0-60åˆ†)
    if response_time:
        if response_time <= 100:
            score += 60
        elif response_time <= 300:
            score += 50
        elif response_time <= 500:
            score += 40
        elif response_time <= 1000:
            score += 30
        elif response_time <= 2000:
            score += 20
        else:
            score += 10
    
    # ä¸‹è½½é€Ÿåº¦è¯„åˆ† (0-40åˆ†)
    if download_speed:
        if download_speed >= 1000:
            score += 40
        elif download_speed >= 500:
            score += 30
        elif download_speed >= 200:
            score += 20
        elif download_speed >= 100:
            score += 10
        else:
            score += 5
    
    return score

def generate_channel_stats(channel_db):
    """ç”Ÿæˆé¢‘é“ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“ˆ ç”Ÿæˆé¢‘é“ç»Ÿè®¡ä¿¡æ¯...")
    
    stats = {
        'total_channels': len(channel_db),
        'total_streams': sum(len(urls) for urls in channel_db.values()),
        'channels_by_stream_count': {},
        'channels_with_speed_info': {},
        'top_channels': []
    }
    
    # ç»Ÿè®¡æ¯ä¸ªé¢‘é“çš„æµæ•°é‡
    for channel_name, urls in channel_db.items():
        stream_count = len(urls)
        alive_count = sum(1 for _, _, info in urls if info.get('alive', False))
        
        if stream_count not in stats['channels_by_stream_count']:
            stats['channels_by_stream_count'][stream_count] = 0
        stats['channels_by_stream_count'][stream_count] += 1
        
        stats['channels_with_speed_info'][channel_name] = {
            'total_streams': stream_count,
            'alive_streams': alive_count,
            'alive_ratio': alive_count / stream_count if stream_count > 0 else 0,
            'best_response_time': min((info.get('response_time', 9999) for _, _, info in urls if info.get('alive', False)), default=0),
            'urls': [{'url': url, 'source': source, 'speed_info': info} for url, source, info in urls]
        }
    
    # ç”ŸæˆTOPé¢‘é“åˆ—è¡¨ï¼ˆæŒ‰æµæ•°é‡æ’åºï¼‰
    sorted_channels = sorted(stats['channels_with_speed_info'].items(), 
                           key=lambda x: x[1]['total_streams'], reverse=True)
    
    stats['top_channels'] = sorted_channels[:20]  # å‰20ä¸ªé¢‘é“
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶
    try:
        with open(OUTPUT_CHANNEL_STATS, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… é¢‘é“ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {OUTPUT_CHANNEL_STATS}")
    except Exception as e:
        print(f"âŒ ä¿å­˜é¢‘é“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    print(f"\nğŸ“Š é¢‘é“ç»Ÿè®¡æ‘˜è¦:")
    print(f"  - æ€»é¢‘é“æ•°: {stats['total_channels']}")
    print(f"  - æ€»æµæ•°é‡: {stats['total_streams']}")
    print(f"  - å¹³å‡æ¯ä¸ªé¢‘é“æµæ•°: {stats['total_streams']/stats['total_channels']:.1f}")
    
    # æ˜¾ç¤ºæµæ•°é‡åˆ†å¸ƒ
    print(f"\nğŸ“‹ æµæ•°é‡åˆ†å¸ƒ:")
    for count in sorted(stats['channels_by_stream_count'].keys(), reverse=True)[:10]:
        if count >= 2:  # åªæ˜¾ç¤ºæœ‰2ä¸ªä»¥ä¸Šæµçš„é¢‘é“
            print(f"  - {count}ä¸ªæµ: {stats['channels_by_stream_count'][count]}ä¸ªé¢‘é“")
    
    # æ˜¾ç¤ºTOPé¢‘é“
    print(f"\nğŸ† TOPé¢‘é“ (æŒ‰æµæ•°é‡):")
    for i, (channel, info) in enumerate(stats['top_channels'][:10], 1):
        print(f"  {i:2d}. {channel}: {info['total_streams']}ä¸ªæµ ({info['alive_streams']}ä¸ªæœ‰æ•ˆ)")
    
    return stats

def match_template_channels(template_channels, channel_db):
    """åŒ¹é…æ¨¡æ¿é¢‘é“å¹¶é€‰æ‹©æœ€ä½³æµ"""
    print("\nğŸ¯ å¼€å§‹æ¨¡æ¿é¢‘é“åŒ¹é…...")
    
    txt_lines = []
    m3u_lines = ['#EXTM3U']
    current_group = "é»˜è®¤åˆ†ç»„"
    matched_count = 0
    
    for line in template_channels:
        # å¤„ç†åˆ†ç»„è¡Œ
        if '#genre#' in line:
            group_name = line.replace(',#genre#', '').strip()
            current_group = group_name
            txt_lines.append(line)
            continue
        
        # å¤„ç†é¢‘é“è¡Œ
        if ',' in line:
            parts = line.split(',', 1)
            if len(parts) == 2:
                template_channel = parts[0].strip()
                template_url = parts[1].strip()
                
                # æŸ¥æ‰¾åŒ¹é…çš„é¢‘é“
                matched_urls = []
                for db_channel, urls in channel_db.items():
                    if is_channel_match(template_channel, db_channel):
                        # åªé€‰æ‹©æœ‰æ•ˆçš„æµ
                        valid_urls = [(url, source, info) for url, source, info in urls 
                                    if info.get('alive', False)]
                        matched_urls.extend(valid_urls)
                
                if matched_urls:
                    # æŒ‰è¯„åˆ†æ’åºå¹¶é€‰æ‹©æœ€ä½³æµ
                    matched_urls.sort(key=lambda x: x[2].get('score', 0), reverse=True)
                    best_urls = matched_urls[:MAX_STREAMS_PER_CHANNEL]
                    
                    # æ·»åŠ åˆ°è¾“å‡º
                    for url, source, info in best_urls:
                        speed_info = format_speed_info(info)
                        txt_lines.append(f"{template_channel}{speed_info},{url}")
                        m3u_lines.append(f'#EXTINF:-1 group-title="{current_group}",{template_channel}{speed_info}')
                        m3u_lines.append(url)
                    
                    matched_count += 1
                    print(f"  âœ… {template_channel}: æ‰¾åˆ° {len(best_urls)} ä¸ªä¼˜è´¨æµ")
                else:
                    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æœ‰æ•ˆæµï¼Œä½¿ç”¨æ¨¡æ¿URL
                    txt_lines.append(line)
                    m3u_lines.append(f'#EXTINF:-1 group-title="{current_group}",{template_channel}')
                    m3u_lines.append(template_url)
                    print(f"  âŒ {template_channel}: æœªæ‰¾åˆ°æœ‰æ•ˆæµ")
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    try:
        with open(OUTPUT_TXT, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        print(f"âœ… ç”ŸæˆTXTæ–‡ä»¶: {OUTPUT_TXT}ï¼Œå…± {len(txt_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥TXTæ–‡ä»¶å¤±è´¥: {e}")
    
    try:
        with open(OUTPUT_M3U, 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))
        print(f"âœ… ç”ŸæˆM3Uæ–‡ä»¶: {OUTPUT_M3U}ï¼Œå…± {len(m3u_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥M3Uæ–‡ä»¶å¤±è´¥: {e}")
    
    print(f"ğŸ¯ æ¨¡æ¿åŒ¹é…å®Œæˆ: {matched_count} ä¸ªé¢‘é“åŒ¹é…æˆåŠŸ")
    return matched_count

def is_channel_match(template_channel, db_channel):
    """åˆ¤æ–­é¢‘é“æ˜¯å¦åŒ¹é…"""
    template_lower = template_channel.lower()
    db_lower = db_channel.lower()
    
    # å¤šç§åŒ¹é…ç­–ç•¥
    match_strategies = [
        template_lower in db_lower,
        db_lower in template_lower,
        template_lower.replace(' ', '') in db_lower.replace(' ', ''),
        template_lower.replace('cctv-', 'cctv') in db_lower.replace('cctv-', 'cctv'),
        any(word in db_lower for word in template_lower.split() if len(word) > 2)
    ]
    
    return any(match_strategies)

def format_speed_info(speed_info):
    """æ ¼å¼åŒ–æµ‹é€Ÿä¿¡æ¯"""
    if not speed_info.get('alive', False):
        return " | æ— æ•ˆ"
    
    parts = []
    if speed_info.get('response_time'):
        parts.append(f"{speed_info['response_time']}ms")
    if speed_info.get('download_speed'):
        parts.append(f"{speed_info['download_speed']}KB/s")
    
    if parts:
        return " | " + " ".join(parts)
    else:
        return " | æœ‰æ•ˆ"

def main():
    """ä¸»å‡½æ•° - æŒ‰ç…§æ–°æµç¨‹æ‰§è¡Œ"""
    print("ğŸ¬ IPTVé¢‘é“æ•´ç†å·¥å…·å¼€å§‹è¿è¡Œ...")
    start_time = time.time()
    
    # 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°æº
    print("\n" + "="*50)
    print("æ­¥éª¤1: ä¼˜å…ˆåŠ è½½æœ¬åœ°æº")
    local_streams = load_local_sources()
    
    # 2. æŠ“å–åœ¨çº¿æº
    print("\n" + "="*50)
    print("æ­¥éª¤2: æŠ“å–åœ¨çº¿æº")
    online_streams = fetch_online_sources()
    
    # 3. åˆå¹¶æ‰€æœ‰æºæ„å»ºå®Œæ•´æ•°æ®åº“
    print("\n" + "="*50)
    print("æ­¥éª¤3: åˆå¹¶æ‰€æœ‰æºæ„å»ºå®Œæ•´æ•°æ®åº“")
    channel_db = build_complete_channel_database(local_streams, online_streams)
    
    # 4. å¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ
    print("\n" + "="*50)
    print("æ­¥éª¤4: å…¨é¢æµ‹é€Ÿå’Œå»¶æ—¶æµ‹è¯•")
    channel_db, speed_stats = speed_test_all_channels(channel_db)
    
    # 5. ç”Ÿæˆé¢‘é“ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*50)
    print("æ­¥éª¤5: ç”Ÿæˆé¢‘é“ç»Ÿè®¡ä¿¡æ¯")
    channel_stats = generate_channel_stats(channel_db)
    
    # 6. åŠ è½½æ¨¡æ¿å¹¶è¿›è¡ŒåŒ¹é…
    print("\n" + "="*50)
    print("æ­¥éª¤6: æ¨¡æ¿é¢‘é“åŒ¹é…")
    template_channels = load_template_channels()
    if template_channels:
        matched_count = match_template_channels(template_channels, channel_db)
    else:
        matched_count = 0
        print("âŒ æ— æ³•åŠ è½½æ¨¡æ¿ï¼Œè·³è¿‡åŒ¹é…")
    
    # æœ€ç»ˆç»Ÿè®¡
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print("\n" + "="*60)
    print("ğŸ‰ æ‰§è¡Œå®Œæˆ!")
    print("="*60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time} ç§’")
    print(f"  ğŸ“º æ€»é¢‘é“æ•°: {channel_stats['total_channels']}")
    print(f"  ğŸ”— æ€»æµæ•°é‡: {channel_stats['total_streams']}")
    print(f"  âœ… æœ‰æ•ˆæµæ•°é‡: {speed_stats['success_count']}")
    print(f"  ğŸ¯ æ¨¡æ¿åŒ¹é…: {matched_count} ä¸ªé¢‘é“")
    print(f"  ğŸ“ˆ å¹³å‡å“åº”: {sum(speed_stats['response_times'])/len(speed_stats['response_times']) if speed_stats['response_times'] else 0:.0f}ms")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {OUTPUT_TXT} (é¢‘é“åˆ—è¡¨)")
    print(f"  - {OUTPUT_M3U} (M3Uæ’­æ”¾åˆ—è¡¨)")
    print(f"  - {OUTPUT_CHANNEL_STATS} (é¢‘é“ç»Ÿè®¡)")
    print(f"  - {OUTPUT_SPEED_TEST} (æµ‹é€Ÿç»“æœ)")
    print("="*60)

if __name__ == "__main__":
    # ç¦ç”¨SSLè­¦å‘Š
    requests.packages.urllib3.disable_warnings()
    
    main()
