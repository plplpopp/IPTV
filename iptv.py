import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
from urllib.parse import urlparse
from tqdm import tqdm
import sys

# ============================ é…ç½®æ–‡ä»¶ ============================
# æºURLåˆ—è¡¨
URL_SOURCES = [
    "https://raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/gd/output/ipv4/result.m3u",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt",
    "http://47.120.41.246:8899/zb.txt",
]

# æœ¬åœ°æºæ–‡ä»¶
LOCAL_SOURCE_FILE = "local.txt"

# æ¨¡æ¿é¢‘é“æ–‡ä»¶
TEMPLATE_FILE = "demo.txt"

# è¾“å‡ºæ–‡ä»¶
OUTPUT_TXT = "iptv.txt"
OUTPUT_M3U = "iptv.m3u"

# æ¯ä¸ªé¢‘é“ä¿ç•™çš„æ¥å£æ•°é‡
MAX_STREAMS_PER_CHANNEL = 5

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 8

# æµ‹é€Ÿè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SPEED_TEST_TIMEOUT = 12

# æœ€å¤§çº¿ç¨‹æ•°
MAX_WORKERS = 20

# ============================ æ­£åˆ™è¡¨è¾¾å¼ ============================
# IPv4åœ°å€åŒ¹é…
ipv4_pattern = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')

# é¢‘é“åç§°å’ŒURLåŒ¹é…
channel_pattern = re.compile(r"^([^,]+?),\s*(https?://.+)", re.IGNORECASE)

# M3Uæ ¼å¼è§£æ
extinf_pattern = re.compile(r'tvg-name="([^"]*)"', re.IGNORECASE)
extinf_name_pattern = re.compile(r'#EXTINF:.*?,(.+)', re.IGNORECASE)

def create_correct_template():
    """åˆ›å»ºæ­£ç¡®çš„æ¨¡æ¿æ–‡ä»¶æ ¼å¼ï¼ˆåªæœ‰é¢‘é“åç§°ï¼‰"""
    print("ğŸ“ åˆ›å»ºæ­£ç¡®çš„æ¨¡æ¿æ–‡ä»¶æ ¼å¼...")
    
    template_content = """å¤®è§†é¢‘é“,#genre#
CCTV-1
CCTV-2
CCTV-3
CCTV-4
CCTV-5
CCTV-5+
CCTV-6
CCTV-7
CCTV-8
CCTV-9
CCTV-10
CCTV-11
CCTV-12
CCTV-13
CCTV-14
CCTV-15
CCTV-16
CCTV-17

å«è§†é¢‘é“,#genre#
å®‰å¾½å«è§†
å¹¿ä¸œå«è§†
æµ™æ±Ÿå«è§†
æ¹–å—å«è§†
åŒ—äº¬å«è§†
æ¹–åŒ—å«è§†
é»‘é¾™æ±Ÿå«è§†
é‡åº†å«è§†
ä¸œæ–¹å«è§†
ä¸œå—å«è§†
ç”˜è‚ƒå«è§†
å‡¤å‡°å«è§†
å¹¿è¥¿å«è§†
è´µå·å«è§†
æµ·å—å«è§†
æ²³åŒ—å«è§†
æ²³å—å«è§†
æ±Ÿè‹å«è§†
æ±Ÿè¥¿å«è§†
å‰æ—å«è§†
è¾½å®å«è§†
å†…è’™å¤å«è§†
å®å¤å«è§†
é’æµ·å«è§†
å±±ä¸œå«è§†
å±±è¥¿å«è§†
é™•è¥¿å«è§†
å››å·å«è§†
å¤©æ´¥å«è§†
è¥¿è—å«è§†
æ–°ç–†å«è§†
äº‘å—å«è§†

å…¶ä»–é¢‘é“,#genre#
å®‰å¾½å›½é™…
å®‰å¾½å½±è§†
ç¬¬ä¸€è´¢ç»
æ¢¨å›­é¢‘é“"""
    
    try:
        with open(TEMPLATE_FILE, 'w', encoding='utf-8') as f:
            f.write(template_content)
        print(f"âœ… åˆ›å»ºæ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
        return True
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return False

def clean_channel_name(channel_name):
    """
    æ¸…ç†é¢‘é“åç§°ï¼Œå»é™¤ä¸éœ€è¦çš„åç¼€
    """
    # å»é™¤å¸¸è§çš„åç¼€
    suffixes = ['ç»¼åˆ', 'é«˜æ¸…', 'è¶…æ¸…', 'æ ‡æ¸…', 'HD', 'FHD', '4K', 'ç›´æ’­', 'é¢‘é“', 'å«è§†å°']
    pattern = r'[\(ï¼ˆ].*?[\)ï¼‰]|\s*-\s*.*$|\s*â€“\s*.*$'
    
    cleaned_name = channel_name.strip()
    
    # å»é™¤æ‹¬å·å†…å®¹
    cleaned_name = re.sub(pattern, '', cleaned_name)
    
    # å»é™¤åç¼€
    for suffix in suffixes:
        cleaned_name = cleaned_name.replace(suffix, '').strip()
    
    # å»é™¤å¤šä½™ç©ºæ ¼
    cleaned_name = re.sub(r'\s+', ' ', cleaned_name).strip()
    
    return cleaned_name

def format_channel_name_for_output(template_channel):
    """
    æ ¼å¼åŒ–è¾“å‡ºç”¨çš„é¢‘é“åç§°ï¼Œç¡®ä¿æ˜¾ç¤ºå®Œæ•´çš„æ ‡å‡†åç§°
    """
    # ä¿æŒæ¨¡æ¿ä¸­çš„åŸå§‹åç§°ï¼Œä¸åšæ¸…ç†
    return template_channel.strip()

def load_template_channels():
    """åŠ è½½æ¨¡æ¿é¢‘é“åˆ—è¡¨ï¼ˆåªæœ‰é¢‘é“åç§°ï¼‰"""
    if not os.path.exists(TEMPLATE_FILE):
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶ {TEMPLATE_FILE} ä¸å­˜åœ¨")
        if not create_correct_template():
            return []
    
    template_channels = []
    template_channel_names = []  # åªå­˜å‚¨é¢‘é“åç§°
    
    try:
        print(f"ğŸ“ æ­£åœ¨åŠ è½½æ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    template_channels.append(line)
                    
                    # åªæå–é¢‘é“åç§°ï¼ˆéåˆ†ç»„è¡Œï¼‰
                    if '#genre#' not in line and ',' not in line:
                        template_channel_names.append(line)
        
        # ç»Ÿè®¡ä¿¡æ¯
        genre_lines = [line for line in template_channels if '#genre#' in line]
        
        print(f"âœ… æ¨¡æ¿æ–‡ä»¶åŠ è½½å®Œæˆ:")
        print(f"  - æ€»è¡Œæ•°: {len(template_channels)}")
        print(f"  - åˆ†ç»„æ•°: {len(genre_lines)}")
        print(f"  - é¢‘é“æ•°: {len(template_channel_names)}")
        
        if not template_channel_names:
            print("âŒ æ¨¡æ¿ä¸­æ²¡æœ‰æœ‰æ•ˆçš„é¢‘é“åç§°")
            return []
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé¢‘é“åç§°ä½œä¸ºç¤ºä¾‹
        print("é¢‘é“åç§°ç¤ºä¾‹:")
        for i, channel in enumerate(template_channel_names[:8], 1):
            print(f"  {i}. {channel}")
        
        return template_channels
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return []

def load_local_sources():
    """ä¼˜å…ˆåŠ è½½æœ¬åœ°æºæ–‡ä»¶"""
    local_streams = []
    if not os.path.exists(LOCAL_SOURCE_FILE):
        print(f"âš ï¸  æœ¬åœ°æºæ–‡ä»¶ {LOCAL_SOURCE_FILE} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return local_streams
    
    try:
        print(f"ğŸ“ æ­£åœ¨ä¼˜å…ˆåŠ è½½æœ¬åœ°æºæ–‡ä»¶: {LOCAL_SOURCE_FILE}")
        with open(LOCAL_SOURCE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    local_streams.append(('local', line))
        print(f"âœ… æœ¬åœ°æºæ–‡ä»¶åŠ è½½å®Œæˆï¼Œå…± {len(local_streams)} ä¸ªæµ")
    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")
    
    return local_streams

def fetch_online_sources():
    """æŠ“å–åœ¨çº¿æºæ•°æ®"""
    online_streams = []
    
    def fetch_single_url(url):
        """è·å–å•ä¸ªURLçš„æºæ•°æ®"""
        try:
            print(f"ğŸŒ æ­£åœ¨æŠ“å–: {url}")
            response = requests.get(url, timeout=25, verify=False)
            response.encoding = 'utf-8'
            if response.status_code == 200:
                lines = [line.strip() for line in response.text.splitlines() if line.strip()]
                print(f"âœ… æˆåŠŸæŠ“å– {url}: {len(lines)} è¡Œ")
                return (url, lines)
            else:
                print(f"âŒ æŠ“å– {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return (url, [])
        except requests.exceptions.Timeout:
            print(f"â° æŠ“å– {url} è¶…æ—¶")
            return (url, [])
        except Exception as e:
            print(f"âŒ æŠ“å– {url} å¤±è´¥: {str(e)[:100]}...")
            return (url, [])
    
    if not URL_SOURCES:
        print("âš ï¸  æ²¡æœ‰é…ç½®åœ¨çº¿æºURL")
        return online_streams
    
    print("ğŸŒ æ­£åœ¨æŠ“å–åœ¨çº¿æº...")
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(URL_SOURCES), 6)) as executor:
            future_to_url = {executor.submit(fetch_single_url, url): url for url in URL_SOURCES}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    source_url, result = future.result()
                    online_streams.extend([(source_url, line) for line in result])
                except Exception as e:
                    print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… åœ¨çº¿æºæŠ“å–å®Œæˆï¼Œå…±è·å– {len(online_streams)} è¡Œæ•°æ®")
    except Exception as e:
        print(f"âŒ æŠ“å–åœ¨çº¿æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    return online_streams

def parse_stream_line(source, line):
    """è§£ææµæ•°æ®è¡Œï¼Œæå–é¢‘é“åç§°å’ŒURL"""
    # è·³è¿‡M3Uæ ¼å¼çš„EXTINFè¡Œ
    if line.startswith('#EXTINF'):
        return None
    
    # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
    if not line or line.startswith('#'):
        return None
    
    # å°è¯•åŒ¹é…æ ‡å‡†æ ¼å¼: é¢‘é“åç§°,URL
    match = channel_pattern.match(line)
    if match:
        channel_name = match.group(1).strip()
        url = match.group(2).strip()
        return (channel_name, url, source)
    
    # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
    if ',' in line:
        parts = line.split(',', 1)
        if len(parts) == 2 and parts[1].startswith(('http://', 'https://')):
            return (parts[0].strip(), parts[1].strip(), source)
    
    return None

def build_complete_channel_database(local_streams, online_streams):
    """æ„å»ºå®Œæ•´çš„é¢‘é“æ•°æ®åº“"""
    print("ğŸ“Š æ­£åœ¨æ„å»ºå®Œæ•´é¢‘é“æ•°æ®åº“...")
    channel_db = {}
    processed_count = 0
    
    all_streams = local_streams + online_streams
    
    for source, line in all_streams:
        result = parse_stream_line(source, line)
        if result:
            channel_name, url, source_info = result
            # æ¸…ç†é¢‘é“åç§°ç”¨äºåŒ¹é…
            cleaned_name = clean_channel_name(channel_name)
            
            if cleaned_name not in channel_db:
                channel_db[cleaned_name] = []
            
            if not any(existing_url == url for existing_url, _, _ in channel_db[cleaned_name]):
                channel_db[cleaned_name].append((url, source_info, {}))
            processed_count += 1
    
    print(f"âœ… å®Œæ•´é¢‘é“æ•°æ®åº“æ„å»ºå®Œæˆ:")
    print(f"  - å¤„ç†æ•°æ®è¡Œ: {processed_count}")
    print(f"  - å”¯ä¸€é¢‘é“æ•°: {len(channel_db)}")
    print(f"  - æ€»æµæ•°é‡: {sum(len(urls) for urls in channel_db.values())}")
    
    # æ˜¾ç¤ºé¢‘é“ç»Ÿè®¡
    print("\nğŸ“ˆ é¢‘é“æ•°é‡ç»Ÿè®¡:")
    channel_counts = {}
    for channel_name, urls in channel_db.items():
        count = len(urls)
        if count not in channel_counts:
            channel_counts[count] = []
        channel_counts[count].append(channel_name)
    
    for count in sorted(channel_counts.keys(), reverse=True)[:10]:
        channels = channel_counts[count]
        print(f"  - {count}ä¸ªæµ: {len(channels)}ä¸ªé¢‘é“")
        if count >= 3:
            print(f"    ç¤ºä¾‹: {', '.join(channels[:2])}")
    
    return channel_db

def comprehensive_speed_test(url):
    """å…¨é¢æµ‹é€ŸåŠŸèƒ½"""
    try:
        start_time = time.time()
        response = requests.head(url, timeout=REQUEST_TIMEOUT, verify=False, 
                               headers={'User-Agent': 'Mozilla/5.0'})
        head_time = time.time()
        response_time_ms = round((head_time - start_time) * 1000)
        
        if response.status_code != 200:
            return (False, None, None, f"HTTP {response.status_code}")
        
        download_speed = None
        try:
            chunk_size = 1024 * 50
            start_download = time.time()
            download_response = requests.get(url, timeout=SPEED_TEST_TIMEOUT, 
                                           verify=False, stream=True,
                                           headers={'User-Agent': 'Mozilla/5.0'})
            
            downloaded = 0
            for chunk in download_response.iter_content(chunk_size=chunk_size):
                if chunk:
                    downloaded += len(chunk)
                    if downloaded >= chunk_size:
                        break
            
            end_download = time.time()
            download_time = end_download - start_download
            
            if download_time > 0.1:
                download_speed = round((downloaded / 1024) / download_time)
            
            download_response.close()
            
        except Exception as e:
            download_speed = None
        
        return (True, response_time_ms, download_speed, None)
        
    except requests.exceptions.Timeout:
        return (False, None, None, "Timeout")
    except requests.exceptions.ConnectionError:
        return (False, None, None, "Connection Error")
    except Exception as e:
        return (False, None, None, str(e)[:50])

def speed_test_all_channels(channel_db):
    """å¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ"""
    print("\nğŸš€ å¼€å§‹å…¨é¢æµ‹é€Ÿ...")
    
    total_urls = sum(len(urls) for urls in channel_db.values())
    print(f"ğŸ“Š éœ€è¦æµ‹é€Ÿçš„URLæ€»æ•°: {total_urls}")
    
    all_urls_to_test = []
    url_to_channel_map = {}
    
    for channel_name, urls in channel_db.items():
        for url, source, _ in urls:
            all_urls_to_test.append(url)
            url_to_channel_map[url] = channel_name
    
    speed_stats = {
        'total_tested': 0,
        'success_count': 0,
        'timeout_count': 0,
        'error_count': 0,
        'response_times': []
    }
    
    print("â±ï¸  æ­£åœ¨è¿›è¡Œå…¨é¢æµ‹é€Ÿ...")
    with tqdm(total=len(all_urls_to_test), desc="å…¨é¢æµ‹é€Ÿ", unit="URL", 
              bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}') as pbar:
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(comprehensive_speed_test, url): url for url in all_urls_to_test}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                channel_name = url_to_channel_map[url]
                
                try:
                    is_alive, response_time, download_speed, error_msg = future.result()
                    speed_stats['total_tested'] += 1
                    
                    for i, (stream_url, source, speed_info) in enumerate(channel_db[channel_name]):
                        if stream_url == url:
                            channel_db[channel_name][i] = (
                                stream_url, 
                                source, 
                                {
                                    'alive': is_alive,
                                    'response_time': response_time,
                                    'download_speed': download_speed,
                                    'error': error_msg,
                                    'score': calculate_stream_score(is_alive, response_time, download_speed)
                                }
                            )
                            break
                    
                    if is_alive:
                        speed_stats['success_count'] += 1
                        if response_time:
                            speed_stats['response_times'].append(response_time)
                    else:
                        if error_msg == "Timeout":
                            speed_stats['timeout_count'] += 1
                        else:
                            speed_stats['error_count'] += 1
                    
                    pbar.set_postfix(
                        success=f"{speed_stats['success_count']}/{speed_stats['total_tested']}",
                        avg_time=f"{sum(speed_stats['response_times'])/len(speed_stats['response_times']) if speed_stats['response_times'] else 0:.0f}ms"
                    )
                    pbar.update(1)
                    
                except Exception as e:
                    pbar.update(1)
    
    if speed_stats['response_times']:
        avg_response_time = sum(speed_stats['response_times']) / len(speed_stats['response_times'])
        min_response_time = min(speed_stats['response_times'])
        max_response_time = max(speed_stats['response_times'])
    else:
        avg_response_time = min_response_time = max_response_time = 0
    
    print(f"\nâœ… å…¨é¢æµ‹é€Ÿå®Œæˆ:")
    print(f"  - æµ‹è¯•æ€»æ•°: {speed_stats['total_tested']}")
    print(f"  - æˆåŠŸ: {speed_stats['success_count']} ({speed_stats['success_count']/speed_stats['total_tested']*100:.1f}%)")
    print(f"  - è¶…æ—¶: {speed_stats['timeout_count']}")
    print(f"  - é”™è¯¯: {speed_stats['error_count']}")
    print(f"  - å¹³å‡å“åº”: {avg_response_time:.0f}ms")
    print(f"  - æœ€å¿«å“åº”: {min_response_time}ms")
    print(f"  - æœ€æ…¢å“åº”: {max_response_time}ms")
    
    return channel_db, speed_stats

def calculate_stream_score(is_alive, response_time, download_speed):
    """è®¡ç®—æµè´¨é‡ç»¼åˆè¯„åˆ†"""
    if not is_alive:
        return 0
    
    score = 0
    
    if response_time:
        if response_time <= 100:
            score += 60
        elif response_time <= 300:
            score += 50
        elif response_time <= 500:
            score += 40
        elif response_time <= 1000:
            score += 30
        elif response_time <= 2000:
            score += 20
        else:
            score += 10
    
    if download_speed:
        if download_speed >= 1000:
            score += 40
        elif download_speed >= 500:
            score += 30
        elif download_speed >= 200:
            score += 20
        elif download_speed >= 100:
            score += 10
        else:
            score += 5
    
    return score

def is_channel_match(template_channel, db_channel):
    """
    ç²¾å‡†åŒ¹é…é¢‘é“åç§°ï¼Œç‰¹åˆ«æ˜¯CCTVé¢‘é“
    """
    template_lower = template_channel.lower().strip()
    db_lower = db_channel.lower().strip()
    
    # å®Œå…¨åŒ¹é…
    if template_lower == db_lower:
        return True
    
    # å¯¹äºCCTVé¢‘é“è¿›è¡Œç²¾å‡†åŒ¹é…
    if 'cctv' in template_lower and 'cctv' in db_lower:
        # æå–CCTVæ•°å­—éƒ¨åˆ†
        template_nums = re.findall(r'cctv[-\s]*(\d+\+?)', template_lower)
        db_nums = re.findall(r'cctv[-\s]*(\d+\+?)', db_lower)
        
        if template_nums and db_nums:
            # æ•°å­—éƒ¨åˆ†å®Œå…¨åŒ¹é…
            if template_nums[0] == db_nums[0]:
                return True
        
        # å¤„ç†CCTV-5+ç­‰ç‰¹æ®Šæƒ…å†µ
        if 'cctv-5+' in template_lower and any(x in db_lower for x in ['cctv5+', 'cctv-5+', 'cctv5plus']):
            return True
        if 'cctv5+' in template_lower and any(x in db_lower for x in ['cctv5+', 'cctv-5+', 'cctv5plus']):
            return True
    
    # å¯¹äºå«è§†é¢‘é“è¿›è¡Œç²¾å‡†åŒ¹é…
    if 'å«è§†' in template_channel and 'å«è§†' in db_channel:
        template_province = template_channel.replace('å«è§†', '').strip()
        db_province = db_channel.replace('å«è§†', '').strip()
        if template_province == db_province:
            return True
        # å¤„ç†ç®€ç§°åŒ¹é…
        if template_province in db_province or db_province in template_province:
            return True
    
    # å…¶ä»–é¢‘é“çš„å®½æ¾åŒ¹é…
    template_no_space = template_lower.replace(' ', '').replace('-', '')
    db_no_space = db_lower.replace(' ', '').replace('-', '')
    
    if template_no_space in db_no_space or db_no_space in template_no_space:
        return True
    
    return False

def find_matching_channels(template_channel, channel_db):
    """æŸ¥æ‰¾åŒ¹é…çš„é¢‘é“"""
    matched_urls = []
    
    for db_channel, urls in channel_db.items():
        if is_channel_match(template_channel, db_channel):
            valid_urls = [(url, source, info) for url, source, info in urls 
                        if info.get('alive', False)]
            matched_urls.extend(valid_urls)
    
    return matched_urls

def match_template_channels(template_channels, channel_db):
    """åŒ¹é…æ¨¡æ¿é¢‘é“å¹¶é€‰æ‹©æœ€ä½³æµ"""
    print("\nğŸ¯ å¼€å§‹æ¨¡æ¿é¢‘é“åŒ¹é…...")
    
    txt_lines = []
    m3u_lines = ['#EXTM3U']
    current_group = "é»˜è®¤åˆ†ç»„"
    matched_count = 0
    
    for line in template_channels:
        if '#genre#' in line:
            group_name = line.replace(',#genre#', '').strip()
            current_group = group_name
            txt_lines.append(line)
            continue
        
        # æ¨¡æ¿è¡Œåªæœ‰é¢‘é“åç§°ï¼ˆæ²¡æœ‰URLï¼‰
        if line and not line.endswith('#genre#'):
            # ä½¿ç”¨åŸå§‹æ¨¡æ¿åç§°ï¼Œä¸è¿›è¡Œæ¸…ç†
            template_channel_original = line
            # ç”¨äºåŒ¹é…çš„æ¸…ç†ååç§°
            template_channel_for_match = clean_channel_name(line)
            
            print(f"  ğŸ” æŸ¥æ‰¾é¢‘é“: {template_channel_original}")
            
            matched_urls = find_matching_channels(template_channel_for_match, channel_db)
            
            if matched_urls:
                matched_urls.sort(key=lambda x: x[2].get('score', 0), reverse=True)
                best_urls = matched_urls[:MAX_STREAMS_PER_CHANNEL]
                
                for url, source, info in best_urls:
                    # ä½¿ç”¨åŸå§‹æ¨¡æ¿åç§°è¾“å‡ºï¼Œç¡®ä¿æ˜¾ç¤ºå®Œæ•´çš„"CCTV-1"ç­‰åç§°
                    output_channel_name = format_channel_name_for_output(template_channel_original)
                    txt_lines.append(f"{output_channel_name},{url}")
                    m3u_lines.append(f'#EXTINF:-1 group-title="{current_group}",{output_channel_name}')
                    m3u_lines.append(url)
                
                matched_count += 1
                print(f"  âœ… {template_channel_original}: æ‰¾åˆ° {len(best_urls)} ä¸ªä¼˜è´¨æµ")
            else:
                print(f"  âŒ {template_channel_original}: æœªæ‰¾åˆ°æœ‰æ•ˆæµ")
    
    try:
        with open(OUTPUT_TXT, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        print(f"âœ… ç”ŸæˆTXTæ–‡ä»¶: {OUTPUT_TXT}ï¼Œå…± {len(txt_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥TXTæ–‡ä»¶å¤±è´¥: {e}")
    
    try:
        with open(OUTPUT_M3U, 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))
        print(f"âœ… ç”ŸæˆM3Uæ–‡ä»¶: {OUTPUT_M3U}ï¼Œå…± {len(m3u_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥M3Uæ–‡ä»¶å¤±è´¥: {e}")
    
    print(f"ğŸ¯ æ¨¡æ¿åŒ¹é…å®Œæˆ: {matched_count} ä¸ªé¢‘é“åŒ¹é…æˆåŠŸ")
    return matched_count

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ IPTVé¢‘é“æ•´ç†å·¥å…·å¼€å§‹è¿è¡Œ...")
    start_time = time.time()
    
    # 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°æº
    print("\n" + "="*50)
    print("æ­¥éª¤1: ä¼˜å…ˆåŠ è½½æœ¬åœ°æº")
    local_streams = load_local_sources()
    
    # 2. æŠ“å–åœ¨çº¿æº
    print("\n" + "="*50)
    print("æ­¥éª¤2: æŠ“å–åœ¨çº¿æº")
    online_streams = fetch_online_sources()
    
    # 3. åˆå¹¶æ‰€æœ‰æºæ„å»ºå®Œæ•´æ•°æ®åº“
    print("\n" + "="*50)
    print("æ­¥éª¤3: åˆå¹¶æ‰€æœ‰æºæ„å»ºå®Œæ•´æ•°æ®åº“")
    channel_db = build_complete_channel_database(local_streams, online_streams)
    
    # 4. å¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ
    print("\n" + "="*50)
    print("æ­¥éª¤4: å…¨é¢æµ‹é€Ÿå’Œå»¶æ—¶æµ‹è¯•")
    channel_db, speed_stats = speed_test_all_channels(channel_db)
    
    # 5. åŠ è½½æ¨¡æ¿å¹¶è¿›è¡ŒåŒ¹é…
    print("\n" + "="*50)
    print("æ­¥éª¤5: æ¨¡æ¿é¢‘é“åŒ¹é…")
    template_channels = load_template_channels()
    if template_channels:
        matched_count = match_template_channels(template_channels, channel_db)
    else:
        matched_count = 0
        print("âŒ æ— æ³•åŠ è½½æ¨¡æ¿ï¼Œè·³è¿‡åŒ¹é…")
    
    # æœ€ç»ˆç»Ÿè®¡
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print("\n" + "="*60)
    print("ğŸ‰ æ‰§è¡Œå®Œæˆ!")
    print("="*60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time} ç§’")
    print(f"  ğŸ“º æ€»é¢‘é“æ•°: {len(channel_db)}")
    print(f"  ğŸ”— æ€»æµæ•°é‡: {sum(len(urls) for urls in channel_db.values())}")
    print(f"  âœ… æœ‰æ•ˆæµæ•°é‡: {speed_stats['success_count']}")
    print(f"  ğŸ¯ æ¨¡æ¿åŒ¹é…: {matched_count} ä¸ªé¢‘é“")
    print(f"  ğŸ“ˆ å¹³å‡å“åº”: {sum(speed_stats['response_times'])/len(speed_stats['response_times']) if speed_stats['response_times'] else 0:.0f}ms")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {OUTPUT_TXT} (é¢‘é“åˆ—è¡¨)")
    print(f"  - {OUTPUT_M3U} (M3Uæ’­æ”¾åˆ—è¡¨)")
    print("="*60)

if __name__ == "__main__":
    requests.packages.urllib3.disable_warnings()
    main()
