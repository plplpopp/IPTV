import requests
import re
import os
import subprocess
import time
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, parse_qs, unquote
import hashlib
import random
import logging
from typing import List, Dict, Tuple, Optional, Any, Set, Union, Callable
import urllib3
from dataclasses import dataclass, field
import threading
from pathlib import Path
import secrets
from logging.handlers import RotatingFileHandler
import ssl
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import platform
import shutil
from functools import lru_cache
import json
from datetime import datetime
import socket
from collections import defaultdict
import tempfile
import signal
import itertools
from contextlib import contextmanager

# å°è¯•å¯¼å…¥æ¨¡ç³ŠåŒ¹é…åº“
try:
    from thefuzz import fuzz
    FUZZ_AVAILABLE = True
except ImportError:
    FUZZ_AVAILABLE = False
    # å¦‚æœæ²¡æœ‰å®‰è£…thefuzzï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
    def fuzz_ratio(a: str, b: str) -> int:
        a_lower = a.lower()
        b_lower = b.lower()
        if a_lower == b_lower:
            return 100
        elif a_lower in b_lower or b_lower in a_lower:
            return 80
        else:
            common_chars = len(set(a_lower) & set(b_lower))
            total_chars = len(set(a_lower) | set(b_lower))
            return int((common_chars / total_chars) * 100) if total_chars > 0 else 0
    
    # åˆ›å»ºå…¼å®¹çš„fuzzå¯¹è±¡
    class SimpleFuzz:
        @staticmethod
        def ratio(a: str, b: str) -> int:
            return fuzz_ratio(a, b)
    
    fuzz = SimpleFuzz()

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==================================================
# é…ç½®ç±»
# ==================================================

@dataclass
class AppConfig:
    """åº”ç”¨ç¨‹åºé…ç½®"""
    version: str = "2.1.0"
    url_sources: List[str] = field(default_factory=lambda: [
        "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
        "https://live.zbds.top/tv/iptv6.txt", 
        "https://live.zbds.top/tv/iptv4.txt",
    ])
    local_source_file: str = "local.txt"
    template_file: str = "demo.txt"
    output_txt: str = "iptv.txt"
    output_m3u: str = "iptv.m3u"
    enable_ffmpeg_test: bool = True
    enable_speed_test: bool = True
    enable_response_test: bool = True
    max_workers: int = 3
    timeout: int = 10
    max_sources_per_channel: int = 8
    max_retries: int = 2
    speed_weight: float = 0.4
    resolution_weight: float = 0.3
    response_weight: float = 0.3
    max_url_length: int = 2048
    max_channels: int = 1000
    max_sources_per_url: int = 500
    max_response_time: float = 10000.0
    min_stream_score: float = 0.2
    max_content_size: int = 10 * 1024 * 1024
    allow_local_files: bool = False
    backup_file_count: int = 5
    connection_pool_size: int = 10
    request_timeout: int = 30
    user_agent_rotation: bool = True
    enable_gzip: bool = True
    dns_timeout: int = 5
    enable_cache: bool = True
    cache_ttl: int = 3600

    @classmethod
    def from_env(cls):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        config = cls()
        if os.getenv('IPTV_MAX_WORKERS'):
            try:
                config.max_workers = int(os.getenv('IPTV_MAX_WORKERS'))
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„IPTV_MAX_WORKERSå€¼: {os.getenv('IPTV_MAX_WORKERS')}")
        
        if os.getenv('IPTV_TIMEOUT'):
            try:
                config.timeout = int(os.getenv('IPTV_TIMEOUT'))
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„IPTV_TIMEOUTå€¼: {os.getenv('IPTV_TIMEOUT')}")
        
        if os.getenv('IPTV_ENABLE_FFMPEG'):
            config.enable_ffmpeg_test = os.getenv('IPTV_ENABLE_FFMPEG').lower() == 'true'
        
        return config

    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        errors = []
        
        # æƒé‡éªŒè¯
        total_weight = self.speed_weight + self.resolution_weight + self.response_weight
        if abs(total_weight - 1.0) > 0.001:
            errors.append(f"æƒé‡é…ç½®é”™è¯¯: æ€»å’Œåº”ä¸º1.0ï¼Œå½“å‰ä¸º{total_weight:.3f}")
        
        # æ•°å€¼èŒƒå›´éªŒè¯
        if self.max_workers < 1 or self.max_workers > 50:
            errors.append(f"max_workerså¿…é¡»åœ¨1-50ä¹‹é—´: {self.max_workers}")
        
        if self.timeout < 1 or self.timeout > 300:
            errors.append(f"timeoutå¿…é¡»åœ¨1-300ç§’ä¹‹é—´: {self.timeout}")
        
        if self.max_response_time <= 0:
            errors.append(f"max_response_timeå¿…é¡»å¤§äº0: {self.max_response_time}")
        
        # æƒé‡èŒƒå›´éªŒè¯
        for weight_name, weight_value in [('speed_weight', self.speed_weight),
                                         ('resolution_weight', self.resolution_weight),
                                         ('response_weight', self.response_weight)]:
            if weight_value < 0 or weight_value > 1:
                errors.append(f"{weight_name}å¿…é¡»åœ¨0-1ä¹‹é—´: {weight_value}")
        
        # æ–‡ä»¶è·¯å¾„éªŒè¯
        if not self.template_file.strip():
            errors.append("æ¨¡æ¿æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        if len(self.output_txt.strip()) == 0:
            errors.append("è¾“å‡ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        if len(self.output_m3u.strip()) == 0:
            errors.append("è¾“å‡ºM3Uæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        # URLæºéªŒè¯
        if not self.url_sources:
            errors.append("è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ªURLæº")
        
        for url in self.url_sources:
            if not url.startswith(('http://', 'https://')):
                errors.append(f"URLæºå¿…é¡»æ˜¯HTTP/HTTPSåè®®: {url}")
        
        # æ•°å€¼é™åˆ¶éªŒè¯
        if self.max_sources_per_channel < 1:
            errors.append(f"max_sources_per_channelå¿…é¡»å¤§äº0: {self.max_sources_per_channel}")
        
        if self.max_retries < 0:
            errors.append(f"max_retrieså¿…é¡»å¤§äºç­‰äº0: {self.max_retries}")
        
        if self.max_channels < 1:
            errors.append(f"max_channelså¿…é¡»å¤§äº0: {self.max_channels}")
        
        if self.max_sources_per_url < 1:
            errors.append(f"max_sources_per_urlå¿…é¡»å¤§äº0: {self.max_sources_per_url}")
        
        return errors

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'version': self.version,
            'url_sources': self.url_sources,
            'local_source_file': self.local_source_file,
            'template_file': self.template_file,
            'output_txt': self.output_txt,
            'output_m3u': self.output_m3u,
            'enable_ffmpeg_test': self.enable_ffmpeg_test,
            'enable_speed_test': self.enable_speed_test,
            'enable_response_test': self.enable_response_test,
            'max_workers': self.max_workers,
            'timeout': self.timeout,
            'max_sources_per_channel': self.max_sources_per_channel,
            'max_retries': self.max_retries,
            'speed_weight': self.speed_weight,
            'resolution_weight': self.resolution_weight,
            'response_weight': self.response_weight,
            'max_url_length': self.max_url_length,
            'max_channels': self.max_channels,
            'max_sources_per_url': self.max_sources_per_url,
            'max_response_time': self.max_response_time,
            'min_stream_score': self.min_stream_score,
            'max_content_size': self.max_content_size,
            'allow_local_files': self.allow_local_files,
            'backup_file_count': self.backup_file_count,
            'connection_pool_size': self.connection_pool_size,
            'request_timeout': self.request_timeout,
            'user_agent_rotation': self.user_agent_rotation,
            'enable_gzip': self.enable_gzip,
            'dns_timeout': self.dns_timeout,
            'enable_cache': self.enable_cache,
            'cache_ttl': self.cache_ttl
        }

# åˆå§‹åŒ–é…ç½®
CONFIG = AppConfig.from_env()

# User-Agentåˆ—è¡¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'
]

# çº¿ç¨‹å±€éƒ¨å­˜å‚¨
_thread_local = threading.local()

# ==================================================
# ç¼“å­˜ç³»ç»Ÿ
# ==================================================

class CacheManager:
    """çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, ttl: int = 3600, max_size: int = 1000):
        self.ttl = ttl
        self.max_size = max_size
        self._cache: Dict[str, Tuple[Any, float]] = {}
        self._lock = threading.RLock()
        self._access_order: List[str] = []  # LRUæ”¯æŒ
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼ - çº¿ç¨‹å®‰å…¨"""
        with self._lock:
            if key in self._cache:
                value, timestamp = self._cache[key]
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - timestamp < self.ttl:
                    # æ›´æ–°è®¿é—®é¡ºåº (LRU)
                    if key in self._access_order:
                        self._access_order.remove(key)
                    self._access_order.append(key)
                    return value
                else:
                    # è¿‡æœŸåˆ é™¤
                    self._remove_key(key)
            return None
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼ - çº¿ç¨‹å®‰å…¨ä¸”é™åˆ¶å¤§å°"""
        with self._lock:
            # å¦‚æœè¾¾åˆ°æœ€å¤§å¤§å°ï¼Œç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„
            if len(self._cache) >= self.max_size and self._access_order:
                oldest_key = self._access_order.pop(0)
                self._remove_key(oldest_key)
            
            self._cache[key] = (value, time.time())
            if key in self._access_order:
                self._access_order.remove(key)
            self._access_order.append(key)
    
    def _remove_key(self, key: str):
        """å®‰å…¨ç§»é™¤é”®"""
        if key in self._cache:
            del self._cache[key]
        if key in self._access_order:
            self._access_order.remove(key)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self._access_order.clear()
    
    def cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        with self._lock:
            current_time = time.time()
            expired_keys = [
                key for key, (_, timestamp) in self._cache.items()
                if current_time - timestamp >= self.ttl
            ]
            for key in expired_keys:
                self._remove_key(key)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                'size': len(self._cache),
                'max_size': self.max_size,
                'lru_queue_size': len(self._access_order),
                'ttl': self.ttl
            }

# å»¶è¿Ÿåˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
def get_cache_manager() -> Optional[CacheManager]:
    """è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    if not hasattr(get_cache_manager, '_instance'):
        get_cache_manager._instance = CacheManager(ttl=CONFIG.cache_ttl, max_size=1000) if CONFIG.enable_cache else None
    return get_cache_manager._instance

# ==================================================
# æ—¥å¿—é…ç½®
# ==================================================

def setup_logging() -> logging.Logger:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger('iptv_crawler')
    
    # é¿å…é‡å¤é…ç½®
    if logger.handlers:
        return logger
    
    logger.setLevel(logging.INFO)
    logger.propagate = False
    
    # æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] - %(message)s'
    )
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶handlerï¼ˆå¸¦è½®è½¬ï¼‰
    try:
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        file_handler = RotatingFileHandler(
            log_dir / 'iptv_crawler.log',
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        # é”™è¯¯æ—¥å¿—å•ç‹¬è®°å½•
        error_handler = RotatingFileHandler(
            log_dir / 'iptv_crawler_error.log',
            maxBytes=5 * 1024 * 1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(formatter)
        logger.addHandler(error_handler)
        
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºæ–‡ä»¶æ—¥å¿—: {e}")
    
    return logger

logger = setup_logging()

# ==================================================
# æ•°æ®æ¨¡å‹
# ==================================================

@dataclass
class StreamSource:
    """æµåª’ä½“æºæ•°æ®æ¨¡å‹"""
    program_name: str
    stream_url: str
    source_type: str = "url"
    source_url: str = ""
    group: str = ""
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if not self.program_name or not self.stream_url:
            raise ValueError("èŠ‚ç›®åç§°å’ŒURLä¸èƒ½ä¸ºç©º")
        
        if len(self.program_name.strip()) == 0:
            raise ValueError("èŠ‚ç›®åç§°ä¸èƒ½ä¸ºç©ºæˆ–çº¯ç©ºæ ¼")
        
        if len(self.stream_url) > CONFIG.max_url_length:
            raise ValueError(f"URLé•¿åº¦è¶…è¿‡é™åˆ¶: {len(self.stream_url)}")
        
        # ä¸¥æ ¼çš„URLæ ¼å¼éªŒè¯
        if not self._is_valid_url(self.stream_url):
            raise ValueError(f"URLæ ¼å¼æ— æ•ˆ: {self.stream_url}")
    
    def _is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            result = urlparse(url)
            valid_schemes = ['http', 'https', 'rtmp', 'rtsp', 'udp', 'rtp', 'mms', 'hls']
            return all([result.scheme in valid_schemes, result.netloc])
        except Exception:
            return False
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'program_name': self.program_name,
            'stream_url': self.stream_url,
            'source_type': self.source_type,
            'source_url': self.source_url,
            'group': self.group
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StreamSource':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)

@dataclass
class StreamTestResult:
    """æµæµ‹è¯•ç»“æœ"""
    url: str
    score: float
    response_time: float
    resolution: str
    source_type: str
    success: bool
    timestamp: float = field(default_factory=time.time)
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if not isinstance(self.score, (int, float)) or self.score < 0 or self.score > 1:
            raise ValueError(f"åˆ†æ•°å¿…é¡»åœ¨0-1ä¹‹é—´: {self.score}")
        
        if not isinstance(self.response_time, (int, float)) or self.response_time < 0:
            raise ValueError(f"å“åº”æ—¶é—´å¿…é¡»ä¸ºéè´Ÿæ•°: {self.response_time}")
        
        if not isinstance(self.success, bool):
            raise ValueError(f"successå¿…é¡»æ˜¯å¸ƒå°”å€¼: {self.success}")
        
        if not isinstance(self.timestamp, (int, float)) or self.timestamp < 0:
            raise ValueError(f"æ—¶é—´æˆ³å¿…é¡»ä¸ºéè´Ÿæ•°: {self.timestamp}")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'url': self.url,
            'score': self.score,
            'response_time': self.response_time,
            'resolution': self.resolution,
            'source_type': self.source_type,
            'success': self.success,
            'timestamp': self.timestamp
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'StreamTestResult':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)

# ==================================================
# å·¥å…·å‡½æ•°
# ==================================================

def get_random_user_agent() -> str:
    """è·å–éšæœºUser-Agent"""
    return random.choice(USER_AGENTS) if CONFIG.user_agent_rotation else USER_AGENTS[0]

def safe_unquote(url: str) -> str:
    """å®‰å…¨çš„URLè§£ç """
    try:
        return unquote(url, errors='strict')
    except Exception as e:
        logger.warning(f"URLè§£ç å¤±è´¥ {url}: {e}")
        return url

def is_local_file_url(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æ–‡ä»¶URL"""
    try:
        parsed = urlparse(url)
        return (parsed.scheme in ('file', '') and not parsed.netloc) or \
               (parsed.netloc == '' and parsed.path.startswith('/'))
    except Exception:
        return False

def validate_url_security(url: str) -> bool:
    """éªŒè¯URLå®‰å…¨æ€§"""
    if not CONFIG.allow_local_files and is_local_file_url(url):
        logger.warning(f"ç¦æ­¢è®¿é—®æœ¬åœ°æ–‡ä»¶: {url}")
        return False
    
    try:
        parsed = urlparse(url)
        
        # æ£€æŸ¥å¯ç–‘çš„URLæ¨¡å¼
        suspicious_patterns = [
            r'/etc/', r'/passwd', r'/shadow', r'file://',
            r'\.\./', r'\.\.\\', r'javascript:', r'vbscript:',
            r'data:', r'about:'
        ]
        
        url_lower = url.lower()
        if any(pattern in url_lower for pattern in suspicious_patterns):
            return False
            
        # æ£€æŸ¥æœ¬åœ°ç½‘ç»œåœ°å€
        if parsed.hostname in ('localhost', '127.0.0.1', '::1', '0.0.0.0'):
            return CONFIG.allow_local_files
            
        return True
    except Exception as e:
        logger.warning(f"URLå®‰å…¨éªŒè¯å¤±è´¥ {url}: {e}")
        return False

def resolve_dns(hostname: str) -> bool:
    """è§£æDNSéªŒè¯ä¸»æœºå¯è¾¾æ€§"""
    try:
        socket.setdefaulttimeout(CONFIG.dns_timeout)
        socket.getaddrinfo(hostname, None)
        return True
    except socket.gaierror:
        logger.warning(f"DNSè§£æå¤±è´¥: {hostname}")
        return False
    except Exception as e:
        logger.warning(f"DNSè§£æå¼‚å¸¸ {hostname}: {e}")
        return False

def create_secure_session() -> requests.Session:
    """åˆ›å»ºå®‰å…¨çš„è¯·æ±‚ä¼šè¯"""
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=CONFIG.max_retries,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"],
        raise_on_status=False
    )
    
    # é…ç½®é€‚é…å™¨
    adapter = HTTPAdapter(
        pool_connections=CONFIG.connection_pool_size,
        pool_maxsize=CONFIG.connection_pool_size,
        max_retries=retry_strategy
    )
    
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    # è®¾ç½®é»˜è®¤headers
    session.headers.update({
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/plain, */*',
        'Accept-Encoding': 'gzip, deflate' if CONFIG.enable_gzip else 'identity',
        'Connection': 'close'
    })
    
    return session

def safe_subprocess_run(cmd: List[str], **kwargs) -> subprocess.CompletedProcess:
    """å®‰å…¨çš„å­è¿›ç¨‹æ‰§è¡Œ"""
    # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯å­—ç¬¦ä¸²
    str_cmd = [str(arg) for arg in cmd]
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    default_kwargs = {
        'capture_output': True,
        'text': True,
        'timeout': kwargs.get('timeout', 30),
        'check': False,
        'encoding': 'utf-8',
        'errors': 'replace'
    }
    default_kwargs.update(kwargs)
    
    try:
        return subprocess.run(str_cmd, **default_kwargs)
    except FileNotFoundError as e:
        logger.error(f"å‘½ä»¤æœªæ‰¾åˆ°: {cmd[0]}")
        raise
    except Exception as e:
        logger.error(f"å­è¿›ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
        raise

def normalize_url(url: str) -> str:
    """URLæ ‡å‡†åŒ– - å»é™¤å†—ä½™å‚æ•°ï¼Œç»Ÿä¸€æ ¼å¼"""
    try:
        parsed = urlparse(url)
        
        # ç§»é™¤å¸¸è§çš„ä¸å¿…è¦å‚æ•°
        query_params = parse_qs(parsed.query)
        filtered_params = {}
        
        for key, values in query_params.items():
            # ä¿ç•™é‡è¦çš„å‚æ•°ï¼Œç§»é™¤è·Ÿè¸ªå‚æ•°ç­‰
            if key in ['id', 'auth', 'token', 'key', 'channel']:
                filtered_params[key] = values
        
        # é‡å»ºURL
        new_query = '&'.join(
            f"{k}={v[0]}" for k, v in filtered_params.items()
        ) if filtered_params else ''
        
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if new_query:
            normalized += f"?{new_query}"
        if parsed.fragment:
            normalized += f"#{parsed.fragment}"
            
        return normalized
    except Exception as e:
        logger.warning(f"URLæ ‡å‡†åŒ–å¤±è´¥ {url}: {e}")
        return url

def parse_source_line(line: str) -> Optional[Tuple[str, str]]:
    """è§£ææºæ–‡ä»¶è¡Œ - æ”¯æŒå¤šç§æ ¼å¼"""
    line = line.strip()
    
    # å°è¯•å¤šç§åˆ†éš”ç¬¦
    separators = [',', '\t', '|', ' ', '    ']
    
    for sep in separators:
        if sep in line:
            parts = line.split(sep, 1)
            if len(parts) == 2:
                name, url = parts[0].strip(), parts[1].strip()
                if name and url and url.startswith(('http://', 'https://', 'rtmp://', 'rtsp://')):
                    return name, url
    
    # å¦‚æœæ²¡æœ‰æ˜ç¡®åˆ†éš”ç¬¦ï¼Œå°è¯•æå–URL
    url_match = re.search(r'(https?://[^\s]+|rtmp://[^\s]+|rtsp://[^\s]+)', line)
    if url_match:
        url = url_match.group(1)
        name = line.replace(url, '').strip()
        if not name:
            name = f"Channel_{hash(url) % 10000:04d}"  # ç”Ÿæˆå”¯ä¸€åç§°
        return name, url
    
    return None

@contextmanager
def thread_local_session():
    """çº¿ç¨‹å±€éƒ¨ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    if not hasattr(_thread_local, 'session'):
        _thread_local.session = create_secure_session()
    
    try:
        yield _thread_local.session
    except Exception as e:
        logger.error(f"ä¼šè¯æ“ä½œå¤±è´¥: {e}")
        raise
    finally:
        # ä¸åœ¨è¿™é‡Œå…³é—­ä¼šè¯ï¼Œåœ¨cleanup_resourcesä¸­ç»Ÿä¸€å¤„ç†
        pass

def cleanup_resources():
    """å®‰å…¨çš„èµ„æºæ¸…ç†"""
    # æ¸…ç†ä¼šè¯
    if hasattr(_thread_local, 'session'):
        try:
            _thread_local.session.close()
            delattr(_thread_local, 'session')
        except Exception as e:
            logger.debug(f"æ¸…ç†sessionå¤±è´¥: {e}")
    
    # å®‰å…¨åœ°æ¸…ç†å…¶ä»–å¯èƒ½çš„å±æ€§
    attrs_to_clean = ['progress_lock', 'last_progress_time', 'current_channel']
    for attr in attrs_to_clean:
        if hasattr(_thread_local, attr):
            try:
                delattr(_thread_local, attr)
            except Exception as e:
                logger.debug(f"æ¸…ç†{attr}å¤±è´¥: {e}")
    
    # æ¸…ç†ç¼“å­˜
    cache_mgr = get_cache_manager()
    if cache_mgr:
        try:
            cache_mgr.clear()
        except Exception as e:
            logger.debug(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    cleanup_temp_files()

def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        current_dir = Path('.')
        # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
        temp_patterns = ['*.tmp', 'temp_*', '*.backup.*']
        for pattern in temp_patterns:
            for temp_file in current_dir.glob(pattern):
                try:
                    if temp_file.is_file():
                        temp_file.unlink()
                        logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                except Exception as e:
                    logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}")
    except Exception as e:
        logger.debug(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    def signal_handler(signum, frame):
        signal_name = "SIGINT" if signum == signal.SIGINT else "SIGTERM"
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {signal_name}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        cleanup_resources()
        sys.exit(1)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    try:
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    except Exception as e:
        logger.warning(f"è®¾ç½®ä¿¡å·å¤„ç†å™¨å¤±è´¥: {e}")

# ==================================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ==================================================

def load_local_sources() -> List[StreamSource]:
    """åŠ è½½æœ¬åœ°æºæ–‡ä»¶"""
    sources = []
    
    if not os.path.exists(CONFIG.local_source_file):
        logger.warning(f"æœ¬åœ°æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG.local_source_file}")
        return sources
    
    try:
        with open(CONFIG.local_source_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                parsed = parse_source_line(line)
                if parsed:
                    program_name, stream_url = parsed
                    try:
                        source = StreamSource(
                            program_name=program_name,
                            stream_url=stream_url,
                            source_type="local", 
                            source_url=CONFIG.local_source_file
                        )
                        sources.append(source)
                    except ValueError as e:
                        logger.warning(f"æœ¬åœ°æºç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯: {e}")
                else:
                    logger.warning(f"æœ¬åœ°æºç¬¬{line_num}è¡Œæ ¼å¼æ— æ•ˆ: {line}")
                        
    except Exception as e:
        logger.error(f"åŠ è½½æœ¬åœ°æºå¤±è´¥: {e}")
    
    logger.info(f"æˆåŠŸåŠ è½½æœ¬åœ°æº: {len(sources)} ä¸ª")
    return sources

def fetch_url_sources() -> List[StreamSource]:
    """ä»URLæºæŠ“å–ç›´æ’­æº"""
    all_sources = []
    
    def fetch_single_url(source_url: str) -> List[StreamSource]:
        """æŠ“å–å•ä¸ªURLæº"""
        sources = []
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                with thread_local_session() as session:
                    response = session.get(
                        source_url, 
                        timeout=(CONFIG.request_timeout, 30),
                        verify=False,
                        stream=True,
                        allow_redirects=True
                    )
                    response.raise_for_status()
                    
                    # ä¿®å¤ç¼–ç å¤„ç†é€»è¾‘
                    content_bytes = b""
                    for chunk in response.iter_content(chunk_size=8192):
                        content_bytes += chunk
                        if len(content_bytes) > CONFIG.max_content_size:
                            logger.warning(f"æºå†…å®¹è¿‡å¤§ï¼Œå·²æˆªæ–­: {source_url}")
                            break
                    
                    # å°è¯•å¤šç§ç¼–ç è§£ç 
                    content = None
                    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'iso-8859-1']
                    
                    for encoding in encodings:
                        try:
                            content = content_bytes.decode(encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if content is None:
                        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨errors='replace'
                        content = content_bytes.decode('utf-8', errors='replace')
                        logger.warning(f"ä½¿ç”¨æ›¿æ¢ç­–ç•¥è§£ç å†…å®¹: {source_url}")
                    
                    # è§£æå†…å®¹
                    line_count = 0
                    for line_num, line in enumerate(content.splitlines(), 1):
                        if line_count >= CONFIG.max_sources_per_url:
                            logger.info(f"è¾¾åˆ°æœ€å¤§æºæ•°é‡é™åˆ¶: {CONFIG.max_sources_per_url}")
                            break
                            
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                        
                        parsed = parse_source_line(line)
                        if parsed:
                            program_name, stream_url = parsed
                            try:
                                normalized_url = normalize_url(stream_url)
                                source = StreamSource(
                                    program_name=program_name,
                                    stream_url=normalized_url,
                                    source_type="url",
                                    source_url=source_url
                                )
                                sources.append(source)
                                line_count += 1
                            except ValueError as e:
                                logger.debug(f"è·³è¿‡æ— æ•ˆæº {source_url}:{line_num}: {e}")
                        else:
                            logger.debug(f"è·³è¿‡æ— æ³•è§£æçš„è¡Œ {source_url}:{line_num}: {line}")
                    
                    logger.info(f"æˆåŠŸä» {source_url} æŠ“å– {len(sources)} ä¸ªæº")
                    break
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"æŠ“å–æºå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}) {source_url}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"æŠ“å–æºæœ€ç»ˆå¤±è´¥ {source_url}: {e}")
            except Exception as e:
                logger.error(f"æŠ“å–æºå¼‚å¸¸ {source_url}: {e}")
                break
        
        return sources
    
    # å¹¶è¡ŒæŠ“å–æ‰€æœ‰URLæº
    with ThreadPoolExecutor(max_workers=min(len(CONFIG.url_sources), CONFIG.max_workers)) as executor:
        future_to_url = {
            executor.submit(fetch_single_url, url): url 
            for url in CONFIG.url_sources
        }
        
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                sources = future.result()
                all_sources.extend(sources)
            except Exception as e:
                logger.error(f"å¤„ç†URLæºå¤±è´¥ {url}: {e}")
    
    logger.info(f"ä»ç½‘ç»œæºå…±æŠ“å– {len(all_sources)} ä¸ªæº")
    return all_sources

def organize_channels_by_template(sources: List[StreamSource], template: List[str]) -> Dict[str, List[StreamSource]]:
    """æŒ‰æ¨¡æ¿æ•´ç†é¢‘é“"""
    organized = {channel: [] for channel in template}
    
    # æ„å»ºé¢‘é“åç§°æ˜ å°„ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
    channel_patterns = {}
    for channel in template:
        # åˆ›å»ºå¤šä¸ªåŒ¹é…æ¨¡å¼
        patterns = [
            channel.lower(),
            channel.lower().replace(' ', ''),
            channel.lower().replace(' ', '_'),
            channel.lower().replace('cctv', 'cctv '),
            channel.lower().replace('cctv ', 'cctv'),
        ]
        
        # ä¸­æ–‡é¢‘é“ç‰¹æ®Šå¤„ç†
        if any(char >= '\u4e00' and char <= '\u9fff' for char in channel):
            patterns.extend([
                channel.lower().replace('å«è§†', ''),
                channel.lower() + 'ç”µè§†å°',
                channel.lower().replace('ç”µè§†å°', '')
            ])
        
        channel_patterns[channel] = patterns
    
    for source in sources:
        matched_channel = None
        source_name_lower = source.program_name.lower()
        
        # ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
        for channel, patterns in channel_patterns.items():
            if source_name_lower in patterns:
                matched_channel = channel
                break
        
        # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ…å«åŒ¹é…
        if not matched_channel:
            for channel, patterns in channel_patterns.items():
                for pattern in patterns:
                    if pattern in source_name_lower or source_name_lower in pattern:
                        matched_channel = channel
                        break
                if matched_channel:
                    break
        
        # å¦‚æœåŒ…å«åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not matched_channel and FUZZ_AVAILABLE:
            best_match = None
            best_score = 0
            
            for channel in template:
                score = fuzz.ratio(source_name_lower, channel.lower())
                if score > best_score and score > 60:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    best_score = score
                    best_match = channel
            
            matched_channel = best_match
        
        if matched_channel:
            organized[matched_channel].append(source)
    
    # ç»Ÿè®¡ç»“æœ
    matched_count = sum(1 for sources in organized.values() if sources)
    total_sources = sum(len(sources) for sources in organized.values())
    
    logger.info(f"é¢‘é“åŒ¹é…å®Œæˆ: {matched_count}/{len(template)} ä¸ªé¢‘é“åŒ¹é…åˆ°æº")
    logger.info(f"æ€»å…±åŒ¹é…åˆ° {total_sources} ä¸ªæº")
    
    # è®°å½•æœªåŒ¹é…çš„æºï¼ˆç”¨äºè°ƒè¯•ï¼‰
    unmatched_sources = []
    for source in sources:
        if not any(source in channel_sources for channel_sources in organized.values()):
            unmatched_sources.append(source)
    
    if unmatched_sources:
        logger.debug(f"æœ‰ {len(unmatched_sources)} ä¸ªæºæœªèƒ½åŒ¹é…åˆ°ä»»ä½•é¢‘é“")
        for source in unmatched_sources[:5]:  # åªè®°å½•å‰5ä¸ª
            logger.debug(f"æœªåŒ¹é…æº: {source.program_name}")
    
    return organized

def test_single_source(source: StreamSource) -> StreamTestResult:
    """æµ‹è¯•å•ä¸ªæºçš„è´¨é‡"""
    try:
        start_time = time.time()
        response_time = 0
        
        # åŸºç¡€å“åº”æµ‹è¯•
        if CONFIG.enable_response_test:
            try:
                with thread_local_session() as session:
                    response_start = time.time()
                    response = session.head(
                        source.stream_url, 
                        timeout=CONFIG.timeout,
                        verify=False,
                        allow_redirects=True
                    )
                    response_time = (time.time() - response_start) * 1000  # æ¯«ç§’
                    
                    if response.status_code != 200:
                        return StreamTestResult(
                            url=source.stream_url,
                            score=0.0,
                            response_time=response_time,
                            resolution="unknown",
                            source_type=source.source_type,
                            success=False
                        )
            except Exception as e:
                logger.debug(f"å“åº”æµ‹è¯•å¤±è´¥ {source.stream_url}: {e}")
                response_time = CONFIG.max_response_time
        
        # FFmpegæµåª’ä½“æµ‹è¯•
        resolution = "unknown"
        speed_score = 0.5  # é»˜è®¤åˆ†æ•°
        resolution_score = 0.5
        
        if CONFIG.enable_ffmpeg_test:
            try:
                # ä½¿ç”¨FFmpegè¿›è¡Œå¿«é€Ÿæµ‹è¯•
                cmd = [
                    'ffmpeg',
                    '-i', source.stream_url,
                    '-t', '5',  # æµ‹è¯•5ç§’
                    '-f', 'null', '-',
                    '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                    '-loglevel', 'error'
                ]
                
                # æ·»åŠ å¹³å°ç‰¹å®šå‚æ•°
                if platform.system() != 'Windows':
                    cmd.extend(['-analyzeduration', '10000000', '-probesize', '10000000'])
                
                result = safe_subprocess_run(
                    cmd, 
                    timeout=CONFIG.timeout + 10,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
                
                # è§£æFFmpegè¾“å‡ºè·å–åˆ†è¾¨ç‡
                output = result.stderr or result.stdout or ""
                resolution_match = re.search(r'(\d+)x(\d+)', output)
                if resolution_match:
                    width = int(resolution_match.group(1))
                    height = int(resolution_match.group(2))
                    resolution = f"{width}x{height}"
                    
                    # æ ¹æ®åˆ†è¾¨ç‡è®¡ç®—åˆ†æ•°
                    if width >= 1920 and height >= 1080:
                        resolution_score = 1.0
                    elif width >= 1280 and height >= 720:
                        resolution_score = 0.8
                    elif width >= 720 and height >= 480:
                        resolution_score = 0.6
                    else:
                        resolution_score = 0.4
                else:
                    resolution_score = 0.3
                
                # æ ¹æ®FFmpegé€€å‡ºç åˆ¤æ–­æˆåŠŸ
                if result.returncode == 0:
                    speed_score = 0.9
                elif result.returncode == 1 and "Conversion failed" not in output:
                    # FFmpegæœ‰æ—¶è¿”å›1ä½†å®é™…ä¸Šæ˜¯æˆåŠŸçš„
                    speed_score = 0.7
                else:
                    speed_score = 0.3
                    
            except subprocess.TimeoutExpired:
                logger.debug(f"FFmpegæµ‹è¯•è¶…æ—¶ {source.stream_url}")
                speed_score = 0.1
                resolution_score = 0.1
            except Exception as e:
                logger.debug(f"FFmpegæµ‹è¯•å¤±è´¥ {source.stream_url}: {e}")
                speed_score = 0.2
                resolution_score = 0.2
        else:
            speed_score = 0.7
            resolution_score = 0.5
        
        # å“åº”æ—¶é—´åˆ†æ•°ï¼ˆå“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜ï¼‰
        response_score = max(0, 1 - (response_time / CONFIG.max_response_time))
        
        # ç»¼åˆè¯„åˆ†
        total_score = (
            speed_score * CONFIG.speed_weight +
            resolution_score * CONFIG.resolution_weight + 
            response_score * CONFIG.response_weight
        )
        
        success = total_score >= CONFIG.min_stream_score
        
        return StreamTestResult(
            url=source.stream_url,
            score=total_score,
            response_time=response_time,
            resolution=resolution,
            source_type=source.source_type,
            success=success
        )
        
    except Exception as e:
        logger.debug(f"æºæµ‹è¯•å¼‚å¸¸ {source.stream_url}: {e}")
        return StreamTestResult(
            url=source.stream_url,
            score=0.0,
            response_time=CONFIG.max_response_time,
            resolution="unknown",
            source_type=source.source_type,
            success=False
        )

def test_channel_sources(channel_name: str, sources: List[StreamSource]) -> List[StreamTestResult]:
    """æµ‹è¯•é¢‘é“çš„æ‰€æœ‰æºå¹¶è¿”å›æœ€ä½³ç»“æœ"""
    if not sources:
        return []
    
    # ä¼˜å…ˆæµ‹è¯•æœ¬åœ°æº
    local_sources = [s for s in sources if s.source_type == "local"]
    url_sources = [s for s in sources if s.source_type == "url"]
    
    all_sources = local_sources + url_sources
    
    # å¹¶è¡Œæµ‹è¯•æ‰€æœ‰æº
    test_results = []
    with ThreadPoolExecutor(max_workers=min(len(all_sources), CONFIG.max_workers)) as executor:
        future_to_source = {
            executor.submit(test_single_source, source): source 
            for source in all_sources
        }
        
        for future in as_completed(future_to_source):
            source = future_to_source[future]
            try:
                result = future.result()
                test_results.append(result)
            except Exception as e:
                logger.error(f"æµ‹è¯•æºå¤±è´¥ {source.stream_url}: {e}")
    
    # ç­›é€‰æˆåŠŸçš„æµ‹è¯•ç»“æœå¹¶æŒ‰åˆ†æ•°æ’åº
    successful_results = [r for r in test_results if r.success]
    successful_results.sort(key=lambda x: x.score, reverse=True)
    
    # è¿”å›æœ€ä½³çš„å‡ ä¸ªæº
    best_results = successful_results[:CONFIG.max_sources_per_channel]
    
    return best_results

def save_to_txt(channels_data: List[Tuple[str, List[StreamTestResult]]]) -> bool:
    """ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼"""
    try:
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(CONFIG.output_txt):
            timestamp = int(time.time())
            backup_file = f"{CONFIG.output_txt}.backup.{timestamp}"
            try:
                shutil.copy2(CONFIG.output_txt, backup_file)
                logger.info(f"åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_file}")
            except Exception as e:
                logger.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
        
        with open(CONFIG.output_txt, 'w', encoding='utf-8') as f:
            for channel_name, test_results in channels_data:
                for result in test_results:
                    f.write(f"{channel_name},{result.url}\n")
        
        logger.info(f"æˆåŠŸä¿å­˜æ–‡æœ¬æ ¼å¼: {CONFIG.output_txt}, å…± {len(channels_data)} ä¸ªé¢‘é“")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        return False

def save_to_m3u(channels_data: List[Tuple[str, List[StreamTestResult]]]) -> bool:
    """ä¿å­˜ä¸ºM3Uæ ¼å¼"""
    try:
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(CONFIG.output_m3u):
            timestamp = int(time.time())
            backup_file = f"{CONFIG.output_m3u}.backup.{timestamp}"
            try:
                shutil.copy2(CONFIG.output_m3u, backup_file)
                logger.info(f"åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_file}")
            except Exception as e:
                logger.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
        
        with open(CONFIG.output_m3u, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            f.write("# Generated by IPTV Crawler\n")
            f.write(f"# Created: {datetime.now().isoformat()}\n")
            f.write(f"# Channels: {len(channels_data)}\n")
            f.write(f"# Version: {CONFIG.version}\n\n")
            
            for channel_name, test_results in channels_data:
                for result in test_results:
                    # M3Uæ ¼å¼æ¡ç›®
                    f.write(f"#EXTINF:-1,{channel_name}\n")
                    f.write(f"{result.url}\n")
        
        logger.info(f"æˆåŠŸä¿å­˜M3Uæ ¼å¼: {CONFIG.output_m3u}, å…± {len(channels_data)} ä¸ªé¢‘é“")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜M3Uæ–‡ä»¶å¤±è´¥: {e}")
        return False

# ==================================================
# æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
# ==================================================

def run_basic_tests() -> bool:
    """è¿è¡ŒåŸºæœ¬æµ‹è¯•"""
    print("ğŸ§ª è¿è¡ŒåŸºæœ¬æµ‹è¯•...")
    
    tests_passed = 0
    tests_failed = 0
    
    # æµ‹è¯•1: é…ç½®éªŒè¯
    try:
        config_errors = CONFIG.validate()
        if not config_errors:
            print("âœ… é…ç½®éªŒè¯æµ‹è¯•é€šè¿‡")
            tests_passed += 1
        else:
            print("âŒ é…ç½®éªŒè¯æµ‹è¯•å¤±è´¥")
            for error in config_errors:
                print(f"   - {error}")
            tests_failed += 1
    except Exception as e:
        print(f"âŒ é…ç½®éªŒè¯æµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    # æµ‹è¯•2: URLæ ‡å‡†åŒ–
    try:
        test_urls = [
            ("http://example.com/test.m3u8?id=123&t=123456", "http://example.com/test.m3u8?id=123"),
            ("https://example.com:8080/live/stream.m3u8", "https://example.com:8080/live/stream.m3u8"),
        ]
        
        all_passed = True
        for input_url, expected in test_urls:
            result = normalize_url(input_url)
            if result.startswith(expected.split('?')[0]):
                print(f"âœ… URLæ ‡å‡†åŒ–æµ‹è¯•é€šè¿‡: {input_url} -> {result}")
            else:
                print(f"âŒ URLæ ‡å‡†åŒ–æµ‹è¯•å¤±è´¥: {input_url} -> {result} (æœŸæœ›: {expected})")
                all_passed = False
        
        if all_passed:
            tests_passed += 1
        else:
            tests_failed += 1
    except Exception as e:
        print(f"âŒ URLæ ‡å‡†åŒ–æµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    # æµ‹è¯•3: æ•°æ®æ¨¡å‹
    try:
        # æµ‹è¯•æœ‰æ•ˆæ•°æ®
        valid_source = StreamSource("æµ‹è¯•é¢‘é“", "http://example.com/stream.m3u8")
        valid_result = StreamTestResult(
            url="http://example.com/stream.m3u8",
            score=0.8,
            response_time=100,
            resolution="1920x1080",
            source_type="url",
            success=True
        )
        print("âœ… æ•°æ®æ¨¡å‹éªŒè¯æµ‹è¯•é€šè¿‡")
        tests_passed += 1
        
        # æµ‹è¯•æ— æ•ˆæ•°æ®
        try:
            invalid_source = StreamSource("", "http://example.com/stream.m3u8")
            print("âŒ æ•°æ®æ¨¡å‹éªŒè¯æµ‹è¯•å¤±è´¥: åº”è¯¥æŠ›å‡ºå¼‚å¸¸")
            tests_failed += 1
        except ValueError:
            print("âœ… æ•°æ®æ¨¡å‹æ— æ•ˆæ•°æ®æµ‹è¯•é€šè¿‡")
            tests_passed += 1
            
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å‹æµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    # æµ‹è¯•4: æ–‡ä»¶æ“ä½œ
    try:
        test_content = "æµ‹è¯•å†…å®¹"
        test_file = "test_temp_file.txt"
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
        
        with open(test_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if content == test_content:
            print("âœ… æ–‡ä»¶æ“ä½œæµ‹è¯•é€šè¿‡")
            tests_passed += 1
        else:
            print("âŒ æ–‡ä»¶æ“ä½œæµ‹è¯•å¤±è´¥")
            tests_failed += 1
        
        # æ¸…ç†
        if os.path.exists(test_file):
            os.remove(test_file)
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æ“ä½œæµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {tests_passed} é€šè¿‡, {tests_failed} å¤±è´¥")
    
    return tests_failed == 0

def run_network_tests() -> bool:
    """è¿è¡Œç½‘ç»œæµ‹è¯•"""
    print("\nğŸŒ è¿è¡Œç½‘ç»œæµ‹è¯•...")
    
    tests_passed = 0
    tests_failed = 0
    
    # æµ‹è¯•1: DNSè§£æ
    try:
        if resolve_dns("www.baidu.com"):
            print("âœ… DNSè§£ææµ‹è¯•é€šè¿‡")
            tests_passed += 1
        else:
            print("âŒ DNSè§£ææµ‹è¯•å¤±è´¥")
            tests_failed += 1
    except Exception as e:
        print(f"âŒ DNSè§£ææµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    # æµ‹è¯•2: ä¼šè¯åˆ›å»º
    try:
        session = create_secure_session()
        session.close()
        print("âœ… ä¼šè¯åˆ›å»ºæµ‹è¯•é€šè¿‡")
        tests_passed += 1
    except Exception as e:
        print(f"âŒ ä¼šè¯åˆ›å»ºæµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    # æµ‹è¯•3: URLå®‰å…¨éªŒè¯
    try:
        safe_urls = ["http://example.com", "https://example.com"]
        unsafe_urls = ["file:///etc/passwd", "javascript:alert('xss')"]
        
        all_safe_passed = all(validate_url_security(url) for url in safe_urls)
        all_unsafe_passed = all(not validate_url_security(url) for url in unsafe_urls)
        
        if all_safe_passed and all_unsafe_passed:
            print("âœ… URLå®‰å…¨éªŒè¯æµ‹è¯•é€šè¿‡")
            tests_passed += 1
        else:
            print("âŒ URLå®‰å…¨éªŒè¯æµ‹è¯•å¤±è´¥")
            tests_failed += 1
    except Exception as e:
        print(f"âŒ URLå®‰å…¨éªŒè¯æµ‹è¯•å¼‚å¸¸: {e}")
        tests_failed += 1
    
    print(f"ğŸ“Š ç½‘ç»œæµ‹è¯•ç»“æœ: {tests_passed} é€šè¿‡, {tests_failed} å¤±è´¥")
    
    return tests_failed == 0

def check_dependencies() -> bool:
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 7):
        logger.error("éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    
    # æ£€æŸ¥å¿…éœ€åº“
    required_libraries = ['requests', 'urllib3']
    missing_libraries = []
    
    for lib in required_libraries:
        try:
            __import__(lib)
            print(f"âœ… {lib} å¯ç”¨")
        except ImportError as e:
            missing_libraries.append((lib, str(e)))
            print(f"âŒ {lib} ç¼ºå¤±")
    
    if missing_libraries:
        for lib, error in missing_libraries:
            logger.error(f"ç¼ºå°‘å¿…éœ€åº“ {lib}: {error}")
        logger.info("è¯·è¿è¡Œ: pip install requests urllib3")
        return False
    
    # æ£€æŸ¥æ ‡å‡†åº“
    std_libraries = ['subprocess', 'threading', 'hashlib', 'ssl', 'json', 'datetime']
    for lib in std_libraries:
        try:
            __import__(lib)
        except ImportError as e:
            logger.error(f"ç¼ºå°‘æ ‡å‡†åº“ {lib}: {e}")
            return False
    
    # æ£€æŸ¥FFmpegï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if CONFIG.enable_ffmpeg_test:
        try:
            result = safe_subprocess_run(['ffmpeg', '-version'], timeout=10)
            if result.returncode == 0:
                print("âœ… FFmpeg å¯ç”¨")
                
                # æ£€æŸ¥FFmpegç‰ˆæœ¬
                version_match = re.search(r'ffmpeg version\s+(\d+\.\d+)', result.stdout)
                if version_match:
                    version = float(version_match.group(1))
                    if version < 3.0:
                        logger.warning(f"FFmpegç‰ˆæœ¬è¾ƒè€: {version}ï¼Œå»ºè®®ä½¿ç”¨4.0æˆ–æ›´é«˜ç‰ˆæœ¬")
            else:
                logger.error(f"FFmpegæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                if "not found" in result.stderr.lower():
                    logger.info("è¯·å®‰è£…FFmpegæˆ–è®¾ç½®ç³»ç»ŸPATHç¯å¢ƒå˜é‡")
                return False
        except subprocess.TimeoutExpired:
            logger.error("FFmpegæ£€æŸ¥è¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"FFmpegæ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    else:
        print("â„¹ï¸ FFmpegæµ‹è¯•å·²ç¦ç”¨")
    
    print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True

def validate_configuration() -> Tuple[List[str], List[str]]:
    """éªŒè¯é…ç½®åˆç†æ€§"""
    errors = []
    warnings = []
    
    # ä½¿ç”¨é…ç½®ç±»çš„éªŒè¯æ–¹æ³•
    config_errors = CONFIG.validate()
    errors.extend(config_errors)
    
    # æ£€æŸ¥URLæºæ ¼å¼å’Œå®‰å…¨æ€§
    for url in CONFIG.url_sources:
        if not url.startswith(('http://', 'https://')):
            errors.append(f"URLæ ¼å¼é”™è¯¯: {url}")
        elif len(url) > CONFIG.max_url_length:
            warnings.append(f"URLè¿‡é•¿: {url}")
        elif not validate_url_security(url):
            warnings.append(f"URLå®‰å…¨æ€§è­¦å‘Š: {url}")
    
    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„å®‰å…¨æ€§
    file_paths = [CONFIG.template_file, CONFIG.local_source_file, CONFIG.output_txt, CONFIG.output_m3u]
    for file_path in file_paths:
        try:
            path = Path(file_path).resolve()
            current_dir = Path('.').resolve()
            
            # æ£€æŸ¥è·¯å¾„éå†
            if '..' in file_path:
                warnings.append(f"æ–‡ä»¶è·¯å¾„åŒ…å«è·¯å¾„éå†å­—ç¬¦: {file_path}")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰ç›®å½•ä¸‹
            try:
                path.relative_to(current_dir)
            except ValueError:
                warnings.append(f"æ–‡ä»¶è·¯å¾„è¶…å‡ºå·¥ä½œç›®å½•: {file_path}")
                
        except Exception as e:
            warnings.append(f"æ–‡ä»¶è·¯å¾„æ£€æŸ¥å¤±è´¥ {file_path}: {e}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists(CONFIG.template_file):
        try:
            create_sample_template()
            errors.append(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {CONFIG.template_file}")
        except Exception as e:
            errors.append(f"åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
    else:
        # éªŒè¯æ¨¡æ¿æ–‡ä»¶å†…å®¹å’Œå¤§å°
        try:
            file_size = os.path.getsize(CONFIG.template_file)
            if file_size > CONFIG.max_content_size:
                errors.append(f"æ¨¡æ¿æ–‡ä»¶è¿‡å¤§: {file_size}å­—èŠ‚")
            elif file_size == 0:
                errors.append("æ¨¡æ¿æ–‡ä»¶ä¸ºç©º")
            
            with open(CONFIG.template_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                valid_channels = [line for line in content.split('\n') 
                                if line.strip() and not line.startswith('#')]
                if not valid_channels:
                    errors.append("æ¨¡æ¿æ–‡ä»¶ä¸ºç©ºæˆ–åªæœ‰æ³¨é‡Šï¼Œè¯·æ·»åŠ é¢‘é“åç§°")
                elif len(valid_channels) > CONFIG.max_channels:
                    warnings.append(f"æ¨¡æ¿é¢‘é“æ•°é‡{len(valid_channels)}è¶…è¿‡å»ºè®®å€¼{CONFIG.max_channels}")
        except Exception as e:
            errors.append(f"è¯»å–æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
    
    if not os.path.exists(CONFIG.local_source_file):
        try:
            create_sample_local_source()
            warnings.append(f"æœ¬åœ°æºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {CONFIG.local_source_file}")
        except Exception as e:
            warnings.append(f"åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•æƒé™
    try:
        output_dir = os.path.dirname(CONFIG.output_txt) or '.'
        test_file = os.path.join(output_dir, f".test_{secrets.token_hex(8)}.tmp")
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write("test")
        
        os.remove(test_file)
        
        if not os.access(output_dir, os.W_OK):
            errors.append(f"è¾“å‡ºç›®å½•æ— å†™æƒé™: {output_dir}")
    except Exception as e:
        errors.append(f"è¾“å‡ºç›®å½•æƒé™é”™è¯¯: {e}")
    
    return errors, warnings

def create_sample_template():
    """åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶"""
    sample_content = """# é¢‘é“æ¨¡æ¿æ–‡ä»¶ - æ¯è¡Œä¸€ä¸ªé¢‘é“åç§°
# ç¨‹åºå°†ä¸¥æ ¼æŒ‰ç…§æ­¤åˆ—è¡¨é¡ºåºå’Œå†…å®¹ç”Ÿæˆç»“æœ
# æ”¯æŒçš„å­—ç¬¦: ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿

CCTV1
CCTV2
CCTV5
æ¹–å—å«è§†
æµ™æ±Ÿå«è§†
åŒ—äº¬å«è§†
"""
    try:
        with open(CONFIG.template_file, 'w', encoding='utf-8') as f:
            f.write(sample_content)
        logger.info(f"å·²åˆ›å»ºæ¨¡æ¿æ–‡ä»¶: {CONFIG.template_file}")
    except Exception as e:
        logger.error(f"åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        raise

def create_sample_local_source():
    """åˆ›å»ºç¤ºä¾‹æœ¬åœ°æºæ–‡ä»¶"""
    sample_content = """# æœ¬åœ°æºæ–‡ä»¶ - æ¯è¡Œæ ¼å¼: é¢‘é“åç§°,URL
# æœ¬åœ°æºå°†ä¼˜å…ˆä½¿ç”¨
# ç¤ºä¾‹æ ¼å¼: é¢‘é“åç§°,http://example.com/stream.m3u8

CCTV1,http://example.com/cctv1.m3u8
CCTV2,http://example.com/cctv2.m3u8
"""
    try:
        with open(CONFIG.local_source_file, 'w', encoding='utf-8') as f:
            f.write(sample_content)
        logger.info(f"å·²åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶: {CONFIG.local_source_file}")
    except Exception as e:
        logger.error(f"åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")

def load_template_channels() -> List[str]:
    """åŠ è½½æ¨¡æ¿é¢‘é“åˆ—è¡¨"""
    if not os.path.exists(CONFIG.template_file):
        logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG.template_file}")
        return []
    
    try:
        channels = []
        with open(CONFIG.template_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line_num > CONFIG.max_channels:
                    logger.warning(f"è¶…è¿‡æœ€å¤§é¢‘é“æ•°é‡é™åˆ¶: {CONFIG.max_channels}")
                    break
                    
                line = line.strip()
                if line and not line.startswith('#'):
                    # å®‰å…¨çš„è¾“å…¥æ¸…ç† - å…è®¸ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€å¸¸è§æ ‡ç‚¹
                    clean_line = re.sub(r'[^\w\s\u4e00-\u9fff\-\.Â·]', '', line)
                    clean_line = re.sub(r'\s+', ' ', clean_line).strip()
                    
                    if clean_line and len(clean_line) <= 100:  # é™åˆ¶é¢‘é“åç§°é•¿åº¦
                        channels.append(clean_line)
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆçš„é¢‘é“åç§°: {line}")
        
        if not channels:
            logger.error("æ¨¡æ¿æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“åç§°")
            return []
        
        logger.info(f"æˆåŠŸåŠ è½½æ¨¡æ¿é¢‘é“: {len(channels)} ä¸ª")
        return channels
    except UnicodeDecodeError:
        logger.error("æ¨¡æ¿æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ä½¿ç”¨UTF-8ç¼–ç ")
        return []
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return []

def print_config_summary():
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("\nâš™ å½“å‰é…ç½®:")
    print(f"  ğŸ“¡ æºä¼˜å…ˆçº§: æœ¬åœ°æº > ç½‘ç»œæº")
    print(f"  â± æµ‹é€Ÿæ—¶é—´: 5ç§’")
    print(f"  ğŸ“º æ¯ä¸ªé¢‘é“ä¿ç•™: {CONFIG.max_sources_per_channel} ä¸ªæœ€ä½³æº")
    print(f"  ğŸ”§ FFmpegæµ‹é€Ÿ: {'âœ…å¼€å¯' if CONFIG.enable_ffmpeg_test else 'âŒå…³é—­'}")
    print(f"  ğŸ¯ æ™ºèƒ½æµ‹é€Ÿ: {'âœ…å¼€å¯' if CONFIG.enable_speed_test else 'âŒå…³é—­'}")
    print(f"  âš¡ å“åº”æµ‹è¯•: {'âœ…å¼€å¯' if CONFIG.enable_response_test else 'âŒå…³é—­'}")
    print(f"  ğŸ”„ æœ€å¤§é‡è¯•æ¬¡æ•°: {CONFIG.max_retries}")
    print(f"  ğŸ‘¥ æœ€å¤§å·¥ä½œçº¿ç¨‹: {CONFIG.max_workers}")
    print(f"  ğŸ›¡ï¸ å®‰å…¨æ¨¡å¼: {'âœ…å¼€å¯' if not CONFIG.allow_local_files else 'âŒå…³é—­'}")
    print(f"  ğŸ’¾ ç¼“å­˜ç³»ç»Ÿ: {'âœ…å¼€å¯' if CONFIG.enable_cache else 'âŒå…³é—­'}")
    print()

def run_main_workflow(start_time: float) -> int:
    """æ‰§è¡Œä¸»è¦å·¥ä½œæµç¨‹"""
    # 1. åŠ è½½æ¨¡æ¿é¢‘é“
    print("\n" + "="*50)
    print("ğŸ“‹ æ­¥éª¤ 1: åŠ è½½æ¨¡æ¿é¢‘é“")
    template_channels = load_template_channels()
    if not template_channels:
        logger.error("æ— æ³•åŠ è½½æ¨¡æ¿é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        return 1
    
    # 2. ä¼˜å…ˆåŠ è½½æœ¬åœ°æº
    print("\n" + "="*50)
    print("ğŸ“‚ æ­¥éª¤ 2: åŠ è½½æœ¬åœ°æº (ä¼˜å…ˆ)")
    local_sources = load_local_sources()
    
    # 3. æŠ“å–ç½‘ç»œæº
    print("\n" + "="*50)
    print("ğŸŒ æ­¥éª¤ 3: æŠ“å–ç½‘ç»œæº")
    url_sources = fetch_url_sources()
    
    # åˆå¹¶æº
    all_sources = local_sources + url_sources
    
    if not all_sources:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç›´æ’­æº")
        return 1
    
    print(f"\nğŸ“Š æºç»Ÿè®¡æ±‡æ€»:")
    print(f"  ğŸ  æœ¬åœ°æº: {len(local_sources)} ä¸ª")
    print(f"  ğŸŒ ç½‘ç»œæº: {len(url_sources)} ä¸ª")
    print(f"  ğŸ“ˆ æ€»è®¡: {len(all_sources)} ä¸ªæº")
    
    # 4. æ•´ç†é¢‘é“
    print("\n" + "="*50)
    print("ğŸ”„ æ­¥éª¤ 4: æ•´ç†é¢‘é“")
    organized_channels = organize_channels_by_template(all_sources, template_channels)
    
    # ç»Ÿè®¡åŒ¹é…ç»“æœ
    matched_channels = sum(1 for sources in organized_channels.values() if sources)
    empty_channels = len(organized_channels) - matched_channels
    
    print(f"ğŸ“º é¢‘é“åŒ¹é…ç»“æœ:")
    print(f"  âœ… æœ‰æºçš„é¢‘é“: {matched_channels} ä¸ª")
    print(f"  âŒ æ— æºçš„é¢‘é“: {empty_channels} ä¸ª")
    
    if matched_channels == 0:
        logger.error("æ²¡æœ‰é¢‘é“åŒ¹é…åˆ°ä»»ä½•æº")
        return 1
    
    # 5. å…¨é¢æµ‹è¯•
    print("\n" + "="*50)
    print("ğŸš€ æ­¥éª¤ 5: å…¨é¢æµ‹è¯•æ‰€æœ‰é¢‘é“æº")
    print("   æµ‹è¯•å†…å®¹:")
    print("   - âš¡ å“åº”æ—¶é—´æµ‹è¯•")
    print("   - â± 5ç§’FFmpegæµåª’ä½“æµ‹è¯•") 
    print("   - ğŸ“º åˆ†è¾¨ç‡æ£€æµ‹")
    print("   - ğŸ¯ è´¨é‡ç»¼åˆè¯„åˆ†")
    print()
    
    final_channels_data = []
    successful_channels = 0
    
    for channel_index, (channel_name, sources) in enumerate(organized_channels.items(), 1):
        print(f"\n[{channel_index}/{len(organized_channels)}] æµ‹è¯•é¢‘é“: {channel_name}")
        
        if sources:
            best_sources = test_channel_sources(channel_name, sources)
            
            if best_sources:
                final_channels_data.append((channel_name, best_sources))
                successful_channels += 1
                
                # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
                local_count = sum(1 for s in best_sources if s.source_type == "local")
                avg_response = sum(s.response_time for s in best_sources) / len(best_sources)
                avg_score = sum(s.score for s in best_sources) / len(best_sources)
                
                print(f"   âœ… å®Œæˆ: ä¿ç•™ {len(best_sources)} ä¸ªæº")
                print(f"      ğŸ  æœ¬åœ°: {local_count}, ğŸŒ ç½‘ç»œ: {len(best_sources) - local_count}")
                print(f"      â± å¹³å‡å“åº”: {avg_response:.0f}ms")
                print(f"      ğŸ¯ å¹³å‡è´¨é‡: {avg_score:.3f}")
            else:
                print(f"   âŒ æ— å¯ç”¨æº")
        else:
            print(f"   âš   æ— åŒ¹é…æº")
    
    if not final_channels_data:
        logger.error("æ‰€æœ‰é¢‘é“æµ‹è¯•åéƒ½æ²¡æœ‰å¯ç”¨çš„æº")
        return 1
    
    # 6. ä¿å­˜ç»“æœ
    print("\n" + "="*50)
    print("ğŸ’¾ æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
    
    save_success = save_to_txt(final_channels_data) and save_to_m3u(final_channels_data)
    
    if save_success:
        # æœ€ç»ˆç»Ÿè®¡
        total_sources = sum(len(sources) for _, sources in final_channels_data)
        total_local = sum(sum(1 for s in sources if s.source_type == "local") for _, sources in final_channels_data)
        total_time = time.time() - start_time
        
        print("\n" + "="*70)
        print("ğŸ‰ ä»»åŠ¡å®Œæˆ!")
        print("="*70)
        print(f"âœ… æˆåŠŸå¤„ç†: {successful_channels}/{len(template_channels)} ä¸ªé¢‘é“")
        print(f"âœ… æ€»å…±ä¿ç•™: {total_sources} ä¸ªé«˜è´¨é‡æº")
        print(f"âœ… å…¶ä¸­æœ¬åœ°æº: {total_local} ä¸ª")
        print(f"âœ… ç½‘ç»œæº: {total_sources - total_local} ä¸ª")
        print(f"â± æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“„ {CONFIG.output_txt} (æ–‡æœ¬æ ¼å¼)")
        print(f"   ğŸ“º {CONFIG.output_m3u} (M3Uæ’­æ”¾åˆ—è¡¨)")
        print("="*70)
        
        logger.info(f"ä»»åŠ¡å®Œæˆ - å¤„ç†{successful_channels}ä¸ªé¢‘é“ï¼Œä¿ç•™{total_sources}ä¸ªæºï¼Œè€—æ—¶{total_time:.1f}ç§’")
        return 0
    else:
        logger.error("æ–‡ä»¶ä¿å­˜å¤±è´¥")
        return 1

# ==================================================
# ä¸»å‡½æ•°
# ==================================================

def main() -> int:
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    start_time = time.time()
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    try:
        print("=" * 70)
        print(f"ğŸ¬ IPTVç›´æ’­æºæŠ“å–ä¸ä¼˜åŒ–å·¥å…· v{CONFIG.version}")
        print("=" * 70)
        
        # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        logger.info(f"Pythonç‰ˆæœ¬: {sys.version}")
        logger.info(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
        logger.info(f"å·¥ä½œç›®å½•: {os.getcwd()}")
        
        # è¿è¡Œå®Œæ•´æ€§æµ‹è¯•
        print("\nğŸ”¬ è¿è¡Œå®Œæ•´æ€§æµ‹è¯•...")
        if not run_basic_tests():
            print("âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return 1
        
        if not run_network_tests():
            print("âŒ ç½‘ç»œæµ‹è¯•å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return 1
        
        print("\nâœ… æ‰€æœ‰å®Œæ•´æ€§æµ‹è¯•é€šè¿‡")
        
        # æ˜¾ç¤ºé…ç½®çŠ¶æ€
        print_config_summary()
        
        # éªŒè¯é…ç½®
        errors, warnings = validate_configuration()
        
        if warnings:
            print("âš  è­¦å‘Šä¿¡æ¯:")
            for warning in warnings:
                print(f"  - {warning}")
            print()
        
        if errors:
            print("âŒ é…ç½®é”™è¯¯:")
            for error in errors:
                print(f"  - {error}")
            print("\nğŸ’¡ è¯·è§£å†³ä¸Šè¿°é—®é¢˜åé‡æ–°è¿è¡Œç¨‹åº")
            return 1
        
        # æ£€æŸ¥ä¾èµ–
        if not check_dependencies():
            return 1
        
        # æ‰§è¡Œä¸»è¦æµç¨‹
        return run_main_workflow(start_time)
            
    except KeyboardInterrupt:
        print("\n\nâ¹ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\n\nğŸ’¥ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        logger.exception("ç¨‹åºæ‰§è¡Œå¼‚å¸¸")
        return 1
    finally:
        cleanup_resources()

if __name__ == "__main__":
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = CONFIG.to_dict()
    logger.info(f"ç¨‹åºå¯åŠ¨é…ç½®: {json.dumps(config_info, indent=2, ensure_ascii=False)}")
    
    exit_code = main()
    
    # è®°å½•ç¨‹åºç»“æŸ
    logger.info(f"ç¨‹åºé€€å‡ºï¼Œä»£ç : {exit_code}")
    sys.exit(exit_code)
