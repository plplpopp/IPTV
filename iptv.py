import time
import requests
import json
import re
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('iptv_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ä¸»æœåŠ¡å™¨åˆ—è¡¨
MAIN_SERVERS = [
    "http://60.214.107.42:8080",
    "http://113.57.127.43:8080", 
    "http://58.222.24.11:8080",
    "http://117.169.120.140:8080",
    "http://112.30.144.207:8080",
    "http://60.214.107.42:8080",
    "http://113.57.127.43:8080",
    "http://58.222.24.11:8080",
    "http://117.169.120.140:8080",
    "http://112.30.144.207:8080"
]

# é…ç½®å¸¸é‡
CONFIG = {
    'timeout': 8,
    'max_workers': 10,
    'request_delay': 0.5
}

class IPTVCollector:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Connection': 'keep-alive'
        })

    def clean_channel_name(self, name):
        """æ¸…ç†é¢‘é“åç§°"""
        if not name:
            return ""
        
        # é¢‘é“åç§°æ˜ å°„è¡¨
        name_mappings = {
            r"ä¸­å¤®(\d+)": r"CCTV\1",
            r"CCTV-?(\d+)": r"CCTV\1",
            r"é«˜æ¸…": "",
            r"HD": "",
            r"è¶…æ¸…": "",
            r"æ ‡æ¸…": "",
            r"é¢‘é“": "",
            r"å°": "",
            r"[-_\s]": "",
            r"PLUS": "+",
            r"[()ï¼ˆï¼‰]": "",
            r"CCTV1ç»¼åˆ": "CCTV1",
            r"CCTV2è´¢ç»": "CCTV2",
            r"CCTV3ç»¼è‰º": "CCTV3",
            r"CCTV4ä¸­æ–‡å›½é™…": "CCTV4",
            r"CCTV4å›½é™…": "CCTV4",
            r"CCTV5ä½“è‚²": "CCTV5",
            r"CCTV6ç”µå½±": "CCTV6",
            r"CCTV7å†›äº‹": "CCTV7",
            r"CCTV7å†›å†œ": "CCTV7",
            r"CCTV7å›½é˜²å†›äº‹": "CCTV7",
            r"CCTV8ç”µè§†å‰§": "CCTV8",
            r"CCTV9è®°å½•": "CCTV9",
            r"CCTV9çºªå½•": "CCTV9",
            r"CCTV10ç§‘æ•™": "CCTV10",
            r"CCTV11æˆæ›²": "CCTV11",
            r"CCTV12ç¤¾ä¼šä¸æ³•": "CCTV12",
            r"CCTV13æ–°é—»": "CCTV13",
            r"CCTVæ–°é—»": "CCTV13",
            r"CCTV14å°‘å„¿": "CCTV14",
            r"CCTV15éŸ³ä¹": "CCTV15",
            r"CCTV16å¥¥æ—åŒ¹å…‹": "CCTV16",
            r"CCTV17å†œä¸šå†œæ‘": "CCTV17",
            r"CCTV5\+ä½“è‚²èµ›è§†": "CCTV5+",
            r"CCTV5\+ä½“è‚²èµ›äº‹": "CCTV5+"
        }
        
        cleaned_name = str(name)
        for pattern, replacement in name_mappings.items():
            cleaned_name = re.sub(pattern, replacement, cleaned_name)
        
        return cleaned_name.strip()

    def is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False

    def test_channel_quality(self, channel_url):
        """æµ‹è¯•é¢‘é“è´¨é‡"""
        try:
            start_time = time.time()
            response = self.session.head(channel_url, timeout=5, allow_redirects=True)
            response_time = time.time() - start_time
            
            if response.status_code in [200, 302]:
                return True, response_time
            return False, response_time
        except:
            return False, float('inf')

    def process_single_server(self, server_url):
        """å¤„ç†å•ä¸ªæœåŠ¡å™¨"""
        results = []
        try:
            # å¯èƒ½çš„JSONè·¯å¾„åˆ—è¡¨
            json_paths = [
                "/iptv/live/1000.json?key=txiptv",
                "/iptv/live/1000.json",
                "/live/1000.json",
                "/tv/1000.json",
                "/iptv/live.json",
                "/api/live",
                "/live.json"
            ]
            
            for json_path in json_paths:
                try:
                    # æ„å»ºå®Œæ•´çš„JSON URL
                    if json_path.startswith('/'):
                        json_url = f"{server_url}{json_path}"
                    else:
                        json_url = f"{server_url}/{json_path}"
                    
                    logging.info(f"Trying: {json_url}")
                    
                    response = self.session.get(json_url, timeout=CONFIG['timeout'])
                    if response.status_code == 200:
                        try:
                            json_data = response.json()
                        except json.JSONDecodeError:
                            logging.warning(f"Invalid JSON from {json_url}")
                            continue
                        
                        channel_count = 0
                        # å¤„ç†ä¸åŒçš„JSONç»“æ„
                        data_list = json_data.get('data', [])
                        if not data_list and isinstance(json_data, list):
                            data_list = json_data
                        
                        for item in data_list:
                            if isinstance(item, dict):
                                name = item.get('name', '') or item.get('title', '')
                                urlx = item.get('url', '') or item.get('link', '')
                                
                                if name and urlx:
                                    cleaned_name = self.clean_channel_name(name)
                                    
                                    # æ„å»ºå®Œæ•´çš„é¢‘é“URL
                                    if urlx.startswith(('http://', 'https://')):
                                        full_url = urlx
                                    elif urlx.startswith('/'):
                                        full_url = f"{server_url}{urlx}"
                                    else:
                                        full_url = f"{server_url}/{urlx}"
                                    
                                    if self.is_valid_url(full_url):
                                        # æµ‹è¯•é¢‘é“å¯ç”¨æ€§
                                        is_available, response_time = self.test_channel_quality(full_url)
                                        if is_available:
                                            results.append({
                                                'name': cleaned_name,
                                                'url': full_url,
                                                'response_time': response_time,
                                                'source': server_url,
                                                'quality': 'GOOD' if response_time < 1.0 else 'NORMAL'
                                            })
                                            channel_count += 1
                        
                        if channel_count > 0:
                            logging.info(f"âœ… Found {channel_count} channels from {server_url} using {json_path}")
                            break  # æ‰¾åˆ°æœ‰æ•ˆè·¯å¾„å°±åœæ­¢å°è¯•
                        else:
                            logging.warning(f"âŒ No valid channels found from {server_url} using {json_path}")
                            
                except requests.RequestException as e:
                    logging.debug(f"Request failed for {json_path}: {e}")
                    continue
                except Exception as e:
                    logging.debug(f"Error processing {json_path}: {e}")
                    continue
                    
            if not results:
                logging.warning(f"âŒ No channels found from {server_url} after trying all paths")
                    
        except Exception as e:
            logging.error(f"âŒ Error processing server {server_url}: {str(e)}")
        
        return results

    def process_all_servers(self):
        """å¤„ç†æ‰€æœ‰æœåŠ¡å™¨"""
        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸš€ Starting IPTV collection from {len(MAIN_SERVERS)} servers")
        logging.info(f"{'='*60}")
        
        try:
            all_results = []
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰€æœ‰æœåŠ¡å™¨
            with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
                future_to_server = {
                    executor.submit(self.process_single_server, server): server 
                    for server in MAIN_SERVERS
                }
                
                completed = 0
                total = len(future_to_server)
                
                for future in as_completed(future_to_server):
                    completed += 1
                    server_url = future_to_server[future]
                    try:
                        results = future.result()
                        if results:
                            all_results.extend(results)
                            logging.info(f"âœ… [{completed}/{total}] {server_url}: {len(results)} channels")
                        else:
                            logging.info(f"âŒ [{completed}/{total}] {server_url}: No channels found")
                    except Exception as e:
                        logging.error(f"âŒ [{completed}/{total}] {server_url}: Failed - {e}")
            
            # æŒ‰å“åº”æ—¶é—´æ’åº
            all_results.sort(key=lambda x: x['response_time'])
            
            logging.info(f"ğŸ“Š Total: {len(all_results)} channels from {len(MAIN_SERVERS)} servers")
            return all_results
            
        except Exception as e:
            logging.error(f"âŒ Error processing servers: {str(e)}")
            return []

    def save_results(self, results, filename, format='txt'):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not results:
            logging.warning(f"âš ï¸ No results to save for {filename}")
            return False
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
            
            if format == 'txt':
                with open(filename, "w", encoding="utf-8") as file:
                    file.write("# IPTV Channel List\n")
                    file.write(f"# Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    file.write(f"# Total Channels: {len(results)}\n")
                    file.write("# Format: ChannelName,URL\n")
                    file.write("# \n")
                    for result in results:
                        file.write(f"{result['name']},{result['url']}\n")
            
            elif format == 'm3u':
                with open(filename, "w", encoding="utf-8") as file:
                    file.write("#EXTM3U\n")
                    file.write(f"# Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    file.write(f"# Total Channels: {len(results)}\n")
                    file.write("# \n")
                    for result in results:
                        file.write(f"#EXTINF:-1 tvg-name=\"{result['name']}\",{result['name']}\n")
                        file.write(f"{result['url']}\n")
            
            logging.info(f"ğŸ’¾ Saved {len(results)} results to {filename}")
            return True
            
        except Exception as e:
            logging.error(f"âŒ Error saving results to {filename}: {e}")
            return False

def remove_duplicate_channels(results):
    """å»é™¤é‡å¤é¢‘é“ï¼ˆåŸºäºåç§°å’ŒURLï¼‰"""
    unique_channels = {}
    
    for result in results:
        key = (result['name'].lower(), result['url'])
        # ä¿ç•™å“åº”æ—¶é—´æ›´çŸ­çš„ç‰ˆæœ¬
        if key not in unique_channels or result['response_time'] < unique_channels[key]['response_time']:
            unique_channels[key] = result
    
    return list(unique_channels.values())

def generate_stats(results):
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    if not results:
        return
    
    stats = {
        'cctv_count': 0,
        'local_count': 0,
        'other_count': 0,
        'sources': set(),
        'quality_good': 0,
        'quality_normal': 0
    }
    
    for result in results:
        stats['sources'].add(result['source'])
        if result['quality'] == 'GOOD':
            stats['quality_good'] += 1
        else:
            stats['quality_normal'] += 1
            
        if 'CCTV' in result['name']:
            stats['cctv_count'] += 1
        elif any(province in result['name'] for province in ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿ä¸œ', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹']):
            stats['local_count'] += 1
        else:
            stats['other_count'] += 1
    
    logging.info(f"ğŸ“Š Channel Statistics:")
    logging.info(f"   ğŸ“º CCTV Channels: {stats['cctv_count']}")
    logging.info(f"   ğŸ  Local Channels: {stats['local_count']}")
    logging.info(f"   ğŸ”„ Other Channels: {stats['other_count']}")
    logging.info(f"   âœ… Good Quality: {stats['quality_good']}")
    logging.info(f"   âš ï¸  Normal Quality: {stats['quality_normal']}")
    logging.info(f"   ğŸŒ Unique Sources: {len(stats['sources'])}")

def main():
    """ä¸»å‡½æ•°"""
    collector = IPTVCollector()
    start_time = time.time()
    
    logging.info("ğŸš€ Starting IPTV collection process...")
    logging.info(f"ğŸ“¡ Total servers to process: {len(MAIN_SERVERS)}")
    
    # å¤„ç†æ‰€æœ‰æœåŠ¡å™¨
    all_results = collector.process_all_servers()
    
    # åˆå¹¶æ‰€æœ‰ç»“æœå¹¶å»é‡
    if all_results:
        logging.info(f"\nğŸ”„ Merging and deduplicating {len(all_results)} channels...")
        final_results = remove_duplicate_channels(all_results)
        final_results.sort(key=lambda x: (x['name'], x['response_time']))
        
        # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
        collector.save_results(final_results, "iptv.txt", 'txt')
        collector.save_results(final_results, "iptv.m3u", 'm3u')
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        generate_stats(final_results)
        
        logging.info(f"âœ… Merged {len(all_results)} channels into {len(final_results)} unique channels")
    else:
        logging.error("âŒ No valid channels collected!")
        # åˆ›å»ºç©ºçš„IPTVæ–‡ä»¶
        with open("iptv.txt", "w", encoding="utf-8") as f:
            f.write("# No channels collected\n")
        with open("iptv.m3u", "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n# No channels collected\n")
    
    # è¾“å‡ºæ€»ç»“
    end_time = time.time()
    processing_time = end_time - start_time
    
    logging.info(f"\n{'='*60}")
    logging.info("ğŸ“Š COLLECTION SUMMARY")
    logging.info(f"{'='*60}")
    logging.info(f"â±ï¸  Processing time: {processing_time:.2f} seconds")
    logging.info(f"ğŸ“º Total channels collected: {len(all_results)}")
    logging.info(f"ğŸ¯ Unique channels: {len(final_results) if all_results else 0}")
    logging.info(f"ğŸ“¡ Servers processed: {len(MAIN_SERVERS)}")
    if all_results:
        logging.info(f"ğŸ“ˆ Success rate: {len([r for r in all_results]) / len(MAIN_SERVERS) * 100:.1f}%")
    logging.info(f"ğŸ’¾ Output files: iptv.txt, iptv.m3u")
    logging.info(f"{'='*60}")

if __name__ == "__main__":
    main()
