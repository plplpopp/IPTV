import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
from typing import List, Dict, Optional, Tuple, Set
from urllib.parse import urlparse

class IPTV:
    """IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·"""
    
    def __init__(self, timeout=8, max_workers=10, test_size_kb=64):
        """
        åˆå§‹åŒ–å·¥å…·
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            test_size_kb: æµ‹é€Ÿæ•°æ®å¤§å°ï¼ˆKBï¼‰
        """
        # é…ç½®å‚æ•°
        self.timeout = timeout
        self.max_workers = max_workers
        self.test_size = test_size_kb * 1024
        
        # è¯·æ±‚ä¼šè¯é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Encoding': 'gzip, deflate',
            'Accept': '*/*',
            'Connection': 'keep-alive'
        })
        
        # æ•°æ®æºé…ç½® - æ›´å¤šç›´æ’­æº
        self.source_urls = [
            "https://raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
            "https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
            "https://raw.githubusercontent.com/Guovin/iptv-api/gd/output/ipv4/result.m3u",  
            "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
            "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
            "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
            "https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt",
            "http://47.120.41.246:8899/zb.txt",
        ]
        
        # æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘
        self.ipv4_pattern = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')
        self.ipv6_pattern = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')
        self.channel_pattern = re.compile(r'^([^,#]+)')
        self.extinf_pattern = re.compile(r'#EXTINF:.*?,(.+)')
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.template_file = os.path.join(os.path.dirname(__file__), "demo.txt")
        self.output_files = {
            'txt': os.path.join(os.path.dirname(__file__), "iptv.txt"),
            'm3u': os.path.join(os.path.dirname(__file__), "iptv.m3u"),
        }
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.template_channels = self.load_template_channels()
        self.all_streams = []

    def load_template_channels(self) -> Set[str]:
        """åŠ è½½æ¨¡æ¿æ–‡ä»¶ä¸­çš„é¢‘é“åˆ—è¡¨"""
        channels = set()
        if not os.path.exists(self.template_file):
            print(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ {self.template_file} ä¸å­˜åœ¨ï¼Œå°†å¤„ç†æ‰€æœ‰é¢‘é“")
            return channels
        
        try:
            with open(self.template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if match := self.channel_pattern.match(line):
                            channels.add(match.group(1).strip())
            print(f"åŠ è½½æ¨¡æ¿é¢‘é“ {len(channels)} ä¸ª")
        except Exception as e:
            print(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶é”™è¯¯: {str(e)}")
        
        return channels

    # ==================== æ•°æ®è·å–ä¸å¤„ç† ====================
    
    def fetch_streams(self) -> Optional[str]:
        """ä»æ‰€æœ‰æºURLæŠ“å–ç›´æ’­æº"""
        contents = []
        successful_sources = 0
        
        for url in self.source_urls:
            print(f"æŠ“å–æº: {self._extract_domain(url)}")
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                # éªŒè¯å†…å®¹æœ‰æ•ˆæ€§
                if self.validate_content(response.text):
                    contents.append(response.text)
                    successful_sources += 1
                    print(f"  âœ“ æˆåŠŸ")
                else:
                    print(f"  âš ï¸ å†…å®¹æ— æ•ˆ")
                    
            except Exception as e:
                print(f"  âœ— å¤±è´¥: {str(e)}")
        
        print(f"æˆåŠŸæŠ“å– {successful_sources}/{len(self.source_urls)} ä¸ªæº")
        return "\n".join(contents) if contents else None

    def validate_content(self, content: str) -> bool:
        """éªŒè¯å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç›´æ’­æºæ ¼å¼"""
        lines = content.splitlines()
        valid_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if line.startswith('http') or (',' in line and 'http' in line):
                valid_lines += 1
        
        return valid_lines >= 5

    def parse_content(self, content: str) -> pd.DataFrame:
        """è§£æç›´æ’­æºå†…å®¹"""
        streams = []
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
        if content.startswith("#EXTM3U"):
            streams.extend(self.parse_m3u_content(content))
        else:
            streams.extend(self.parse_txt_content(content))
        
        if not streams:
            print("æœªè§£æåˆ°æœ‰æ•ˆç›´æ’­æº")
            return pd.DataFrame(columns=['program_name', 'stream_url'])
        
        df = pd.DataFrame(streams)
        
        # æ•°æ®æ¸…æ´—
        df = self.clean_stream_data(df)
        
        print(f"è§£æåˆ° {len(df)} ä¸ªç›´æ’­æºï¼Œ{len(df['program_name'].unique())} ä¸ªé¢‘é“")
        return df

    def parse_m3u_content(self, content: str) -> List[Dict]:
        """è§£æM3Uæ ¼å¼å†…å®¹"""
        streams = []
        current_program = None
        
        for line in content.splitlines():
            line = line.strip()
            if line.startswith("#EXTINF"):
                # å°è¯•å¤šç§æ ¼å¼æå–èŠ‚ç›®å
                if match := re.search(r'tvg-name="([^"]+)"', line):
                    current_program = match.group(1).strip()
                elif match := self.extinf_pattern.search(line):
                    current_program = match.group(1).strip()
            elif line.startswith("http"):
                if current_program:
                    streams.append({
                        "program_name": current_program,
                        "stream_url": line
                    })
                current_program = None
        
        return streams

    def parse_txt_content(self, content: str) -> List[Dict]:
        """è§£æTXTæ ¼å¼å†…å®¹"""
        streams = []
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            # æ”¯æŒå¤šç§åˆ†éš”ç¬¦
            if ',' in line:
                parts = line.split(',', 1)
            elif ' ' in line and 'http' in line:
                parts = line.split(' ', 1)
            else:
                continue
                
            if len(parts) == 2:
                program_name = parts[0].strip()
                stream_url = parts[1].strip()
                
                if program_name and stream_url.startswith('http'):
                    streams.append({
                        "program_name": program_name,
                        "stream_url": stream_url
                    })
        
        return streams

    def clean_stream_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        if df.empty:
            return df
        
        # å»é™¤èŠ‚ç›®åä¸­çš„å¤šä½™ç©ºæ ¼
        df['program_name'] = df['program_name'].str.strip()
        
        # è¿‡æ»¤æ— æ•ˆURL
        initial_count = len(df)
        df = df[df['stream_url'].str.startswith('http')]
        
        # å»é™¤æ˜æ˜¾æ— æ•ˆçš„èŠ‚ç›®å
        invalid_names = ['', 'None', 'null', 'undefined']
        df = df[~df['program_name'].isin(invalid_names)]
        
        print(f"æ•°æ®æ¸…æ´—: {initial_count} -> {len(df)} ä¸ªæº")
        return df

    def organize_streams(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•´ç†ç›´æ’­æºæ•°æ®ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤šä¿ç•™30ä¸ªæºç”¨äºæµ‹é€Ÿ"""
        grouped = df.groupby('program_name')['stream_url'].apply(list).reset_index()
        
        # é™åˆ¶æ¯ä¸ªé¢‘é“çš„æºæ•°é‡ä¸º30ä¸ª
        grouped['stream_url'] = grouped['stream_url'].apply(lambda x: x[:30])
        
        print(f"æ•´ç†å: {len(grouped)} ä¸ªé¢‘é“ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤š30ä¸ªæº")
        return grouped

    # ==================== å¢å¼ºæµ‹é€ŸåŠŸèƒ½ ====================
    
    def test_single_url(self, url: str) -> Tuple[Optional[float], Optional[str], float]:
        """æµ‹è¯•å•ä¸ªURLçš„é€Ÿåº¦ï¼Œè¿”å›é€Ÿåº¦ã€é”™è¯¯ä¿¡æ¯å’Œå“åº”æ—¶é—´"""
        start_time = time.time()
        
        try:
            response = self.session.get(
                url, 
                timeout=self.timeout, 
                stream=True,
                headers={'Range': f'bytes=0-{self.test_size-1}'}
            )
            response_time = time.time() - start_time
            
            if response.status_code not in [200, 206]:
                return None, f"HTTP {response.status_code}", response_time
            
            content_length = 0
            chunk_start_time = time.time()
            
            for chunk in response.iter_content(chunk_size=8192):
                if not chunk:
                    break
                    
                content_length += len(chunk)
                if content_length >= self.test_size:
                    break
                    
                # æ£€æŸ¥ä¸‹è½½æ˜¯å¦è¶…æ—¶
                if time.time() - chunk_start_time > self.timeout:
                    return None, "ä¸‹è½½è¶…æ—¶", response_time
            
            if content_length == 0:
                return 0, "æ— æ•°æ®", response_time
            
            total_time = time.time() - start_time
            speed = content_length / total_time / 1024  # KB/s
            
            return speed, None, response_time
            
        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶", time.time() - start_time
        except requests.exceptions.SSLError:
            return None, "SSLé”™è¯¯", time.time() - start_time
        except requests.exceptions.ConnectionError:
            return None, "è¿æ¥å¤±è´¥", time.time() - start_time
        except requests.exceptions.HTTPError as e:
            return None, f"HTTPé”™è¯¯ {e.response.status_code}", time.time() - start_time
        except Exception as e:
            return None, f"é”™è¯¯: {str(e)}", time.time() - start_time

    def test_urls_concurrently(self, urls: List[str]) -> List[Tuple[str, Optional[float], Optional[str], float]]:
        """å¹¶å‘æµ‹è¯•URLåˆ—è¡¨"""
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.test_single_url, url): url for url in urls}
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                speed, error, response_time = future.result()
                results.append((url, speed, error, response_time))
        return results

    def test_all_channels(self, grouped_streams: pd.DataFrame) -> Dict[str, List[Tuple[str, float]]]:
        """æµ‹è¯•æ‰€æœ‰é¢‘é“å¹¶ä¿ç•™æœ€ä½³8ä¸ªæº"""
        results = {}
        total_channels = len(grouped_streams)
        tested_channels = 0
        successful_channels = 0
        
        print(f"\nå¼€å§‹æµ‹é€Ÿ {total_channels} ä¸ªé¢‘é“")
        print("æ¯ä¸ªé¢‘é“æµ‹è¯•æœ€å¤š30ä¸ªæºï¼Œä¿ç•™æœ€ä¼˜8ä¸ª")
        
        for idx, (_, row) in enumerate(grouped_streams.iterrows(), 1):
            channel = row['program_name']
            urls = row['stream_url']
            
            print(f"[{idx}/{total_channels}] æµ‹è¯•é¢‘é“: {channel} ({len(urls)}ä¸ªæº)")
            
            test_results = self.test_urls_concurrently(urls)
            valid_streams = []
            
            fast_count = 0
            medium_count = 0
            slow_count = 0
            failed_count = 0
            
            for url, speed, error, response_time in test_results:
                if speed is not None:
                    valid_streams.append((url, speed))
                    if speed > 500:
                        fast_count += 1
                    elif speed > 100:
                        medium_count += 1
                    else:
                        slow_count += 1
                else:
                    failed_count += 1
            
            # æŒ‰é€Ÿåº¦æ’åºå¹¶ä¿ç•™æœ€ä½³8ä¸ª
            valid_streams.sort(key=lambda x: x[1], reverse=True)
            best_streams = valid_streams[:8]
            results[channel] = best_streams
            
            tested_channels += 1
            if best_streams:
                successful_channels += 1
                best_speed = best_streams[0][1]
                print(f"  âœ… æˆåŠŸ: æœ€ä½³é€Ÿåº¦ {best_speed:.1f} KB/s")
                print(f"     è¯¦æƒ…: å¿«é€Ÿ{fast_count} ä¸­é€Ÿ{medium_count} æ…¢é€Ÿ{slow_count} å¤±è´¥{failed_count}")
                print(f"     ä¿ç•™: {len(best_streams)}ä¸ªæœ€ä¼˜æº")
            else:
                print(f"  âŒ å¤±è´¥: æ— æœ‰æ•ˆæº")
        
        print(f"\næµ‹é€Ÿå®Œæˆ: {successful_channels}/{tested_channels} ä¸ªé¢‘é“æœ‰æœ‰æ•ˆæº")
        return results

    # ==================== æ¨¡æ¿åŒ¹é…å’Œç»“æœç”Ÿæˆ ====================
    
    def filter_by_template(self, speed_results: Dict[str, List[Tuple[str, float]]]) -> Dict[str, List[Tuple[str, float]]]:
        """æ ¹æ®æ¨¡æ¿é¢‘é“è¿‡æ»¤ç»“æœ"""
        if not self.template_channels:
            print("æœªä½¿ç”¨æ¨¡æ¿è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰é¢‘é“")
            return speed_results
        
        filtered_results = {}
        matched_count = 0
        
        for channel in self.template_channels:
            if channel in speed_results and speed_results[channel]:
                filtered_results[channel] = speed_results[channel]
                matched_count += 1
        
        print(f"æ¨¡æ¿åŒ¹é…: {matched_count}/{len(self.template_channels)} ä¸ªé¢‘é“")
        
        # æ˜¾ç¤ºæœªåŒ¹é…çš„æ¨¡æ¿é¢‘é“
        unmatched = self.template_channels - set(speed_results.keys())
        if unmatched:
            print(f"æœªæ‰¾åˆ°æºçš„æ¨¡æ¿é¢‘é“: {len(unmatched)}ä¸ª")
            for channel in list(unmatched)[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {channel}")
            if len(unmatched) > 10:
                print(f"  ... è¿˜æœ‰ {len(unmatched) - 10} ä¸ª")
        
        return filtered_results

    def generate_output_files(self, speed_results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        self.generate_txt_file(speed_results)
        self.generate_m3u_file(speed_results)
        self.generate_report(speed_results)

    def generate_txt_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆTXTæ ¼å¼æ–‡ä»¶"""
        categories = {
            "å¤®è§†é¢‘é“,#genre#": ["CCTV", "å¤®è§†"],
            "å«è§†é¢‘é“,#genre#": ["å«è§†", "æ¹–å—", "æµ™æ±Ÿ", "æ±Ÿè‹", "ä¸œæ–¹", "åŒ—äº¬"],
            "åœ°æ–¹é¢‘é“,#genre#": ["é‡åº†", "å¹¿ä¸œ", "æ·±åœ³", "å—æ–¹", "å¤©æ´¥", "æ²³åŒ—"],
            "æ¸¯æ¾³é¢‘é“,#genre#": ["å‡¤å‡°", "ç¿¡ç¿ ", "æ˜ç ", "æ¾³äºš"],
            "å…¶ä»–é¢‘é“,#genre#": []
        }
        
        categorized = {cat: [] for cat in categories}
        
        for channel in self.get_ordered_channels(results.keys()):
            streams = results.get(channel, [])
            if not streams:
                continue
                
            matched = False
            for cat, keywords in categories.items():
                if any(keyword in channel for keyword in keywords):
                    categorized[cat].extend(
                        f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                        for url, speed in streams
                    )
                    matched = True
                    break
            
            if not matched:
                categorized["å…¶ä»–é¢‘é“,#genre#"].extend(
                    f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                    for url, speed in streams
                )
        
        with open(self.output_files['txt'], 'w', encoding='utf-8') as f:
            for cat, items in categorized.items():
                if items:
                    f.write(f"\n{cat}\n")
                    f.write("\n".join(items) + "\n")
        
        print(f"ç”ŸæˆTXTæ–‡ä»¶: {self.output_files['txt']}")

    def generate_m3u_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆM3Uæ ¼å¼æ–‡ä»¶"""
        with open(self.output_files['m3u'], 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            
            for channel in self.get_ordered_channels(results.keys()):
                streams = results.get(channel, [])
                for url, speed in streams:
                    quality = self.get_speed_quality(speed)
                    f.write(f'#EXTINF:-1 tvg-name="{channel}",{channel} [é€Ÿåº¦: {speed:.1f}KB/s {quality}]\n{url}\n')
        
        print(f"ç”ŸæˆM3Uæ–‡ä»¶: {self.output_files['m3u']}")

    def generate_report(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæµ‹é€ŸæŠ¥å‘Š"""
        speed_stats = []
        valid_channels = []
        total_sources = 0
        
        for channel, streams in results.items():
            if streams:
                best_speed = streams[0][1]
                speed_stats.append(best_speed)
                valid_channels.append((channel, best_speed, len(streams)))
                total_sources += len(streams)
        
        if not speed_stats:
            print("âš ï¸ æ— æœ‰æ•ˆæµ‹é€Ÿç»“æœ")
            return
        
        # æŒ‰é€Ÿåº¦æ’åºé¢‘é“
        valid_channels.sort(key=lambda x: x[1], reverse=True)
        
        print("\n" + "="*50)
        print("æµ‹é€ŸæŠ¥å‘Š")
        print("="*50)
        print(f"æœ‰æ•ˆé¢‘é“æ•°: {len(valid_channels)}")
        print(f"æ€»æºæ•°é‡: {total_sources}")
        print(f"å¹³å‡é€Ÿåº¦: {sum(speed_stats)/len(speed_stats):.1f} KB/s")
        print(f"æœ€å¿«é€Ÿåº¦: {max(speed_stats):.1f} KB/s")
        print(f"æœ€æ…¢é€Ÿåº¦: {min(speed_stats):.1f} KB/s")
        
        print("\né¢‘é“é€Ÿåº¦æ’å TOP 20:")
        for i, (channel, speed, count) in enumerate(valid_channels[:20], 1):
            print(f"{i:2d}. {channel}: {speed:.1f} KB/s ({count}ä¸ªæº)")

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def get_ordered_channels(self, channels: List[str]) -> List[str]:
        """æŒ‰ç…§æ¨¡æ¿é¡ºåºæ’åºé¢‘é“åˆ—è¡¨"""
        if not self.template_channels:
            return sorted(channels)
        
        ordered = []
        # é¦–å…ˆæŒ‰æ¨¡æ¿é¡ºåº
        with open(self.template_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if match := self.channel_pattern.match(line):
                        channel = match.group(1).strip()
                        if channel in channels and channel not in ordered:
                            ordered.append(channel)
        
        # æ·»åŠ æœªåœ¨æ¨¡æ¿ä¸­çš„é¢‘é“ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼Œå› ä¸ºå·²ç»è¿‡æ»¤äº†ï¼‰
        for channel in channels:
            if channel not in ordered:
                ordered.append(channel)
                
        return ordered

    def _extract_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            netloc = urlparse(url).netloc
            return netloc.split(':')[0]
        except:
            return url[:30] + "..." if len(url) > 30 else url

    def get_speed_quality(self, speed: float) -> str:
        """è·å–é€Ÿåº¦è´¨é‡è¯„çº§"""
        if speed > 1000: return "æä½³"
        if speed > 500: return "ä¼˜ç§€"
        if speed > 200: return "è‰¯å¥½"
        if speed > 100: return "ä¸€èˆ¬"
        if speed > 50: return "è¾ƒå·®"
        return "æå·®"

    # ==================== ä¸»æµç¨‹ ====================
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("="*50)
        print("IPTVç›´æ’­æºå¤„ç†å·¥å…·")
        print("="*50)
        
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæŠ“å–æ‰€æœ‰ç›´æ’­æº
            print("\nç¬¬ä¸€æ­¥ï¼šæŠ“å–ç›´æ’­æº...")
            content = self.fetch_streams()
            if not content:
                print("âŒ æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®")
                return
            
            # ç¬¬äºŒæ­¥ï¼šè§£æå’Œæ•´ç†æ•°æ®
            print("\nç¬¬äºŒæ­¥ï¼šè§£æç›´æ’­æºæ•°æ®...")
            df = self.parse_content(content)
            if df.empty:
                print("âŒ æœªè§£æåˆ°æœ‰æ•ˆç›´æ’­æº")
                return
            
            # ç¬¬ä¸‰æ­¥ï¼šæ•´ç†æ•°æ®ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤š30ä¸ªæº
            grouped = self.organize_streams(df)
            
            # ç¬¬å››æ­¥ï¼šå¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ
            print("\nç¬¬ä¸‰æ­¥ï¼šæµ‹é€Ÿä¼˜åŒ–...")
            speed_results = self.test_all_channels(grouped)
            
            # ç¬¬äº”æ­¥ï¼šæ ¹æ®æ¨¡æ¿é¢‘é“è¿‡æ»¤ç»“æœ
            print("\nç¬¬å››æ­¥ï¼šæ¨¡æ¿åŒ¹é…...")
            filtered_results = self.filter_by_template(speed_results)
            
            # ç¬¬å…­æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            print("\nç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
            self.generate_output_files(filtered_results)
            
            # å®Œæˆç»Ÿè®¡
            total_time = time.time() - start_time
            print(f"\nğŸ‰ å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        'timeout': 6,      # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        'max_workers': 8,  # æœ€å¤§å¹¶å‘æ•°ï¼ˆæé«˜å¹¶å‘ï¼‰
        'test_size_kb': 32 # æµ‹é€Ÿæ•°æ®å¤§å°(KB)
    }
    
    tool = IPTV(**config)
    tool.run()
