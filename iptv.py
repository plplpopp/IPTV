import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import requests
import json
import re
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# ä½¿ç”¨å­—å…¸ç®¡ç†URLï¼Œæ›´æ˜“äºç»´æŠ¤
REGION_URLS = {
    "hebei": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iSGViZWki",
    "beijing": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iYmVpamluZyI%3D",
    "guangdong": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iZ3Vhbmdkb25nIg%3D%3D",
    "shanghai": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0ic2hhbmdoYWki",
    "tianjin": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0idGlhbmppbiI%3D",
    "chongqing": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iY2hvbmdxaW5nIg%3D%3D",
    "shanxi": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0ic2hhbnhpIg%3D%3D",
    "shaanxi": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iU2hhYW54aSI%3D",
    "liaoning": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0ibGlhb25pbmci",
    "jiangsu": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iamlhbmdzdSI%3D",
    "zhejiang": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iemhlamlhbmci",
    "anhui": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5a6J5b69Ig%3D%3D",
    "fujian": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0iRnVqaWFuIg%3D%3D",
    "jiangxi": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5rGf6KW%2FIg%3D%3D",
    "shandong": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5bGx5LicIg%3D%3D",
    "henan": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5rKz5Y2XIg%3D%3D",
    "hubei": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5rmW5YyXIg%3D%3D",
    "hunan": "https://fofa.info/result?qbase64=ImlwdHYvbGl2ZS96aF9jbi5qcyIgJiYgY291bnRyeT0iQ04iICYmIHJlZ2lvbj0i5rmW5Y2XIg%3D%3D"
}

# å¤‡ç”¨æœåŠ¡å™¨åˆ—è¡¨ï¼ˆå¦‚æœFOFAæ— æ³•è®¿é—®ï¼‰
BACKUP_SERVERS = [
    "http://60.214.107.42:8080",
    "http://113.57.127.43:8080", 
    "http://58.222.24.11:8080",
    "http://117.169.120.140:8080",
    "http://112.30.144.207:8080",
    "http://60.214.107.42:8080",
    "http://113.57.127.43:8080",
    "http://58.222.24.11:8080",
    "http://117.169.120.140:8080",
    "http://112.30.144.207:8080"
]

def setup_driver():
    """è®¾ç½®Chromeé©±åŠ¨"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # æ·»åŠ æ›´å¤šé€‰é¡¹é¿å…æ£€æµ‹
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    try:
        # ä½¿ç”¨webdriver-managerè‡ªåŠ¨ä¸‹è½½å’Œç®¡ç†ChromeDriver
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    except Exception as e:
        print(f"Error setting up driver: {e}")
        raise

def clean_channel_name(name):
    """æ¸…ç†é¢‘é“åç§°"""
    if not name:
        return ""
    
    # é¢‘é“åç§°æ˜ å°„è¡¨
    mappings = {
        "ä¸­å¤®": "CCTV",
        "é«˜æ¸…": "",
        "HD": "",
        "æ ‡æ¸…": "",
        "é¢‘é“": "",
        "-": "",
        " ": "",
        "PLUS": "+",
        "(": "",
        ")": "",
        "CCTV1ç»¼åˆ": "CCTV1",
        "CCTV2è´¢ç»": "CCTV2",
        "CCTV3ç»¼è‰º": "CCTV3",
        "CCTV4å›½é™…": "CCTV4",
        "CCTV4ä¸­æ–‡å›½é™…": "CCTV4",
        "CCTV5ä½“è‚²": "CCTV5",
        "CCTV6ç”µå½±": "CCTV6",
        "CCTV7å†›äº‹": "CCTV7",
        "CCTV7å†›å†œ": "CCTV7",
        "CCTV7å›½é˜²å†›äº‹": "CCTV7",
        "CCTV8ç”µè§†å‰§": "CCTV8",
        "CCTV9è®°å½•": "CCTV9",
        "CCTV9çºªå½•": "CCTV9",
        "CCTV10ç§‘æ•™": "CCTV10",
        "CCTV11æˆæ›²": "CCTV11",
        "CCTV12ç¤¾ä¼šä¸æ³•": "CCTV12",
        "CCTV13æ–°é—»": "CCTV13",
        "CCTVæ–°é—»": "CCTV13",
        "CCTV14å°‘å„¿": "CCTV14",
        "CCTV15éŸ³ä¹": "CCTV15",
        "CCTV16å¥¥æ—åŒ¹å…‹": "CCTV16",
        "CCTV17å†œä¸šå†œæ‘": "CCTV17",
        "CCTV5+ä½“è‚²èµ›è§†": "CCTV5+",
        "CCTV5+ä½“è‚²èµ›äº‹": "CCTV5+"
    }
        
    for old, new in mappings.items():
        name = name.replace(old, new)
    return name.strip()

def extract_urls_from_page(driver, url):
    """ä»é¡µé¢æå–URL"""
    try:
        print(f"ğŸŒ Accessing FOFA: {url}")
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(15)  # å¢åŠ ç­‰å¾…æ—¶é—´
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°IPåœ°å€
        selectors = [
            "//a[contains(@href, 'http://')]",
            "//span[contains(text(), 'http://')]",
            "//div[contains(text(), 'http://')]",
            "//td[contains(text(), 'http://')]",
            "//code[contains(text(), 'http://')]"
        ]
        
        page_content = driver.page_source
        print(f"ğŸ“„ Page content length: {len(page_content)}")
        
        # ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…IPåœ°å€
        patterns = [
            r"http://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+",
            r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+",
            r"ip.*\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}",
        ]
        
        all_urls = []
        for pattern in patterns:
            matches = re.findall(pattern, page_content, re.IGNORECASE)
            for match in matches:
                if match.startswith('http://'):
                    all_urls.append(match)
                elif ':' in match and match.count('.') == 3:
                    all_urls.append(f"http://{match}")
        
        unique_urls = list(set(all_urls))
        print(f"ğŸ” Found {len(unique_urls)} server URLs using regex")
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å¤‡ç”¨æœåŠ¡å™¨
        if not unique_urls:
            print("âš ï¸ No servers found in FOFA, using backup servers")
            return BACKUP_SERVERS
        
        return unique_urls
        
    except Exception as e:
        print(f"âŒ Error extracting URLs from {url}: {str(e)}")
        print("ğŸ”„ Using backup servers instead")
        return BACKUP_SERVERS

def process_single_server(url):
    """å¤„ç†å•ä¸ªæœåŠ¡å™¨"""
    results = []
    try:
        # å°è¯•å¤šç§å¯èƒ½çš„JSONè·¯å¾„
        json_paths = [
            "/iptv/live/1000.json?key=txiptv",
            "/iptv/live/1000.json",
            "/live/1000.json",
            "/tv/1000.json",
            "/iptv/live.json"
        ]
        
        for json_path in json_paths:
            try:
                json_url = f"{url}{json_path}"
                print(f"ğŸ“¡ Trying: {json_url}")
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/json, text/plain, */*',
                    'Referer': url
                }
                
                response = requests.get(json_url, timeout=8, headers=headers)
                if response.status_code == 200:
                    json_data = response.json()
                    
                    channel_count = 0
                    for item in json_data.get('data', []):
                        if isinstance(item, dict):
                            name = item.get('name')
                            urlx = item.get('url')
                            
                            if name and urlx:
                                cleaned_name = clean_channel_name(name)
                                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                                if urlx.startswith('/'):
                                    full_url = f"{url}{urlx}"
                                else:
                                    full_url = f"{url}/{urlx}"
                                results.append(f"{cleaned_name},{full_url}")
                                channel_count += 1
                    
                    print(f"âœ… Found {channel_count} channels from {url} using {json_path}")
                    break  # æ‰¾åˆ°æœ‰æ•ˆè·¯å¾„å°±åœæ­¢å°è¯•
                    
            except Exception as e:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                
        if not results:
            print(f"âŒ No channels found from {url}")
                    
    except Exception as e:
        print(f"âŒ Error processing server {url}: {str(e)}")
    
    return results

def process_region(region_name, url):
    """å¤„ç†å•ä¸ªåœ°åŒº"""
    print(f"\n{'='*60}")
    print(f"ğŸ Processing {region_name.upper()}")
    print(f"{'='*60}")
    
    driver = setup_driver()
    try:
        # æå–æœåŠ¡å™¨URL
        server_urls = extract_urls_from_page(driver, url)
        print(f"ğŸ“¡ Found {len(server_urls)} servers for {region_name}")
        
        if not server_urls:
            print(f"âš ï¸ No servers available for {region_name}")
            return []
        
        # å¤„ç†æœåŠ¡å™¨
        all_results = []
        success_count = 0
        
        for i, server_url in enumerate(server_urls[:8]):  # é™åˆ¶å¤„ç†æ•°é‡
            try:
                print(f"ğŸ”„ [{i+1}/{len(server_urls[:8])}] Processing {server_url}")
                results = process_single_server(server_url)
                if results:
                    all_results.extend(results)
                    success_count += 1
                    print(f"âœ… Successfully got {len(results)} channels from {server_url}")
                time.sleep(1)  # è¯·æ±‚é—´éš”
            except Exception as e:
                print(f"âŒ Failed to process {server_url}: {e}")
                continue
        
        # å»é‡
        unique_results = list(set(all_results))
        print(f"ğŸ“Š {region_name}: {len(unique_results)} unique channels from {success_count} servers")
        return unique_results
        
    except Exception as e:
        print(f"âŒ Error processing region {region_name}: {str(e)}")
        return []
    finally:
        if driver:
            driver.quit()

def save_results(results, filename):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    if not results:
        print(f"âš ï¸ No results to save for {filename}")
        return False
    
    # æŒ‰é¢‘é“åç§°æ’åº
    sorted_results = sorted(results, key=lambda x: x.split(',')[0])
    
    with open(filename, "w", encoding="utf-8") as file:
        for result in sorted_results:
            file.write(result + "\n")
    print(f"ğŸ’¾ Saved {len(sorted_results)} results to {filename}")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting IPTV collection process...")
    start_time = time.time()
    
    all_files = []
    successful_regions = 0
    
    # å¤„ç†æ‰€æœ‰åœ°åŒº
    for region_name, url in REGION_URLS.items():
        try:
            results = process_region(region_name, url)
            filename = f"{region_name}.txt"
            if save_results(results, filename):
                all_files.append(filename)
                successful_regions += 1
            time.sleep(3)  # åœ°åŒºé—´å»¶è¿Ÿ
        except Exception as e:
            print(f"âŒ Failed to process {region_name}: {e}")
            continue
    
    # åˆå¹¶æ–‡ä»¶
    print(f"\n{'='*60}")
    print("ğŸ”„ Merging regional files...")
    print(f"{'='*60}")
    
    if all_files:
        # åˆå¹¶å¹¶å»é‡
        all_channels = set()
        for file in all_files:
            if os.path.exists(file):
                with open(file, 'r', encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            all_channels.add(line)
        
        # ä¿å­˜åˆå¹¶æ–‡ä»¶
        sorted_channels = sorted(list(all_channels))
        with open("IPTV.txt", "w", encoding="utf-8") as f:
            f.write('\n'.join(sorted_channels))
        
        print(f"âœ… Merged {len(all_files)} files into IPTV.txt")
        print(f"ğŸ“º Total unique channels: {len(sorted_channels)}")
    else:
        print("âŒ No valid files to merge")
        # åˆ›å»ºç©ºçš„IPTV.txtæ–‡ä»¶
        with open("IPTV.txt", "w", encoding="utf-8") as f:
            f.write("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\n{'='*60}")
    print("ğŸ“Š COLLECTION SUMMARY")
    print(f"{'='*60}")
    print(f"âœ… Successful regions: {successful_regions}/{len(REGION_URLS)}")
    print(f"â±ï¸  Processing time: {processing_time:.2f} seconds")
    
    if os.path.exists("IPTV.txt"):
        with open("IPTV.txt", "r", encoding="utf-8") as f:
            total_channels = len(f.readlines())
        print(f"ğŸ“º Total channels collected: {total_channels}")
    else:
        print("ğŸ“º Total channels collected: 0")
    
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
