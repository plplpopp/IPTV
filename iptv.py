import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
from urllib.parse import urlparse
from tqdm import tqdm
import sys

# ============================ é…ç½®æ–‡ä»¶ ============================
# æºURLåˆ—è¡¨ - å·²æ›´æ–°ä¸ºæ–°çš„æºåœ°å€
URL_SOURCES = [
    "https://raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-api/gd/output/ipv4/result.m3u",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt",
    "http://47.120.41.246:8899/zb.txt",
]

# æœ¬åœ°æºæ–‡ä»¶
LOCAL_SOURCE_FILE = "local.txt"

# æ¨¡æ¿é¢‘é“æ–‡ä»¶
TEMPLATE_FILE = "demo.txt"

# è¾“å‡ºæ–‡ä»¶
OUTPUT_TXT = "iptv.txt"
OUTPUT_M3U = "iptv.m3u"

# æ¯ä¸ªé¢‘é“ä¿ç•™çš„æ¥å£æ•°é‡
MAX_STREAMS_PER_CHANNEL = 8

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 3

# æœ€å¤§çº¿ç¨‹æ•°
MAX_WORKERS = 10

# ============================ æ­£åˆ™è¡¨è¾¾å¼ ============================
# IPv4åœ°å€åŒ¹é…
ipv4_pattern = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')

# é¢‘é“åç§°å’ŒURLåŒ¹é…
channel_pattern = re.compile(r"^([^,]+?),\s*(https?://.+)", re.IGNORECASE)

# M3Uæ ¼å¼è§£æ
extinf_pattern = re.compile(r'tvg-name="([^"]*)"', re.IGNORECASE)
extinf_name_pattern = re.compile(r'#EXTINF:.*?,(.+)', re.IGNORECASE)

def create_test_files():
    """åˆ›å»ºæµ‹è¯•æ–‡ä»¶ç”¨äºå®Œæ•´æ€§æµ‹è¯•"""
    print("åˆ›å»ºæµ‹è¯•æ–‡ä»¶...")
    
    # åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
    template_content = """å¤®è§†é¢‘é“,#genre#
CCTV-1ç»¼åˆ,http://example.com/cctv1
CCTV-2è´¢ç»,http://example.com/cctv2
CCTV-5ä½“è‚²,http://example.com/cctv5
CCTV-13æ–°é—»,http://example.com/cctv13
å«è§†é¢‘é“,#genre#
æ¹–å—å«è§†,http://example.com/hunan
æµ™æ±Ÿå«è§†,http://example.com/zhejiang
ä¸œæ–¹å«è§†,http://example.com/dongfang
åŒ—äº¬å«è§†,http://example.com/beijing
æ±Ÿè‹å«è§†,http://example.com/jiangsu"""
    
    try:
        with open(TEMPLATE_FILE, 'w', encoding='utf-8') as f:
            f.write(template_content)
        print(f"âœ… åˆ›å»ºæ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶
    local_content = """CCTV-1ç»¼åˆ,http://192.168.1.100/cctv1
CCTV-2è´¢ç»,http://192.168.1.100/cctv2
æ¹–å—å«è§†,http://192.168.1.100/hunan
æµ™æ±Ÿå«è§†,http://192.168.1.100/zhejiang
CCTV-1ç»¼åˆ,http://10.0.0.100/cctv1
æµ‹è¯•é¢‘é“,http://example.com/test"""
    
    try:
        with open(LOCAL_SOURCE_FILE, 'w', encoding='utf-8') as f:
            f.write(local_content)
        print(f"âœ… åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶: {LOCAL_SOURCE_FILE}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    return True

def load_template_channels():
    """
    åŠ è½½æ¨¡æ¿é¢‘é“åˆ—è¡¨
    è¿”å›: é¢‘é“åç§°åˆ—è¡¨ï¼Œä¿æŒæ¨¡æ¿ä¸­çš„é¡ºåºï¼ŒåŒ…å«åˆ†ç±»è¡Œ
    """
    if not os.path.exists(TEMPLATE_FILE):
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶ {TEMPLATE_FILE} ä¸å­˜åœ¨")
        if not create_test_files():
            return []
    
    template_channels = []
    try:
        print(f"ğŸ“ æ­£åœ¨åŠ è½½æ¨¡æ¿æ–‡ä»¶: {TEMPLATE_FILE}")
        with open(TEMPLATE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    template_channels.append(line)
        print(f"âœ… æ¨¡æ¿æ–‡ä»¶åŠ è½½å®Œæˆï¼Œå…± {len(template_channels)} è¡Œ")
        
        # éªŒè¯æ¨¡æ¿æ ¼å¼
        actual_channels = [line for line in template_channels if '#genre#' not in line and ',' in line]
        if not actual_channels:
            print("âš ï¸  è­¦å‘Š: æ¨¡æ¿æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“è¡Œ")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return []
    
    return template_channels

def load_local_sources():
    """
    åŠ è½½æœ¬åœ°æºæ–‡ä»¶
    è¿”å›: æµæ•°æ®è¡Œåˆ—è¡¨
    """
    local_streams = []
    if not os.path.exists(LOCAL_SOURCE_FILE):
        print(f"âš ï¸  æœ¬åœ°æºæ–‡ä»¶ {LOCAL_SOURCE_FILE} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return local_streams
    
    try:
        print(f"ğŸ“ æ­£åœ¨åŠ è½½æœ¬åœ°æºæ–‡ä»¶: {LOCAL_SOURCE_FILE}")
        with open(LOCAL_SOURCE_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    local_streams.append(line)
        print(f"âœ… æœ¬åœ°æºæ–‡ä»¶åŠ è½½å®Œæˆï¼Œå…± {len(local_streams)} ä¸ªæµ")
    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°æºæ–‡ä»¶å¤±è´¥: {e}")
    
    return local_streams

def fetch_online_sources():
    """
    æŠ“å–åœ¨çº¿æºæ•°æ®
    è¿”å›: æµæ•°æ®åˆ—è¡¨
    """
    online_streams = []
    
    def fetch_single_url(url):
        """è·å–å•ä¸ªURLçš„æºæ•°æ®"""
        try:
            print(f"ğŸŒ æ­£åœ¨æŠ“å–: {url}")
            response = requests.get(url, timeout=15, verify=False)
            response.encoding = 'utf-8'
            if response.status_code == 200:
                lines = [line.strip() for line in response.text.splitlines() if line.strip()]
                print(f"âœ… æˆåŠŸæŠ“å– {url}: {len(lines)} è¡Œ")
                return lines
            else:
                print(f"âŒ æŠ“å– {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except requests.exceptions.Timeout:
            print(f"â° æŠ“å– {url} è¶…æ—¶")
        except Exception as e:
            print(f"âŒ æŠ“å– {url} å¤±è´¥: {str(e)[:100]}...")
        return []
    
    if not URL_SOURCES:
        print("âš ï¸  æ²¡æœ‰é…ç½®åœ¨çº¿æºURL")
        return online_streams
    
    print("ğŸŒ æ­£åœ¨æŠ“å–åœ¨çº¿æº...")
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(URL_SOURCES), 5)) as executor:
            future_to_url = {executor.submit(fetch_single_url, url): url for url in URL_SOURCES}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    online_streams.extend(result)
                except Exception as e:
                    print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… åœ¨çº¿æºæŠ“å–å®Œæˆï¼Œå…±è·å– {len(online_streams)} è¡Œæ•°æ®")
    except Exception as e:
        print(f"âŒ æŠ“å–åœ¨çº¿æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    return online_streams

def parse_stream_line(line):
    """
    è§£ææµæ•°æ®è¡Œï¼Œæå–é¢‘é“åç§°å’ŒURL
    è¿”å›: (channel_name, url) æˆ– None
    """
    # è·³è¿‡M3Uæ ¼å¼çš„EXTINFè¡Œ
    if line.startswith('#EXTINF'):
        return None
    
    # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
    if not line or line.startswith('#'):
        return None
    
    # å°è¯•åŒ¹é…æ ‡å‡†æ ¼å¼: é¢‘é“åç§°,URL
    match = channel_pattern.match(line)
    if match:
        channel_name = match.group(1).strip()
        url = match.group(2).strip()
        return (channel_name, url)
    
    # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
    if ',' in line:
        parts = line.split(',', 1)
        if len(parts) == 2 and parts[1].startswith(('http://', 'https://')):
            return (parts[0].strip(), parts[1].strip())
    
    return None

def parse_m3u_content(content):
    """
    è§£æM3Uæ ¼å¼å†…å®¹
    è¿”å›: [(channel_name, url)] åˆ—è¡¨
    """
    channels = []
    lines = content.splitlines()
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('#EXTINF'):
            # æå–é¢‘é“åç§°
            channel_name = None
            
            # å°è¯•ä»tvg-nameå±æ€§æå–
            tvg_match = extinf_pattern.search(line)
            if tvg_match:
                channel_name = tvg_match.group(1)
            else:
                # ä»EXTINFè¡Œæœ«å°¾æå–
                name_match = extinf_name_pattern.search(line)
                if name_match:
                    channel_name = name_match.group(1)
            
            # ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯URL
            if i + 1 < len(lines) and channel_name:
                url_line = lines[i + 1].strip()
                if url_line and not url_line.startswith('#'):
                    channels.append((channel_name, url_line))
                    i += 2
                    continue
        i += 1
    
    return channels

def build_channel_database(stream_lines):
    """
    æ„å»ºé¢‘é“æ•°æ®åº“
    è¿”å›: {channel_name: [url1, url2, ...]}
    """
    print("ğŸ“Š æ­£åœ¨æ„å»ºé¢‘é“æ•°æ®åº“...")
    channel_db = {}
    
    for line in stream_lines:
        result = parse_stream_line(line)
        if result:
            channel_name, url = result
            
            # æ ‡å‡†åŒ–é¢‘é“åç§°ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
            channel_name = re.sub(r'\s+', ' ', channel_name).strip()
            
            if channel_name not in channel_db:
                channel_db[channel_name] = []
            
            # é¿å…é‡å¤URL
            if url not in channel_db[channel_name]:
                channel_db[channel_name].append(url)
    
    print(f"âœ… é¢‘é“æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå…± {len(channel_db)} ä¸ªé¢‘é“")
    return channel_db

def test_stream_quality(url):
    """
    æµ‹è¯•æµè´¨é‡
    è¿”å›: (is_alive, response_time) æˆ– (False, None) å¦‚æœæµ‹è¯•å¤±è´¥
    """
    try:
        start_time = time.time()
        response = requests.head(url, timeout=REQUEST_TIMEOUT, verify=False, 
                               headers={'User-Agent': 'Mozilla/5.0'})
        end_time = time.time()
        
        response_time = round((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        if response.status_code == 200:
            return (True, response_time)
        else:
            return (False, None)
    except:
        return (False, None)

def select_best_streams(urls, max_streams=MAX_STREAMS_PER_CHANNEL):
    """
    ä¸ºé¢‘é“é€‰æ‹©æœ€ä½³æµ
    è¿”å›: æ’åºåçš„URLåˆ—è¡¨
    """
    if not urls:
        return []
    
    # å¦‚æœURLæ•°é‡è¾ƒå°‘ï¼Œç›´æ¥è¿”å›
    if len(urls) <= max_streams:
        return urls
    
    print(f"  ğŸ” æµ‹è¯•é¢‘é“æµè´¨é‡ ({len(urls)} ä¸ªæµ)...")
    valid_streams = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± æµ‹è¯•æµè´¨é‡
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_url = {executor.submit(test_stream_quality, url): url for url in urls}
        
        for future in concurrent.futures.as_completed(future_to_url):
            url = future_to_url[future]
            try:
                is_alive, response_time = future.result()
                if is_alive:
                    valid_streams.append((url, response_time))
            except:
                pass
    
    # æŒ‰å“åº”æ—¶é—´æ’åºï¼ˆå¿«çš„åœ¨å‰ï¼‰
    valid_streams.sort(key=lambda x: x[1] if x[1] is not None else float('inf'))
    
    # è¿”å›æœ€ä½³æµï¼Œé™åˆ¶æ•°é‡
    best_streams = [stream[0] for stream in valid_streams[:max_streams]]
    
    print(f"  âœ… æ‰¾åˆ° {len(best_streams)} ä¸ªæœ‰æ•ˆæµ")
    return best_streams

def generate_output_files(template_channels, channel_db):
    """
    ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    """
    print("ğŸ“ æ­£åœ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
    
    txt_lines = []
    m3u_lines = ['#EXTM3U']
    
    current_group = "é»˜è®¤åˆ†ç»„"
    
    for line in template_channels:
        # å¤„ç†åˆ†ç»„è¡Œ
        if '#genre#' in line:
            group_name = line.replace(',#genre#', '').strip()
            current_group = group_name
            txt_lines.append(line)
            continue
        
        # å¤„ç†é¢‘é“è¡Œ
        if ',' in line:
            parts = line.split(',', 1)
            if len(parts) == 2:
                template_channel = parts[0].strip()
                template_url = parts[1].strip()
                
                # åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾åŒ¹é…çš„é¢‘é“
                matched_urls = []
                for db_channel, db_urls in channel_db.items():
                    # ç®€å•çš„åç§°åŒ¹é…ï¼ˆå¯ä»¥åœ¨è¿™é‡Œæ”¹è¿›åŒ¹é…ç®—æ³•ï¼‰
                    if template_channel.lower() in db_channel.lower() or db_channel.lower() in template_channel.lower():
                        matched_urls.extend(db_urls)
                
                # é€‰æ‹©æœ€ä½³æµ
                best_urls = select_best_streams(matched_urls)
                
                if best_urls:
                    # ä½¿ç”¨æ‰¾åˆ°çš„æœ€ä½³æµ
                    for url in best_urls:
                        txt_lines.append(f"{template_channel},{url}")
                        m3u_lines.append(f'#EXTINF:-1 group-title="{current_group}",{template_channel}')
                        m3u_lines.append(url)
                else:
                    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æµï¼Œä½¿ç”¨æ¨¡æ¿ä¸­çš„URL
                    txt_lines.append(line)
                    m3u_lines.append(f'#EXTINF:-1 group-title="{current_group}",{template_channel}')
                    m3u_lines.append(template_url)
    
    # å†™å…¥TXTæ–‡ä»¶
    try:
        with open(OUTPUT_TXT, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_lines))
        print(f"âœ… ç”ŸæˆTXTæ–‡ä»¶: {OUTPUT_TXT}ï¼Œå…± {len(txt_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥TXTæ–‡ä»¶å¤±è´¥: {e}")
    
    # å†™å…¥M3Uæ–‡ä»¶
    try:
        with open(OUTPUT_M3U, 'w', encoding='utf-8') as f:
            f.write('\n'.join(m3u_lines))
        print(f"âœ… ç”ŸæˆM3Uæ–‡ä»¶: {OUTPUT_M3U}ï¼Œå…± {len(m3u_lines)} è¡Œ")
    except Exception as e:
        print(f"âŒ å†™å…¥M3Uæ–‡ä»¶å¤±è´¥: {e}")
    
    return len([line for line in txt_lines if ',' in line and '#genre#' not in line])

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ IPTVé¢‘é“æ•´ç†å·¥å…·å¼€å§‹è¿è¡Œ...")
    start_time = time.time()
    
    # 1. åŠ è½½æ¨¡æ¿é¢‘é“
    template_channels = load_template_channels()
    if not template_channels:
        print("âŒ æ— æ³•åŠ è½½æ¨¡æ¿é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 2. åŠ è½½æœ¬åœ°æº
    local_streams = load_local_sources()
    
    # 3. æŠ“å–åœ¨çº¿æº
    online_streams = fetch_online_sources()
    
    # 4. åˆå¹¶æ‰€æœ‰æµæ•°æ®
    all_streams = local_streams + online_streams
    if not all_streams:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æµæ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"ğŸ“Š æ€»å…±è·å– {len(all_streams)} ä¸ªæµæ•°æ®")
    
    # 5. æ„å»ºé¢‘é“æ•°æ®åº“
    channel_db = build_channel_database(all_streams)
    
    # 6. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    total_channels = generate_output_files(template_channels, channel_db)
    
    # ç»Ÿè®¡ä¿¡æ¯
    end_time = time.time()
    execution_time = round(end_time - start_time, 2)
    
    print("\n" + "="*50)
    print("ğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
    print(f"  æ¨¡æ¿é¢‘é“: {len([c for c in template_channels if '#genre#' not in c and ',' in c])} ä¸ª")
    print(f"  æºæ•°æ®æµ: {len(all_streams)} ä¸ª")
    print(f"  å‘ç°é¢‘é“: {len(channel_db)} ä¸ª")
    print(f"  è¾“å‡ºé¢‘é“: {total_channels} ä¸ª")
    print(f"  æ‰§è¡Œæ—¶é—´: {execution_time} ç§’")
    print(f"  è¾“å‡ºæ–‡ä»¶: {OUTPUT_TXT}, {OUTPUT_M3U}")
    print("="*50)

if __name__ == "__main__":
    # ç¦ç”¨SSLè­¦å‘Š
    requests.packages.urllib3.disable_warnings()
    
    main()
