import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
import json
import hashlib
import pickle
import logging
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Set
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential


class Config:
    """é…ç½®ç®¡ç†ç±»"""
    DEFAULT_CONFIG = {
        'timeout': 10,
        'max_workers': 15,
        'test_size_kb': 512,  # å¢å¤§æµ‹è¯•æ•°æ®é‡ä»¥è·å¾—æ›´å‡†ç¡®çš„é€Ÿåº¦
        'cache_ttl_hours': 2,
        'max_sources_per_channel': 25,
        'keep_best_sources': 5,
        'min_speed_mbps': 0.3,  # æœ€ä½é€Ÿåº¦è¦æ±‚ 0.3 MB/s
        'sources': [
            "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
            "https://live.zbds.top/tv/iptv6.txt", 
            "https://live.zbds.top/tv/iptv4.txt",
            "https://raw.githubusercontent.com/YanG-1989/m3u/main/Gather.m3u",
            "https://raw.githubusercontent.com/Free-IPTV/Countries/master/CN.m3u",
            "https://raw.githubusercontent.com/guaguagu/iptv/main/iptv.txt",
            "https://raw.githubusercontent.com/free-iptv/iptv/master/streams/cn.m3u",
            "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
        ],
        'output_formats': ['txt', 'm3u'],
        'template_file': 'demo.txt',
        'output_files': {
            'txt': 'iptv.txt',
            'm3u': 'iptv.m3u'
        }
    }
    
    def __init__(self, config_file='config.json'):
        self.config = self.DEFAULT_CONFIG.copy()
        self.config_file = config_file
        self.load_from_file()
    
    def load_from_file(self):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    self.config.update(user_config)
                print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            else:
                self.create_default_config()
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    def create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.DEFAULT_CONFIG, f, indent=4, ensure_ascii=False)
            print(f"âœ… å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {self.config_file}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get(self, key, default=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(key, default)


class CacheManager:
    """ç¼“å­˜ç®¡ç†"""
    def __init__(self, cache_dir='cache', ttl_hours=2):
        self.cache_dir = cache_dir
        self.ttl = timedelta(hours=ttl_hours)
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, url):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def is_valid(self, cache_file):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_file):
            return False
        mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
        return datetime.now() - mod_time < self.ttl
    
    def save(self, key, data):
        """ä¿å­˜ç¼“å­˜"""
        try:
            cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump({'data': data, 'timestamp': datetime.now()}, f)
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def load(self, key):
        """åŠ è½½ç¼“å­˜"""
        try:
            cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
            if self.is_valid(cache_file):
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)['data']
        except Exception as e:
            print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
        return None


class IPTV:
    """IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·"""
    
    def __init__(self, config_file='config.json'):
        """
        åˆå§‹åŒ–å·¥å…·
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åˆå§‹åŒ–é…ç½®
        self.config = Config(config_file)
        
        # é…ç½®å‚æ•°
        self.timeout = self.config.get('timeout', 10)
        self.max_workers = self.config.get('max_workers', 15)
        self.test_size = self.config.get('test_size_kb', 512) * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.min_speed_mbps = self.config.get('min_speed_mbps', 0.3)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = self.setup_logging()
        self.cache_manager = CacheManager(ttl_hours=self.config.get('cache_ttl_hours', 2))
        
        # è¯·æ±‚ä¼šè¯é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Encoding': 'gzip, deflate',
            'Accept': '*/*',
            'Connection': 'keep-alive'
        })
        
        # æ•°æ®æºé…ç½®
        self.source_urls = self.config.get('sources', [])
        
        # æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘
        self.ipv4_pattern = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')
        self.ipv6_pattern = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')
        self.channel_pattern = re.compile(r'^([^,#]+)')
        self.extinf_pattern = re.compile(r'#EXTINF:.*?,(.+)')
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.template_file = self.config.get('template_file', 'demo.txt')
        output_files = self.config.get('output_files', {
            'txt': 'iptv.txt',
            'm3u': 'iptv.m3u'
        })
        self.output_files = {
            'txt': output_files['txt'],
            'm3u': output_files['m3u']
        }
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.template_channels = self.load_template_channels()
        self.all_streams = []

    def setup_logging(self, level=logging.INFO):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('iptv.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)

    def load_template_channels(self) -> Set[str]:
        """åŠ è½½æ¨¡æ¿æ–‡ä»¶ä¸­çš„é¢‘é“åˆ—è¡¨"""
        channels = set()
        template_file = self.config.get('template_file', 'demo.txt')
        
        if not os.path.exists(template_file):
            self.logger.warning(f"æ¨¡æ¿æ–‡ä»¶ {template_file} ä¸å­˜åœ¨ï¼Œå°†å¤„ç†æ‰€æœ‰é¢‘é“")
            return channels
        
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if match := self.channel_pattern.match(line):
                            channel_name = self.normalize_channel_name(match.group(1).strip())
                            channels.add(channel_name)
            self.logger.info(f"åŠ è½½æ¨¡æ¿é¢‘é“ {len(channels)} ä¸ª")
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶é”™è¯¯: {str(e)}")
        
        return channels

    def normalize_channel_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–é¢‘é“åç§°"""
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'\s+', ' ', name.strip())
        
        # ç»Ÿä¸€å¤®è§†å‘½å
        cctv_patterns = [
            (r'CCTV-?(\d+)', r'CCTV\1'),
            (r'å¤®è§†(\d+)', r'CCTV\1'),
            (r'ä¸­å¤®(\d+)', r'CCTV\1')
        ]
        
        for pattern, replacement in cctv_patterns:
            name = re.sub(pattern, replacement, name)
        
        # ç»Ÿä¸€å«è§†é¢‘é“å‘½å
        ws_patterns = [
            (r'æ¹–å—å«è§†', 'æ¹–å—å«è§†'),
            (r'æ±Ÿè‹å«è§†', 'æ±Ÿè‹å«è§†'),
            (r'æµ™æ±Ÿå«è§†', 'æµ™æ±Ÿå«è§†'),
            (r'ä¸œæ–¹å«è§†', 'ä¸œæ–¹å«è§†'),
            (r'åŒ—äº¬å«è§†', 'åŒ—äº¬å«è§†'),
        ]
        
        for pattern, replacement in ws_patterns:
            if pattern in name:
                name = replacement
                break
        
        return name

    # ==================== æ•°æ®è·å–ä¸å¤„ç† ====================
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def fetch_with_retry(self, url):
        """å¸¦é‡è¯•çš„æŠ“å–"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            self.logger.warning(f"æŠ“å–å¤±è´¥: {url}, é”™è¯¯: {e}")
            raise

    def fetch_streams(self) -> Optional[str]:
        """ä»æ‰€æœ‰æºURLæŠ“å–ç›´æ’­æº"""
        contents = []
        successful_sources = 0
        
        for url in self.source_urls:
            domain = self._extract_domain(url)
            self.logger.info(f"æŠ“å–æº: {domain}")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.cache_manager.get_cache_key(url)
            cached_content = self.cache_manager.load(cache_key)
            
            if cached_content:
                self.logger.info(f"  âœ“ ä½¿ç”¨ç¼“å­˜")
                contents.append(cached_content)
                successful_sources += 1
                continue
            
            try:
                content = self.fetch_with_retry(url)
                
                # éªŒè¯å†…å®¹æœ‰æ•ˆæ€§
                if self.validate_content(content):
                    contents.append(content)
                    successful_sources += 1
                    self.cache_manager.save(cache_key, content)
                    self.logger.info(f"  âœ“ æˆåŠŸ")
                else:
                    self.logger.warning(f"  âš ï¸ å†…å®¹æ— æ•ˆ")
                    
            except Exception as e:
                self.logger.error(f"  âœ— å¤±è´¥: {str(e)}")
        
        self.logger.info(f"æˆåŠŸæŠ“å– {successful_sources}/{len(self.source_urls)} ä¸ªæº")
        return "\n".join(contents) if contents else None

    def validate_content(self, content: str) -> bool:
        """éªŒè¯å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç›´æ’­æºæ ¼å¼"""
        lines = content.splitlines()
        valid_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if line.startswith('http') or (',' in line and 'http' in line):
                valid_lines += 1
        
        return valid_lines >= 5

    def parse_content(self, content: str) -> pd.DataFrame:
        """è§£æç›´æ’­æºå†…å®¹"""
        streams = []
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
        if content.startswith("#EXTM3U"):
            streams.extend(self.parse_m3u_content(content))
        else:
            streams.extend(self.parse_txt_content(content))
        
        if not streams:
            self.logger.warning("æœªè§£æåˆ°æœ‰æ•ˆç›´æ’­æº")
            return pd.DataFrame(columns=['program_name', 'stream_url'])
        
        df = pd.DataFrame(streams)
        
        # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        df = self.clean_stream_data(df)
        
        self.logger.info(f"è§£æåˆ° {len(df)} ä¸ªç›´æ’­æºï¼Œ{len(df['program_name'].unique())} ä¸ªé¢‘é“")
        return df

    def parse_m3u_content(self, content: str) -> List[Dict]:
        """è§£æM3Uæ ¼å¼å†…å®¹"""
        streams = []
        current_program = None
        
        for line in content.splitlines():
            line = line.strip()
            if line.startswith("#EXTINF"):
                # å°è¯•å¤šç§æ ¼å¼æå–èŠ‚ç›®å
                if match := re.search(r'tvg-name="([^"]+)"', line):
                    current_program = self.normalize_channel_name(match.group(1).strip())
                elif match := self.extinf_pattern.search(line):
                    current_program = self.normalize_channel_name(match.group(1).strip())
            elif line.startswith("http"):
                if current_program:
                    streams.append({
                        "program_name": current_program,
                        "stream_url": line
                    })
                current_program = None
        
        return streams

    def parse_txt_content(self, content: str) -> List[Dict]:
        """è§£æTXTæ ¼å¼å†…å®¹"""
        streams = []
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            # æ”¯æŒå¤šç§åˆ†éš”ç¬¦
            if ',' in line:
                parts = line.split(',', 1)
            elif ' ' in line and 'http' in line:
                parts = line.split(' ', 1)
            else:
                continue
                
            if len(parts) == 2:
                program_name = self.normalize_channel_name(parts[0].strip())
                stream_url = parts[1].strip()
                
                if program_name and stream_url.startswith('http'):
                    streams.append({
                        "program_name": program_name,
                        "stream_url": stream_url
                    })
        
        return streams

    def clean_stream_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        if df.empty:
            return df
        
        # å»é™¤èŠ‚ç›®åä¸­çš„å¤šä½™ç©ºæ ¼
        df['program_name'] = df['program_name'].str.strip()
        
        # è¿‡æ»¤æ— æ•ˆURL
        initial_count = len(df)
        df = df[df['stream_url'].str.startswith('http')]
        
        # å»é™¤æ˜æ˜¾æ— æ•ˆçš„èŠ‚ç›®å
        invalid_names = ['', 'None', 'null', 'undefined']
        df = df[~df['program_name'].isin(invalid_names)]
        
        # å»é™¤é‡å¤çš„èŠ‚ç›®åå’ŒURLç»„åˆ
        df = df.drop_duplicates(subset=['program_name', 'stream_url'])
        
        self.logger.info(f"æ•°æ®æ¸…æ´—: {initial_count} -> {len(df)} ä¸ªæº")
        return df

    def organize_streams(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•´ç†ç›´æ’­æºæ•°æ®ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤šä¿ç•™æŒ‡å®šæ•°é‡çš„æºç”¨äºæµ‹é€Ÿ"""
        max_sources = self.config.get('max_sources_per_channel', 25)
        grouped = df.groupby('program_name')['stream_url'].apply(list).reset_index()
        
        # é™åˆ¶æ¯ä¸ªé¢‘é“çš„æºæ•°é‡
        grouped['stream_url'] = grouped['stream_url'].apply(lambda x: x[:max_sources])
        
        self.logger.info(f"æ•´ç†å: {len(grouped)} ä¸ªé¢‘é“ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤š{max_sources}ä¸ªæº")
        return grouped

    # ==================== å¢å¼ºæµ‹é€ŸåŠŸèƒ½ ====================
    
    def test_single_url(self, url: str) -> Tuple[Optional[float], Optional[str], float]:
        """
        æµ‹è¯•å•ä¸ªURLçš„é€Ÿåº¦ï¼Œè¿”å›é€Ÿåº¦(MB/s)ã€é”™è¯¯ä¿¡æ¯å’Œå“åº”æ—¶é—´
        
        Args:
            url: è¦æµ‹è¯•çš„URL
            
        Returns:
            Tuple[é€Ÿåº¦(MB/s), é”™è¯¯ä¿¡æ¯, å“åº”æ—¶é—´]
        """
        start_time = time.time()
        
        try:
            response = self.session.get(
                url, 
                timeout=self.timeout, 
                stream=True,
                headers={'Range': f'bytes=0-{self.test_size-1}'}
            )
            response_time = time.time() - start_time
            
            if response.status_code not in [200, 206]:
                return None, f"HTTP {response.status_code}", response_time
            
            content_length = 0
            chunk_start_time = time.time()
            
            for chunk in response.iter_content(chunk_size=64*1024):  # 64KB chunks
                if not chunk:
                    break
                    
                content_length += len(chunk)
                if content_length >= self.test_size:
                    break
                    
                # æ£€æŸ¥ä¸‹è½½æ˜¯å¦è¶…æ—¶
                if time.time() - chunk_start_time > self.timeout:
                    return None, "ä¸‹è½½è¶…æ—¶", response_time
            
            if content_length == 0:
                return 0.0, "æ— æ•°æ®", response_time
            
            total_time = time.time() - start_time
            speed_mbps = (content_length / total_time) / (1024 * 1024)  # MB/s
            
            return speed_mbps, None, response_time
            
        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚è¶…æ—¶", time.time() - start_time
        except requests.exceptions.SSLError:
            return None, "SSLé”™è¯¯", time.time() - start_time
        except requests.exceptions.ConnectionError:
            return None, "è¿æ¥å¤±è´¥", time.time() - start_time
        except requests.exceptions.HTTPError as e:
            return None, f"HTTPé”™è¯¯ {e.response.status_code}", time.time() - start_time
        except Exception as e:
            return None, f"é”™è¯¯: {str(e)}", time.time() - start_time

    def test_urls_concurrently(self, urls: List[str]) -> List[Tuple[str, Optional[float], Optional[str], float]]:
        """å¹¶å‘æµ‹è¯•URLåˆ—è¡¨"""
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.test_single_url, url): url for url in urls}
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                speed, error, response_time = future.result()
                results.append((url, speed, error, response_time))
        return results

    def test_all_channels(self, grouped_streams: pd.DataFrame) -> Dict[str, List[Tuple[str, float]]]:
        """æµ‹è¯•æ‰€æœ‰é¢‘é“å¹¶ä¿ç•™æœ€ä½³æº"""
        keep_best = self.config.get('keep_best_sources', 5)
        min_speed = self.config.get('min_speed_mbps', 0.3)
        results = {}
        total_channels = len(grouped_streams)
        tested_channels = 0
        successful_channels = 0
        
        self.logger.info(f"å¼€å§‹æµ‹é€Ÿ {total_channels} ä¸ªé¢‘é“")
        self.logger.info(f"æ¯ä¸ªé¢‘é“æµ‹è¯•æœ€å¤š{self.config.get('max_sources_per_channel', 25)}ä¸ªæºï¼Œä¿ç•™æœ€ä¼˜{keep_best}ä¸ª")
        self.logger.info(f"æœ€ä½é€Ÿåº¦è¦æ±‚: {min_speed} MB/s")
        
        for idx, (_, row) in enumerate(grouped_streams.iterrows(), 1):
            channel = row['program_name']
            urls = row['stream_url']
            
            self.logger.info(f"[{idx}/{total_channels}] æµ‹è¯•é¢‘é“: {channel} ({len(urls)}ä¸ªæº)")
            
            test_results = self.test_urls_concurrently(urls)
            valid_streams = []
            
            excellent_count = 0
            good_count = 0
            slow_count = 0
            failed_count = 0
            
            for url, speed, error, response_time in test_results:
                if speed is not None:
                    if speed >= min_speed:  # è¿‡æ»¤ä½é€Ÿæº
                        valid_streams.append((url, speed))
                        if speed > 2.0:
                            excellent_count += 1
                        elif speed > 1.0:
                            good_count += 1
                        else:
                            slow_count += 1
                    else:
                        slow_count += 1
                else:
                    failed_count += 1
            
            # æŒ‰é€Ÿåº¦æ’åºå¹¶ä¿ç•™æœ€ä½³æº
            valid_streams.sort(key=lambda x: x[1], reverse=True)
            best_streams = valid_streams[:keep_best]
            results[channel] = best_streams
            
            tested_channels += 1
            if best_streams:
                successful_channels += 1
                best_speed = best_streams[0][1]
                self.logger.info(f"  âœ… æˆåŠŸ: æœ€ä½³é€Ÿåº¦ {best_speed:.2f} MB/s")
                self.logger.info(f"     è¯¦æƒ…: æé€Ÿ{excellent_count} å¿«é€Ÿ{good_count} æ…¢é€Ÿ{slow_count} å¤±è´¥{failed_count}")
                self.logger.info(f"     ä¿ç•™: {len(best_streams)}ä¸ªæœ€ä¼˜æº")
            else:
                self.logger.warning(f"  âŒ å¤±è´¥: æ— æœ‰æ•ˆæº (æœ€ä½è¦æ±‚: {min_speed} MB/s)")
        
        self.logger.info(f"æµ‹é€Ÿå®Œæˆ: {successful_channels}/{tested_channels} ä¸ªé¢‘é“æœ‰æœ‰æ•ˆæº")
        return results

    # ==================== æ¨¡æ¿åŒ¹é…å’Œç»“æœç”Ÿæˆ ====================
    
    def filter_by_template(self, speed_results: Dict[str, List[Tuple[str, float]]]) -> Dict[str, List[Tuple[str, float]]]:
        """æ ¹æ®æ¨¡æ¿é¢‘é“è¿‡æ»¤ç»“æœ"""
        if not self.template_channels:
            self.logger.info("æœªä½¿ç”¨æ¨¡æ¿è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰é¢‘é“")
            return speed_results
        
        filtered_results = {}
        matched_count = 0
        
        for channel in self.template_channels:
            if channel in speed_results and speed_results[channel]:
                filtered_results[channel] = speed_results[channel]
                matched_count += 1
        
        self.logger.info(f"æ¨¡æ¿åŒ¹é…: {matched_count}/{len(self.template_channels)} ä¸ªé¢‘é“")
        
        # æ˜¾ç¤ºæœªåŒ¹é…çš„æ¨¡æ¿é¢‘é“
        unmatched = self.template_channels - set(speed_results.keys())
        if unmatched:
            self.logger.warning(f"æœªæ‰¾åˆ°æºçš„æ¨¡æ¿é¢‘é“: {len(unmatched)}ä¸ª")
            for channel in list(unmatched)[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                self.logger.warning(f"  - {channel}")
            if len(unmatched) > 10:
                self.logger.warning(f"  ... è¿˜æœ‰ {len(unmatched) - 10} ä¸ª")
        
        return filtered_results

    def generate_output_files(self, speed_results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        self.generate_txt_file(speed_results)
        self.generate_m3u_file(speed_results)
        self.generate_report(speed_results)

    def generate_txt_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆTXTæ ¼å¼æ–‡ä»¶"""
        categories = {
            "å¤®è§†é¢‘é“,#genre#": ["CCTV", "å¤®è§†"],
            "å«è§†é¢‘é“,#genre#": ["å«è§†", "æ¹–å—", "æµ™æ±Ÿ", "æ±Ÿè‹", "ä¸œæ–¹", "åŒ—äº¬"],
            "åœ°æ–¹é¢‘é“,#genre#": ["é‡åº†", "å¹¿ä¸œ", "æ·±åœ³", "å—æ–¹", "å¤©æ´¥", "æ²³åŒ—"],
            "æ¸¯æ¾³é¢‘é“,#genre#": ["å‡¤å‡°", "ç¿¡ç¿ ", "æ˜ç ", "æ¾³äºš"],
            "å…¶ä»–é¢‘é“,#genre#": []
        }
        
        categorized = {cat: [] for cat in categories}
        
        for channel in self.get_ordered_channels(results.keys()):
            streams = results.get(channel, [])
            if not streams:
                continue
                
            matched = False
            for cat, keywords in categories.items():
                if any(keyword in channel for keyword in keywords):
                    categorized[cat].extend(
                        f"{channel},{url} # é€Ÿåº¦: {speed:.2f}MB/s" 
                        for url, speed in streams
                    )
                    matched = True
                    break
            
            if not matched:
                categorized["å…¶ä»–é¢‘é“,#genre#"].extend(
                    f"{channel},{url} # é€Ÿåº¦: {speed:.2f}MB/s" 
                    for url, speed in streams
                )
        
        with open(self.output_files['txt'], 'w', encoding='utf-8') as f:
            for cat, items in categorized.items():
                if items:
                    f.write(f"\n{cat}\n")
                    f.write("\n".join(items) + "\n")
        
        self.logger.info(f"ç”ŸæˆTXTæ–‡ä»¶: {self.output_files['txt']}")

    def generate_m3u_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆM3Uæ ¼å¼æ–‡ä»¶"""
        with open(self.output_files['m3u'], 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            
            for channel in self.get_ordered_channels(results.keys()):
                streams = results.get(channel, [])
                for url, speed in streams:
                    quality = self.get_speed_quality(speed)
                    f.write(f'#EXTINF:-1 tvg-name="{channel}",{channel} [é€Ÿåº¦: {speed:.2f}MB/s {quality}]\n{url}\n')
        
        self.logger.info(f"ç”ŸæˆM3Uæ–‡ä»¶: {self.output_files['m3u']}")

    def generate_report(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæµ‹é€ŸæŠ¥å‘Š"""
        speed_stats = []
        valid_channels = []
        total_sources = 0
        
        for channel, streams in results.items():
            if streams:
                best_speed = streams[0][1]
                speed_stats.append(best_speed)
                valid_channels.append((channel, best_speed, len(streams)))
                total_sources += len(streams)
        
        if not speed_stats:
            self.logger.warning("âš ï¸ æ— æœ‰æ•ˆæµ‹é€Ÿç»“æœ")
            return
        
        # æŒ‰é€Ÿåº¦æ’åºé¢‘é“
        valid_channels.sort(key=lambda x: x[1], reverse=True)
        
        print("\n" + "="*60)
        print("IPTVç›´æ’­æºæµ‹é€ŸæŠ¥å‘Š")
        print("="*60)
        print(f"æœ‰æ•ˆé¢‘é“æ•°: {len(valid_channels)}")
        print(f"æ€»æºæ•°é‡: {total_sources}")
        print(f"å¹³å‡é€Ÿåº¦: {sum(speed_stats)/len(speed_stats):.2f} MB/s")
        print(f"æœ€å¿«é€Ÿåº¦: {max(speed_stats):.2f} MB/s")
        print(f"æœ€æ…¢é€Ÿåº¦: {min(speed_stats):.2f} MB/s")
        print(f"é€Ÿåº¦è¦æ±‚: â‰¥{self.min_speed_mbps} MB/s")
        
        # é€Ÿåº¦åˆ†å¸ƒç»Ÿè®¡
        excellent = len([s for s in speed_stats if s > 2.0])
        good = len([s for s in speed_stats if 1.0 < s <= 2.0])
        normal = len([s for s in speed_stats if 0.5 < s <= 1.0])
        slow = len([s for s in speed_stats if s <= 0.5])
        
        print(f"\né€Ÿåº¦åˆ†å¸ƒ:")
        print(f"  æé€Ÿ(>2.0MB/s): {excellent}ä¸ªé¢‘é“")
        print(f"  å¿«é€Ÿ(1.0-2.0MB/s): {good}ä¸ªé¢‘é“")
        print(f"  ä¸­é€Ÿ(0.5-1.0MB/s): {normal}ä¸ªé¢‘é“")
        print(f"  æ…¢é€Ÿ(â‰¤0.5MB/s): {slow}ä¸ªé¢‘é“")
        
        print("\né¢‘é“é€Ÿåº¦æ’å TOP 20:")
        for i, (channel, speed, count) in enumerate(valid_channels[:20], 1):
            quality = self.get_speed_quality(speed)
            print(f"{i:2d}. {channel:<15} {speed:>5.2f} MB/s ({quality}) [{count}ä¸ªæº]")

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def get_ordered_channels(self, channels: List[str]) -> List[str]:
        """æŒ‰ç…§æ¨¡æ¿é¡ºåºæ’åºé¢‘é“åˆ—è¡¨"""
        if not self.template_channels:
            return sorted(channels)
        
        ordered = []
        # é¦–å…ˆæŒ‰æ¨¡æ¿é¡ºåº
        with open(self.template_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if match := self.channel_pattern.match(line):
                        channel = self.normalize_channel_name(match.group(1).strip())
                        if channel in channels and channel not in ordered:
                            ordered.append(channel)
        
        # æ·»åŠ æœªåœ¨æ¨¡æ¿ä¸­çš„é¢‘é“ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼Œå› ä¸ºå·²ç»è¿‡æ»¤äº†ï¼‰
        for channel in channels:
            if channel not in ordered:
                ordered.append(channel)
                
        return ordered

    def _extract_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            netloc = urlparse(url).netloc
            return netloc.split(':')[0]
        except:
            return url[:30] + "..." if len(url) > 30 else url

    def get_speed_quality(self, speed: float) -> str:
        """è·å–é€Ÿåº¦è´¨é‡è¯„çº§"""
        if speed > 2.0: return "æä½³"
        if speed > 1.0: return "ä¼˜ç§€" 
        if speed > 0.5: return "è‰¯å¥½"
        if speed > 0.3: return "ä¸€èˆ¬"
        return "è¾ƒå·®"

    # ==================== ä¸»æµç¨‹ ====================
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("="*60)
        print("IPTVç›´æ’­æºå¤„ç†å·¥å…·")
        print("="*60)
        print(f"é…ç½®: è¶…æ—¶{self.timeout}s çº¿ç¨‹{self.max_workers} æµ‹é€Ÿ{self.test_size//1024}KB")
        print(f"è¦æ±‚: æœ€ä½é€Ÿåº¦{self.min_speed_mbps}MB/s")
        
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæŠ“å–æ‰€æœ‰ç›´æ’­æº
            self.logger.info("ç¬¬ä¸€æ­¥ï¼šæŠ“å–ç›´æ’­æº...")
            content = self.fetch_streams()
            if not content:
                self.logger.error("âŒ æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®")
                return
            
            # ç¬¬äºŒæ­¥ï¼šè§£æå’Œæ•´ç†æ•°æ®
            self.logger.info("ç¬¬äºŒæ­¥ï¼šè§£æç›´æ’­æºæ•°æ®...")
            df = self.parse_content(content)
            if df.empty:
                self.logger.error("âŒ æœªè§£æåˆ°æœ‰æ•ˆç›´æ’­æº")
                return
            
            # ç¬¬ä¸‰æ­¥ï¼šæ•´ç†æ•°æ®ï¼Œæ¯ä¸ªé¢‘é“æœ€å¤šæŒ‡å®šæ•°é‡çš„æº
            grouped = self.organize_streams(df)
            
            # ç¬¬å››æ­¥ï¼šå¯¹æ‰€æœ‰é¢‘é“è¿›è¡Œæµ‹é€Ÿ
            self.logger.info("ç¬¬ä¸‰æ­¥ï¼šæµ‹é€Ÿä¼˜åŒ–...")
            speed_results = self.test_all_channels(grouped)
            
            # ç¬¬äº”æ­¥ï¼šæ ¹æ®æ¨¡æ¿é¢‘é“è¿‡æ»¤ç»“æœ
            self.logger.info("ç¬¬å››æ­¥ï¼šæ¨¡æ¿åŒ¹é…...")
            filtered_results = self.filter_by_template(speed_results)
            
            if not filtered_results:
                self.logger.error("âŒ æ— åŒ¹é…çš„é¢‘é“ç»“æœ")
                return
            
            # ç¬¬å…­æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            self.logger.info("ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
            self.generate_output_files(filtered_results)
            
            # å®Œæˆç»Ÿè®¡
            total_time = time.time() - start_time
            self.logger.info(f"ğŸ‰ å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
            
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='IPTVç›´æ’­æºå¤„ç†å·¥å…·')
    parser.add_argument('--config', '-c', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--timeout', '-t', type=int, help='è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--workers', '-w', type=int, help='å¹¶å‘çº¿ç¨‹æ•°')
    parser.add_argument('--test-size', '-s', type=int, help='æµ‹é€Ÿæ•°æ®å¤§å°(KB)')
    parser.add_argument('--min-speed', type=float, help='æœ€ä½é€Ÿåº¦è¦æ±‚(MB/s)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ—¥å¿—è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = logging.DEBUG if args.verbose else logging.INFO
    
    try:
        # åˆ›å»ºIPTVå®ä¾‹
        tool = IPTV(args.config)
        
        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        if args.timeout:
            tool.timeout = args.timeout
        if args.workers:
            tool.max_workers = args.workers
        if args.test_size:
            tool.test_size = args.test_size * 1024
        if args.min_speed:
            tool.min_speed_mbps = args.min_speed
        
        # è¿è¡Œä¸»æµç¨‹
        tool.run()
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        logging.exception("ç¨‹åºå¼‚å¸¸")


if __name__ == "__main__":
    main()
